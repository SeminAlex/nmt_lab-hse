{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle as pickle\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readed en0\n",
      "readed es0\n",
      "readed en1\n",
      "readed es1\n",
      "readed en2\n",
      "readed es2\n",
      "readed en3\n",
      "readed es3\n"
     ]
    }
   ],
   "source": [
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)\n",
    "#dataRead\n",
    "\n",
    "en_sentences = []\n",
    "es_sentences = []\n",
    "\n",
    "for i in range(4):\n",
    "    with open(\"dataset/europarl-v7.es-en-\" + str(i) +\".en\", 'r') as en_file:\n",
    "        for en_line in en_file:\n",
    "            en_sentences.append(en_line)\n",
    "    print \"readed en\" + str(i)\n",
    "    with open(\"dataset/europarl-v7.es-en-\" + str(i) +\".es\", 'r') as es_file:\n",
    "        for es_line in es_file:\n",
    "            es_sentences.append(es_line)\n",
    "    print \"readed es\" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apart from anything else, they divisively create social problems for workers in the shipping industry and residents of island regions.\n",
      "\n",
      "Aparte de todo, crean, generando discrepancias, problemas sociales para los trabajadores del sector naviero y los residentes de las regiones insulares.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = 999999\n",
    "print en_sentences[index]\n",
    "print es_sentences[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#DataPreparation\n",
    "def create_dataset(source_sentences,target_sentences):\n",
    "    source_vocab_dict = Counter(word.strip(',.\" ;:)(][?!') for sentence in source_sentences for word in sentence.split())\n",
    "    target_vocab_dict = Counter(word.strip(',.\" ;:)(][?!') for sentence in target_sentences for word in sentence.split())\n",
    "\n",
    "    source_vocab = map(lambda x: x[0], sorted(source_vocab_dict.items(), key = lambda x: -x[1]))\n",
    "    target_vocab = map(lambda x: x[0], sorted(target_vocab_dict.items(), key = lambda x: -x[1]))\n",
    "    \n",
    "    source_vocab = source_vocab[:20000]\n",
    "    target_vocab = target_vocab[:30000]\n",
    "    \n",
    "    start_idx = 2\n",
    "    source_word2idx = dict([(word, idx+start_idx) for idx, word in enumerate(source_vocab)])\n",
    "    source_word2idx['<ukn>'] = 0\n",
    "    source_word2idx['<pad>'] = 1\n",
    "    source_idx2word = dict([(idx, word) for word, idx in source_word2idx.iteritems()])\n",
    "    \n",
    "    start_idx = 4\n",
    "    target_word2idx = dict([(word, idx+start_idx) for idx, word in enumerate(target_vocab)])\n",
    "    target_word2idx['<ukn>'] = 0\n",
    "    target_word2idx['<go>']  = 1\n",
    "    target_word2idx['<eos>'] = 2\n",
    "    target_word2idx['<pad>'] = 3\n",
    "    \n",
    "    target_idx2word = dict([(idx, word) for word, idx in target_word2idx.iteritems()])\n",
    "    x = [[source_word2idx.get(word.strip(',.\" ;:)(][?!'), 0) for word in sentence.split()] for sentence in source_sentences]\n",
    "    y = [[target_word2idx.get(word.strip(',.\" ;:)(][?!'), 0) for word in sentence.split()] for sentence in target_sentences]\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        n1 = len(x[i])\n",
    "        n2 = len(y[i])\n",
    "        n = n1 if n1 < n2 else n2 \n",
    "        if abs(n1 - n2) <= 0.3 * n:\n",
    "            if n1 <= 15 and n2 <= 15:\n",
    "                X.append(x[i])\n",
    "                Y.append(y[i])\n",
    "    return X, Y, source_word2idx, source_idx2word, source_vocab, target_word2idx, target_idx2word, target_vocab\n",
    "\n",
    "def save_dataset(file_path, obj):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(obj, f, -1)\n",
    "\n",
    "def read_dataset(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#en_sentences = ['hello my friend', 'we need to reboot the server']\n",
    "#es_sentences = ['hola mi amigo', 'necesitamos reiniciar el servidor']\n",
    "save_dataset('./data.pkl', create_dataset(en_sentences, es_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read dataset\n",
    "X, Y, en_word2idx, en_idx2word, en_vocab, es_word2idx, es_idx2word, es_vocab = read_dataset('data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence in English - encoded: [8425, 3, 2, 1695]\n",
      "Sentence in Spanish - encoded: [10624, 13, 575, 4, 1420]\n",
      "Decoded:\n",
      "------------------------\n",
      "Resumption of the session \n",
      "\n",
      "Reanudación del período de sesiones\n"
     ]
    }
   ],
   "source": [
    "#CHECK THAT WORKs\n",
    "print 'Sentence in English - encoded:', X[0]\n",
    "print 'Sentence in Spanish - encoded:', Y[0]\n",
    "print 'Decoded:\\n------------------------'\n",
    "\n",
    "for i in range(len(X[0])):\n",
    "    print en_idx2word[X[0][i]],\n",
    "    \n",
    "print '\\n'\n",
    "\n",
    "for i in range(len(Y[0])):\n",
    "    print es_idx2word[Y[0][i]],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data processing\n",
    "\n",
    "# data padding\n",
    "def data_padding(x, y, length = 15):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = x[i] + (length - len(x[i])) * [en_word2idx['<pad>']]\n",
    "        y[i] = [es_word2idx['<go>']] + y[i] + [es_word2idx['<eos>']] + (length-len(y[i])) * [es_word2idx['<pad>']]\n",
    "\n",
    "data_padding(X, Y)\n",
    "#print X\n",
    "\n",
    "# data splitting\n",
    "X_train,  X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1)\n",
    "\n",
    "del X\n",
    "del Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build a model\n",
    "\n",
    "input_seq_len = 15\n",
    "output_seq_len = 17\n",
    "en_vocab_size = len(en_vocab) + 2 # + <pad>, <ukn>\n",
    "es_vocab_size = len(es_vocab) + 4 # + <pad>, <ukn>, <eos>, <go>\n",
    "\n",
    "# placeholders\n",
    "encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'encoder{}'.format(i)) for i in range(input_seq_len)]\n",
    "decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'decoder{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "targets = [decoder_inputs[i+1] for i in range(output_seq_len-1)]\n",
    "# add one more target\n",
    "targets.append(tf.placeholder(dtype = tf.int32, shape = [None], name = 'last_target'))\n",
    "target_weights = [tf.placeholder(dtype = tf.float32, shape = [None], name = 'target_w{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "# output projection\n",
    "size = 512\n",
    "w_t = tf.get_variable('proj_w', [es_vocab_size, size], tf.float32)\n",
    "b = tf.get_variable('proj_b', [es_vocab_size], tf.float32)\n",
    "w = tf.transpose(w_t)\n",
    "output_projection = (w, b)\n",
    "\n",
    "outputs, states = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "                                            encoder_inputs,\n",
    "                                            decoder_inputs,\n",
    "                                            tf.contrib.rnn.BasicLSTMCell(size),\n",
    "                                            num_encoder_symbols = en_vocab_size,\n",
    "                                            num_decoder_symbols = es_vocab_size,\n",
    "                                            embedding_size = 100,\n",
    "                                            feed_previous = False,\n",
    "                                            output_projection = output_projection,\n",
    "                                            dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define our loss function\n",
    "\n",
    "# sampled softmax loss - returns: A batch_size 1-D tensor of per-example sampled softmax losses\n",
    "def sampled_loss(labels, logits):\n",
    "    return tf.nn.sampled_softmax_loss(\n",
    "                        weights = w_t,\n",
    "                        biases = b,\n",
    "                        labels = tf.reshape(labels, [-1, 1]),\n",
    "                        inputs = logits,\n",
    "                        num_sampled = 512,\n",
    "                        num_classes = es_vocab_size)\n",
    "\n",
    "# Weighted cross-entropy loss for a sequence of logits\n",
    "loss = tf.contrib.legacy_seq2seq.sequence_loss(outputs, targets, target_weights, softmax_loss_function = sampled_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's define some helper functions\n",
    "\n",
    "# simple softmax function\n",
    "def softmax(x):\n",
    "    n = np.max(x)\n",
    "    e_x = np.exp(x - n)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# feed data into placeholders\n",
    "def feed_dict(x, y, batch_size = 64):\n",
    "    feed = {}\n",
    "    \n",
    "    idxes = np.random.choice(len(x), size = batch_size, replace = False)\n",
    "    \n",
    "    for i in range(input_seq_len):\n",
    "        feed[encoder_inputs[i].name] = np.array([x[j][i] for j in idxes], dtype = np.int32)\n",
    "        \n",
    "    for i in range(output_seq_len):\n",
    "        feed[decoder_inputs[i].name] = np.array([y[j][i] for j in idxes], dtype = np.int32)\n",
    "        \n",
    "    feed[targets[len(targets)-1].name] = np.full(shape = [batch_size], fill_value = es_word2idx['<pad>'], dtype = np.int32)\n",
    "    \n",
    "    for i in range(output_seq_len-1):\n",
    "        batch_weights = np.ones(batch_size, dtype = np.float32)\n",
    "        target = feed[decoder_inputs[i+1].name]\n",
    "        for j in range(batch_size):\n",
    "            if target[j] == es_word2idx['<pad>']:\n",
    "                batch_weights[j] = 0.0\n",
    "        feed[target_weights[i].name] = batch_weights\n",
    "        \n",
    "    feed[target_weights[output_seq_len-1].name] = np.zeros(batch_size, dtype = np.float32)\n",
    "    \n",
    "    return feed\n",
    "\n",
    "# decode output sequence\n",
    "def decode_output(output_seq):\n",
    "    words = []\n",
    "    for i in range(output_seq_len):\n",
    "        smax = softmax(output_seq[i])\n",
    "        idx = np.argmax(smax)\n",
    "        words.append(es_idx2word[idx])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ops and hyperparameters\n",
    "learning_rate = 5e-3\n",
    "batch_size = 64\n",
    "steps = 10000\n",
    "\n",
    "# ops for projecting outputs\n",
    "outputs_proj = [tf.matmul(outputs[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "\n",
    "# training op\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# init op\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# forward step\n",
    "def forward_step(sess, feed):\n",
    "    output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
    "    return output_sequences\n",
    "\n",
    "# training step\n",
    "def backward_step(sess, feed):\n",
    "    sess.run(optimizer, feed_dict = feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------TRAINING------------------\n",
      "step: 0, loss: 8.92805767059\n",
      "step: 4, loss: 9.06284236908\n",
      "step: 9, loss: 8.9558095932\n",
      "step: 14, loss: 9.03004360199\n",
      "step: 19, loss: 8.93416500092\n",
      "Checkpoint is saved\n",
      "step: 24, loss: 8.97299194336\n",
      "step: 29, loss: 8.93711280823\n",
      "step: 34, loss: 8.96745300293\n",
      "step: 39, loss: 8.91863059998\n",
      "Checkpoint is saved\n",
      "step: 44, loss: 8.87254905701\n",
      "step: 49, loss: 8.93920612335\n",
      "step: 54, loss: 8.7603302002\n",
      "step: 59, loss: 8.10283946991\n",
      "Checkpoint is saved\n",
      "step: 64, loss: 7.48526477814\n",
      "step: 69, loss: 7.36025714874\n",
      "step: 74, loss: 6.68564033508\n",
      "step: 79, loss: 6.38210296631\n",
      "Checkpoint is saved\n",
      "step: 84, loss: 7.42009544373\n",
      "step: 89, loss: 5.92046117783\n",
      "step: 94, loss: 6.91994142532\n",
      "step: 99, loss: 6.00060224533\n",
      "Checkpoint is saved\n",
      "step: 104, loss: 5.9369468689\n",
      "step: 109, loss: 5.46219968796\n",
      "step: 114, loss: 6.90664482117\n",
      "step: 119, loss: 5.73440647125\n",
      "Checkpoint is saved\n",
      "step: 124, loss: 5.69991445541\n",
      "step: 129, loss: 5.35616922379\n",
      "step: 134, loss: 6.65883970261\n",
      "step: 139, loss: 5.45341920853\n",
      "Checkpoint is saved\n",
      "step: 144, loss: 5.40574169159\n",
      "step: 149, loss: 5.25493001938\n",
      "step: 154, loss: 5.87260007858\n",
      "step: 159, loss: 5.07461452484\n",
      "Checkpoint is saved\n",
      "step: 164, loss: 4.99566602707\n",
      "step: 169, loss: 5.1402387619\n",
      "step: 174, loss: 4.9320526123\n",
      "step: 179, loss: 6.15887069702\n",
      "Checkpoint is saved\n",
      "step: 184, loss: 4.99999666214\n",
      "step: 189, loss: 4.59330463409\n",
      "step: 194, loss: 4.99308395386\n",
      "step: 199, loss: 4.98245286942\n",
      "Checkpoint is saved\n",
      "step: 204, loss: 4.76064729691\n",
      "step: 209, loss: 4.55762672424\n",
      "step: 214, loss: 5.06251382828\n",
      "step: 219, loss: 4.34664392471\n",
      "Checkpoint is saved\n",
      "step: 224, loss: 4.46476650238\n",
      "step: 229, loss: 3.8424577713\n",
      "step: 234, loss: 4.18482780457\n",
      "step: 239, loss: 3.9493162632\n",
      "Checkpoint is saved\n",
      "step: 244, loss: 4.00663757324\n",
      "step: 249, loss: 3.80388069153\n",
      "step: 254, loss: 4.39574384689\n",
      "step: 259, loss: 4.13242530823\n",
      "Checkpoint is saved\n",
      "step: 264, loss: 3.76178026199\n",
      "step: 269, loss: 3.79005861282\n",
      "step: 274, loss: 4.04507732391\n",
      "step: 279, loss: 3.51771330833\n",
      "Checkpoint is saved\n",
      "step: 284, loss: 3.33559036255\n",
      "step: 289, loss: 3.46062278748\n",
      "step: 294, loss: 3.84532403946\n",
      "step: 299, loss: 3.52903437614\n",
      "Checkpoint is saved\n",
      "step: 304, loss: 3.43338823318\n",
      "step: 309, loss: 3.43024849892\n",
      "step: 314, loss: 3.21597003937\n",
      "step: 319, loss: 3.19100999832\n",
      "Checkpoint is saved\n",
      "step: 324, loss: 3.43262529373\n",
      "step: 329, loss: 3.2396531105\n",
      "step: 334, loss: 3.03096604347\n",
      "step: 339, loss: 3.18285059929\n",
      "Checkpoint is saved\n",
      "step: 344, loss: 3.43548631668\n",
      "step: 349, loss: 3.33850073814\n",
      "step: 354, loss: 2.89265632629\n",
      "step: 359, loss: 2.71736359596\n",
      "Checkpoint is saved\n",
      "step: 364, loss: 3.13578271866\n",
      "step: 369, loss: 3.20820665359\n",
      "step: 374, loss: 2.62113285065\n",
      "step: 379, loss: 2.81516551971\n",
      "Checkpoint is saved\n",
      "step: 384, loss: 3.00734043121\n",
      "step: 389, loss: 2.7548251152\n",
      "step: 394, loss: 2.66754722595\n",
      "step: 399, loss: 2.63168263435\n",
      "Checkpoint is saved\n",
      "step: 404, loss: 2.67125916481\n",
      "step: 409, loss: 2.77879095078\n",
      "step: 414, loss: 2.75912714005\n",
      "step: 419, loss: 2.47463679314\n",
      "Checkpoint is saved\n",
      "step: 424, loss: 2.63456964493\n",
      "step: 429, loss: 2.46069598198\n",
      "step: 434, loss: 2.31960964203\n",
      "step: 439, loss: 2.54570579529\n",
      "Checkpoint is saved\n",
      "step: 444, loss: 2.61559963226\n",
      "step: 449, loss: 2.26377153397\n",
      "step: 454, loss: 2.76690721512\n",
      "step: 459, loss: 2.28184843063\n",
      "Checkpoint is saved\n",
      "step: 464, loss: 2.22459673882\n",
      "step: 469, loss: 2.45011568069\n",
      "step: 474, loss: 2.10932397842\n",
      "step: 479, loss: 2.333070755\n",
      "Checkpoint is saved\n",
      "step: 484, loss: 2.09190297127\n",
      "step: 489, loss: 2.05713605881\n",
      "step: 494, loss: 2.51509451866\n",
      "step: 499, loss: 1.92808020115\n",
      "Checkpoint is saved\n",
      "step: 504, loss: 2.11499214172\n",
      "step: 509, loss: 2.32505512238\n",
      "step: 514, loss: 2.01368021965\n",
      "step: 519, loss: 1.91621422768\n",
      "Checkpoint is saved\n",
      "step: 524, loss: 2.1048874855\n",
      "step: 529, loss: 2.17499232292\n",
      "step: 534, loss: 2.12900233269\n",
      "step: 539, loss: 2.33401489258\n",
      "Checkpoint is saved\n",
      "step: 544, loss: 2.16475057602\n",
      "step: 549, loss: 2.21747088432\n",
      "step: 554, loss: 2.14827775955\n",
      "step: 559, loss: 1.93436408043\n",
      "Checkpoint is saved\n",
      "step: 564, loss: 1.92162632942\n",
      "step: 569, loss: 2.27503871918\n",
      "step: 574, loss: 1.79728245735\n",
      "step: 579, loss: 1.93776237965\n",
      "Checkpoint is saved\n",
      "step: 584, loss: 1.93924379349\n",
      "step: 589, loss: 2.0840139389\n",
      "step: 594, loss: 1.76666164398\n",
      "step: 599, loss: 2.15056085587\n",
      "Checkpoint is saved\n",
      "step: 604, loss: 1.76754248142\n",
      "step: 609, loss: 2.16067695618\n",
      "step: 614, loss: 1.72275722027\n",
      "step: 619, loss: 1.99905264378\n",
      "Checkpoint is saved\n",
      "step: 624, loss: 1.71545433998\n",
      "step: 629, loss: 1.6606900692\n",
      "step: 634, loss: 1.66155815125\n",
      "step: 639, loss: 1.85790276527\n",
      "Checkpoint is saved\n",
      "step: 644, loss: 1.97876429558\n",
      "step: 649, loss: 1.85710370541\n",
      "step: 654, loss: 1.74840474129\n",
      "step: 659, loss: 1.99155509472\n",
      "Checkpoint is saved\n",
      "step: 664, loss: 1.51530361176\n",
      "step: 669, loss: 1.64849770069\n",
      "step: 674, loss: 1.52988314629\n",
      "step: 679, loss: 1.61436474323\n",
      "Checkpoint is saved\n",
      "step: 684, loss: 2.0693423748\n",
      "step: 689, loss: 1.53807246685\n",
      "step: 694, loss: 1.75939786434\n",
      "step: 699, loss: 1.62613582611\n",
      "Checkpoint is saved\n",
      "step: 704, loss: 1.42685556412\n",
      "step: 709, loss: 1.57598996162\n",
      "step: 714, loss: 1.75875353813\n",
      "step: 719, loss: 1.6587741375\n",
      "Checkpoint is saved\n",
      "step: 724, loss: 1.7242872715\n",
      "step: 729, loss: 1.77690410614\n",
      "step: 734, loss: 1.48386132717\n",
      "step: 739, loss: 1.39536118507\n",
      "Checkpoint is saved\n",
      "step: 744, loss: 1.52669692039\n",
      "step: 749, loss: 1.53849029541\n",
      "step: 754, loss: 1.82224738598\n",
      "step: 759, loss: 1.49887275696\n",
      "Checkpoint is saved\n",
      "step: 764, loss: 1.55596446991\n",
      "step: 769, loss: 1.52561354637\n",
      "step: 774, loss: 1.37936425209\n",
      "step: 779, loss: 1.460496068\n",
      "Checkpoint is saved\n",
      "step: 784, loss: 1.72907090187\n",
      "step: 789, loss: 1.46536850929\n",
      "step: 794, loss: 1.50450384617\n",
      "step: 799, loss: 1.32864570618\n",
      "Checkpoint is saved\n",
      "step: 804, loss: 1.40698683262\n",
      "step: 809, loss: 1.54082036018\n",
      "step: 814, loss: 1.48048913479\n",
      "step: 819, loss: 1.44694209099\n",
      "Checkpoint is saved\n",
      "step: 824, loss: 1.4107670784\n",
      "step: 829, loss: 1.30431354046\n",
      "step: 834, loss: 1.22585368156\n",
      "step: 839, loss: 1.55142378807\n",
      "Checkpoint is saved\n",
      "step: 844, loss: 1.27586627007\n",
      "step: 849, loss: 1.41285800934\n",
      "step: 854, loss: 1.39246225357\n",
      "step: 859, loss: 1.35967516899\n",
      "Checkpoint is saved\n",
      "step: 864, loss: 1.33869290352\n",
      "step: 869, loss: 1.62364208698\n",
      "step: 874, loss: 1.42014980316\n",
      "step: 879, loss: 1.08821630478\n",
      "Checkpoint is saved\n",
      "step: 884, loss: 1.28142869473\n",
      "step: 889, loss: 1.49582982063\n",
      "step: 894, loss: 1.1988710165\n",
      "step: 899, loss: 1.13909053802\n",
      "Checkpoint is saved\n",
      "step: 904, loss: 1.42483222485\n",
      "step: 909, loss: 1.46167087555\n",
      "step: 914, loss: 1.28991222382\n",
      "step: 919, loss: 1.44402575493\n",
      "Checkpoint is saved\n",
      "step: 924, loss: 1.22268915176\n",
      "step: 929, loss: 1.38170206547\n",
      "step: 934, loss: 1.01578068733\n",
      "step: 939, loss: 1.39517581463\n",
      "Checkpoint is saved\n",
      "step: 944, loss: 1.34962630272\n",
      "step: 949, loss: 1.35397064686\n",
      "step: 954, loss: 1.32637429237\n",
      "step: 959, loss: 1.25871086121\n",
      "Checkpoint is saved\n",
      "step: 964, loss: 1.21140623093\n",
      "step: 969, loss: 1.11886501312\n",
      "step: 974, loss: 1.2779173851\n",
      "step: 979, loss: 1.24382328987\n",
      "Checkpoint is saved\n",
      "step: 984, loss: 1.2525498867\n",
      "step: 989, loss: 1.3241263628\n",
      "step: 994, loss: 1.22201514244\n",
      "step: 999, loss: 1.29130709171\n",
      "Checkpoint is saved\n",
      "step: 1004, loss: 1.65720486641\n",
      "step: 1009, loss: 1.24408090115\n",
      "step: 1014, loss: 1.20786786079\n",
      "step: 1019, loss: 1.21742892265\n",
      "Checkpoint is saved\n",
      "step: 1024, loss: 0.987391233444\n",
      "step: 1029, loss: 1.08747494221\n",
      "step: 1034, loss: 1.21094286442\n",
      "step: 1039, loss: 1.35359859467\n",
      "Checkpoint is saved\n",
      "step: 1044, loss: 1.21290540695\n",
      "step: 1049, loss: 1.07023823261\n",
      "step: 1054, loss: 1.22357058525\n",
      "step: 1059, loss: 1.1816573143\n",
      "Checkpoint is saved\n",
      "step: 1064, loss: 1.18419015408\n",
      "step: 1069, loss: 1.13807535172\n",
      "step: 1074, loss: 1.26065278053\n",
      "step: 1079, loss: 1.17752313614\n",
      "Checkpoint is saved\n",
      "step: 1084, loss: 1.32002842426\n",
      "step: 1089, loss: 1.20548784733\n",
      "step: 1094, loss: 1.30007123947\n",
      "step: 1099, loss: 1.19423604012\n",
      "Checkpoint is saved\n",
      "step: 1104, loss: 1.193816185\n",
      "step: 1109, loss: 1.00695681572\n",
      "step: 1114, loss: 1.045566082\n",
      "step: 1119, loss: 1.14300632477\n",
      "Checkpoint is saved\n",
      "step: 1124, loss: 1.02306675911\n",
      "step: 1129, loss: 0.950623810291\n",
      "step: 1134, loss: 1.03607690334\n",
      "step: 1139, loss: 1.324010849\n",
      "Checkpoint is saved\n",
      "step: 1144, loss: 1.1174185276\n",
      "step: 1149, loss: 1.25059580803\n",
      "step: 1154, loss: 1.08923637867\n",
      "step: 1159, loss: 1.2246004343\n",
      "Checkpoint is saved\n",
      "step: 1164, loss: 1.09547901154\n",
      "step: 1169, loss: 1.17680692673\n",
      "step: 1174, loss: 1.20923781395\n",
      "step: 1179, loss: 1.24005222321\n",
      "Checkpoint is saved\n",
      "step: 1184, loss: 1.12279248238\n",
      "step: 1189, loss: 0.851395726204\n",
      "step: 1194, loss: 1.16773188114\n",
      "step: 1199, loss: 1.12692403793\n",
      "Checkpoint is saved\n",
      "step: 1204, loss: 0.956023514271\n",
      "step: 1209, loss: 1.53166246414\n",
      "step: 1214, loss: 1.16189968586\n",
      "step: 1219, loss: 1.05826234818\n",
      "Checkpoint is saved\n",
      "step: 1224, loss: 1.22936940193\n",
      "step: 1229, loss: 1.09985888004\n",
      "step: 1234, loss: 1.05775332451\n",
      "step: 1239, loss: 1.22009468079\n",
      "Checkpoint is saved\n",
      "step: 1244, loss: 1.04761230946\n",
      "step: 1249, loss: 1.01922535896\n",
      "step: 1254, loss: 0.852102994919\n",
      "step: 1259, loss: 0.961713194847\n",
      "Checkpoint is saved\n",
      "step: 1264, loss: 0.969807088375\n",
      "step: 1269, loss: 1.13667559624\n",
      "step: 1274, loss: 1.09995532036\n",
      "step: 1279, loss: 1.10861241817\n",
      "Checkpoint is saved\n",
      "step: 1284, loss: 0.950466096401\n",
      "step: 1289, loss: 1.04414224625\n",
      "step: 1294, loss: 1.24245429039\n",
      "step: 1299, loss: 1.044028759\n",
      "Checkpoint is saved\n",
      "step: 1304, loss: 1.09740161896\n",
      "step: 1309, loss: 1.00026988983\n",
      "step: 1314, loss: 1.22688055038\n",
      "step: 1319, loss: 0.894127249718\n",
      "Checkpoint is saved\n",
      "step: 1324, loss: 1.00304698944\n",
      "step: 1329, loss: 0.882758140564\n",
      "step: 1334, loss: 1.03948199749\n",
      "step: 1339, loss: 1.0016554594\n",
      "Checkpoint is saved\n",
      "step: 1344, loss: 1.05997467041\n",
      "step: 1349, loss: 0.8172519207\n",
      "step: 1354, loss: 1.01955413818\n",
      "step: 1359, loss: 0.994255006313\n",
      "Checkpoint is saved\n",
      "step: 1364, loss: 1.14993596077\n",
      "step: 1369, loss: 1.03098499775\n",
      "step: 1374, loss: 0.896600842476\n",
      "step: 1379, loss: 0.826029062271\n",
      "Checkpoint is saved\n",
      "step: 1384, loss: 0.978549718857\n",
      "step: 1389, loss: 1.0750579834\n",
      "step: 1394, loss: 0.861383199692\n",
      "step: 1399, loss: 0.98639768362\n",
      "Checkpoint is saved\n",
      "step: 1404, loss: 0.968085587025\n",
      "step: 1409, loss: 1.0663408041\n",
      "step: 1414, loss: 0.926073014736\n",
      "step: 1419, loss: 1.11266279221\n",
      "Checkpoint is saved\n",
      "step: 1424, loss: 1.15709877014\n",
      "step: 1429, loss: 0.791409850121\n",
      "step: 1434, loss: 0.910498380661\n",
      "step: 1439, loss: 1.07703614235\n",
      "Checkpoint is saved\n",
      "step: 1444, loss: 1.01926231384\n",
      "step: 1449, loss: 0.869293689728\n",
      "step: 1454, loss: 0.798017680645\n",
      "step: 1459, loss: 0.829725503922\n",
      "Checkpoint is saved\n",
      "step: 1464, loss: 0.906362116337\n",
      "step: 1469, loss: 1.01994407177\n",
      "step: 1474, loss: 0.997175872326\n",
      "step: 1479, loss: 0.919745922089\n",
      "Checkpoint is saved\n",
      "step: 1484, loss: 1.10727190971\n",
      "step: 1489, loss: 0.990934610367\n",
      "step: 1494, loss: 0.857448756695\n",
      "step: 1499, loss: 1.06985020638\n",
      "Checkpoint is saved\n",
      "step: 1504, loss: 0.919063508511\n",
      "step: 1509, loss: 0.880096852779\n",
      "step: 1514, loss: 0.952840268612\n",
      "step: 1519, loss: 1.00663149357\n",
      "Checkpoint is saved\n",
      "step: 1524, loss: 1.04470825195\n",
      "step: 1529, loss: 0.971590399742\n",
      "step: 1534, loss: 1.08633720875\n",
      "step: 1539, loss: 0.922982692719\n",
      "Checkpoint is saved\n",
      "step: 1544, loss: 1.06036484241\n",
      "step: 1549, loss: 1.00502443314\n",
      "step: 1554, loss: 0.797563612461\n",
      "step: 1559, loss: 1.09942889214\n",
      "Checkpoint is saved\n",
      "step: 1564, loss: 0.935488581657\n",
      "step: 1569, loss: 0.988563776016\n",
      "step: 1574, loss: 1.08879470825\n",
      "step: 1579, loss: 0.848185837269\n",
      "Checkpoint is saved\n",
      "step: 1584, loss: 0.902905702591\n",
      "step: 1589, loss: 0.985586941242\n",
      "step: 1594, loss: 0.890867948532\n",
      "step: 1599, loss: 0.950767695904\n",
      "Checkpoint is saved\n",
      "step: 1604, loss: 0.992557525635\n",
      "step: 1609, loss: 0.998113155365\n",
      "step: 1614, loss: 0.968246102333\n",
      "step: 1619, loss: 0.801865458488\n",
      "Checkpoint is saved\n",
      "step: 1624, loss: 0.819135069847\n",
      "step: 1629, loss: 0.761048078537\n",
      "step: 1634, loss: 0.920428574085\n",
      "step: 1639, loss: 0.869076728821\n",
      "Checkpoint is saved\n",
      "step: 1644, loss: 0.715156376362\n",
      "step: 1649, loss: 1.1618783474\n",
      "step: 1654, loss: 0.851938605309\n",
      "step: 1659, loss: 0.90817886591\n",
      "Checkpoint is saved\n",
      "step: 1664, loss: 0.925565779209\n",
      "step: 1669, loss: 0.987964451313\n",
      "step: 1674, loss: 0.872397601604\n",
      "step: 1679, loss: 0.788329005241\n",
      "Checkpoint is saved\n",
      "step: 1684, loss: 0.773280262947\n",
      "step: 1689, loss: 0.724846243858\n",
      "step: 1694, loss: 0.934550404549\n",
      "step: 1699, loss: 0.848575115204\n",
      "Checkpoint is saved\n",
      "step: 1704, loss: 0.938032269478\n",
      "step: 1709, loss: 0.82658803463\n",
      "step: 1714, loss: 0.68236219883\n",
      "step: 1719, loss: 1.02557134628\n",
      "Checkpoint is saved\n",
      "step: 1724, loss: 0.895392894745\n",
      "step: 1729, loss: 0.773585140705\n",
      "step: 1734, loss: 0.997862458229\n",
      "step: 1739, loss: 0.941045582294\n",
      "Checkpoint is saved\n",
      "step: 1744, loss: 0.931787967682\n",
      "step: 1749, loss: 0.823754787445\n",
      "step: 1754, loss: 0.935998797417\n",
      "step: 1759, loss: 0.942831933498\n",
      "Checkpoint is saved\n",
      "step: 1764, loss: 0.757217764854\n",
      "step: 1769, loss: 0.705445885658\n",
      "step: 1774, loss: 0.814126670361\n",
      "step: 1779, loss: 0.78264772892\n",
      "Checkpoint is saved\n",
      "step: 1784, loss: 0.736603140831\n",
      "step: 1789, loss: 0.804632663727\n",
      "step: 1794, loss: 0.841583967209\n",
      "step: 1799, loss: 0.767400622368\n",
      "Checkpoint is saved\n",
      "step: 1804, loss: 0.723185420036\n",
      "step: 1809, loss: 0.770001649857\n",
      "step: 1814, loss: 0.875857532024\n",
      "step: 1819, loss: 0.914272665977\n",
      "Checkpoint is saved\n",
      "step: 1824, loss: 1.12892889977\n",
      "step: 1829, loss: 0.637654304504\n",
      "step: 1834, loss: 0.722855985165\n",
      "step: 1839, loss: 0.846059501171\n",
      "Checkpoint is saved\n",
      "step: 1844, loss: 0.741267740726\n",
      "step: 1849, loss: 0.75312936306\n",
      "step: 1854, loss: 0.795081019402\n",
      "step: 1859, loss: 0.724816560745\n",
      "Checkpoint is saved\n",
      "step: 1864, loss: 0.954928576946\n",
      "step: 1869, loss: 0.86215698719\n",
      "step: 1874, loss: 0.784177660942\n",
      "step: 1879, loss: 0.712559401989\n",
      "Checkpoint is saved\n",
      "step: 1884, loss: 0.902841806412\n",
      "step: 1889, loss: 0.898560285568\n",
      "step: 1894, loss: 0.731236815453\n",
      "step: 1899, loss: 1.06632614136\n",
      "Checkpoint is saved\n",
      "step: 1904, loss: 0.820878267288\n",
      "step: 1909, loss: 0.78480297327\n",
      "step: 1914, loss: 0.881821453571\n",
      "step: 1919, loss: 0.862018764019\n",
      "Checkpoint is saved\n",
      "step: 1924, loss: 0.834370076656\n",
      "step: 1929, loss: 0.773689568043\n",
      "step: 1934, loss: 0.790039300919\n",
      "step: 1939, loss: 0.852666497231\n",
      "Checkpoint is saved\n",
      "step: 1944, loss: 0.821851789951\n",
      "step: 1949, loss: 0.73176330328\n",
      "step: 1954, loss: 0.701304852962\n",
      "step: 1959, loss: 0.78399014473\n",
      "Checkpoint is saved\n",
      "step: 1964, loss: 0.693636894226\n",
      "step: 1969, loss: 0.765697479248\n",
      "step: 1974, loss: 0.676313519478\n",
      "step: 1979, loss: 0.633999228477\n",
      "Checkpoint is saved\n",
      "step: 1984, loss: 0.846947312355\n",
      "step: 1989, loss: 0.69765651226\n",
      "step: 1994, loss: 0.729976892471\n",
      "step: 1999, loss: 0.660640001297\n",
      "Checkpoint is saved\n",
      "step: 2004, loss: 0.727263569832\n",
      "step: 2009, loss: 0.793112397194\n",
      "step: 2014, loss: 0.664262533188\n",
      "step: 2019, loss: 0.715699672699\n",
      "Checkpoint is saved\n",
      "step: 2024, loss: 0.774464130402\n",
      "step: 2029, loss: 0.913030743599\n",
      "step: 2034, loss: 0.896286487579\n",
      "step: 2039, loss: 0.719336211681\n",
      "Checkpoint is saved\n",
      "step: 2044, loss: 0.848805308342\n",
      "step: 2049, loss: 0.879641652107\n",
      "step: 2054, loss: 0.837008953094\n",
      "step: 2059, loss: 0.735587239265\n",
      "Checkpoint is saved\n",
      "step: 2064, loss: 0.754061102867\n",
      "step: 2069, loss: 0.620927453041\n",
      "step: 2074, loss: 0.799340248108\n",
      "step: 2079, loss: 0.848523855209\n",
      "Checkpoint is saved\n",
      "step: 2084, loss: 0.892475366592\n",
      "step: 2089, loss: 0.752982914448\n",
      "step: 2094, loss: 0.701592206955\n",
      "step: 2099, loss: 0.670402944088\n",
      "Checkpoint is saved\n",
      "step: 2104, loss: 0.768382310867\n",
      "step: 2109, loss: 0.792662858963\n",
      "step: 2114, loss: 0.878240466118\n",
      "step: 2119, loss: 0.72488117218\n",
      "Checkpoint is saved\n",
      "step: 2124, loss: 0.797700405121\n",
      "step: 2129, loss: 0.814859807491\n",
      "step: 2134, loss: 0.761887907982\n",
      "step: 2139, loss: 0.738344550133\n",
      "Checkpoint is saved\n",
      "step: 2144, loss: 0.826152682304\n",
      "step: 2149, loss: 0.823640286922\n",
      "step: 2154, loss: 0.643023312092\n",
      "step: 2159, loss: 0.700664043427\n",
      "Checkpoint is saved\n",
      "step: 2164, loss: 0.838414371014\n",
      "step: 2169, loss: 0.621411085129\n",
      "step: 2174, loss: 0.738799214363\n",
      "step: 2179, loss: 0.691259980202\n",
      "Checkpoint is saved\n",
      "step: 2184, loss: 0.654655098915\n",
      "step: 2189, loss: 0.895725190639\n",
      "step: 2194, loss: 0.759344637394\n",
      "step: 2199, loss: 0.805408835411\n",
      "Checkpoint is saved\n",
      "step: 2204, loss: 0.822674632072\n",
      "step: 2209, loss: 0.654742538929\n",
      "step: 2214, loss: 0.762491166592\n",
      "step: 2219, loss: 0.683923482895\n",
      "Checkpoint is saved\n",
      "step: 2224, loss: 0.826487660408\n",
      "step: 2229, loss: 0.679542541504\n",
      "step: 2234, loss: 0.743664801121\n",
      "step: 2239, loss: 0.840339004993\n",
      "Checkpoint is saved\n",
      "step: 2244, loss: 0.79495871067\n",
      "step: 2249, loss: 0.713716804981\n",
      "step: 2254, loss: 0.657116413116\n",
      "step: 2259, loss: 0.606532752514\n",
      "Checkpoint is saved\n",
      "step: 2264, loss: 0.773144423962\n",
      "step: 2269, loss: 0.776923179626\n",
      "step: 2274, loss: 0.637550115585\n",
      "step: 2279, loss: 0.726188361645\n",
      "Checkpoint is saved\n",
      "step: 2284, loss: 0.797501802444\n",
      "step: 2289, loss: 0.752057611942\n",
      "step: 2294, loss: 0.715882718563\n",
      "step: 2299, loss: 0.687512516975\n",
      "Checkpoint is saved\n",
      "step: 2304, loss: 0.682065963745\n",
      "step: 2309, loss: 0.686281979084\n",
      "step: 2314, loss: 0.600484371185\n",
      "step: 2319, loss: 0.762734353542\n",
      "Checkpoint is saved\n",
      "step: 2324, loss: 0.711491584778\n",
      "step: 2329, loss: 0.741281449795\n",
      "step: 2334, loss: 0.684504747391\n",
      "step: 2339, loss: 0.700811624527\n",
      "Checkpoint is saved\n",
      "step: 2344, loss: 0.817415177822\n",
      "step: 2349, loss: 0.655015766621\n",
      "step: 2354, loss: 0.725991427898\n",
      "step: 2359, loss: 0.648941218853\n",
      "Checkpoint is saved\n",
      "step: 2364, loss: 0.558658361435\n",
      "step: 2369, loss: 0.677165269852\n",
      "step: 2374, loss: 0.674177885056\n",
      "step: 2379, loss: 0.862187385559\n",
      "Checkpoint is saved\n",
      "step: 2384, loss: 0.607809066772\n",
      "step: 2389, loss: 0.641927242279\n",
      "step: 2394, loss: 0.590607881546\n",
      "step: 2399, loss: 0.738066792488\n",
      "Checkpoint is saved\n",
      "step: 2404, loss: 0.59178596735\n",
      "step: 2409, loss: 0.7423183918\n",
      "step: 2414, loss: 0.745561957359\n",
      "step: 2419, loss: 0.755548655987\n",
      "Checkpoint is saved\n",
      "step: 2424, loss: 0.759366333485\n",
      "step: 2429, loss: 0.661434650421\n",
      "step: 2434, loss: 0.704337954521\n",
      "step: 2439, loss: 0.709540128708\n",
      "Checkpoint is saved\n",
      "step: 2444, loss: 0.565376281738\n",
      "step: 2449, loss: 0.63972902298\n",
      "step: 2454, loss: 0.72768163681\n",
      "step: 2459, loss: 0.737873315811\n",
      "Checkpoint is saved\n",
      "step: 2464, loss: 0.772359371185\n",
      "step: 2469, loss: 0.703007042408\n",
      "step: 2474, loss: 0.727179884911\n",
      "step: 2479, loss: 0.714152157307\n",
      "Checkpoint is saved\n",
      "step: 2484, loss: 0.68432533741\n",
      "step: 2489, loss: 0.611568808556\n",
      "step: 2494, loss: 0.867162883282\n",
      "step: 2499, loss: 0.709054648876\n",
      "Checkpoint is saved\n",
      "step: 2504, loss: 0.776449203491\n",
      "step: 2509, loss: 0.676363587379\n",
      "step: 2514, loss: 0.79677939415\n",
      "step: 2519, loss: 0.670250356197\n",
      "Checkpoint is saved\n",
      "step: 2524, loss: 0.755025148392\n",
      "step: 2529, loss: 0.712149143219\n",
      "step: 2534, loss: 0.565374016762\n",
      "step: 2539, loss: 0.624254226685\n",
      "Checkpoint is saved\n",
      "step: 2544, loss: 0.725561380386\n",
      "step: 2549, loss: 0.662581920624\n",
      "step: 2554, loss: 0.713003635406\n",
      "step: 2559, loss: 0.651506304741\n",
      "Checkpoint is saved\n",
      "step: 2564, loss: 0.702907621861\n",
      "step: 2569, loss: 0.567674279213\n",
      "step: 2574, loss: 0.524872124195\n",
      "step: 2579, loss: 0.701982796192\n",
      "Checkpoint is saved\n",
      "step: 2584, loss: 0.592731416225\n",
      "step: 2589, loss: 0.83559513092\n",
      "step: 2594, loss: 0.694360673428\n",
      "step: 2599, loss: 0.541762590408\n",
      "Checkpoint is saved\n",
      "step: 2604, loss: 0.810243189335\n",
      "step: 2609, loss: 0.589747667313\n",
      "step: 2614, loss: 0.848698854446\n",
      "step: 2619, loss: 0.573458433151\n",
      "Checkpoint is saved\n",
      "step: 2624, loss: 0.68062722683\n",
      "step: 2629, loss: 0.750450730324\n",
      "step: 2634, loss: 0.603109061718\n",
      "step: 2639, loss: 0.649098873138\n",
      "Checkpoint is saved\n",
      "step: 2644, loss: 0.538924813271\n",
      "step: 2649, loss: 0.699203491211\n",
      "step: 2654, loss: 0.852746367455\n",
      "step: 2659, loss: 0.751857638359\n",
      "Checkpoint is saved\n",
      "step: 2664, loss: 0.757155835629\n",
      "step: 2669, loss: 0.631483495235\n",
      "step: 2674, loss: 0.816804528236\n",
      "step: 2679, loss: 0.724706530571\n",
      "Checkpoint is saved\n",
      "step: 2684, loss: 0.786504566669\n",
      "step: 2689, loss: 0.681736707687\n",
      "step: 2694, loss: 0.718575179577\n",
      "step: 2699, loss: 0.620159387589\n",
      "Checkpoint is saved\n",
      "step: 2704, loss: 0.658834576607\n",
      "step: 2709, loss: 0.779821157455\n",
      "step: 2714, loss: 0.710270166397\n",
      "step: 2719, loss: 0.746442556381\n",
      "Checkpoint is saved\n",
      "step: 2724, loss: 0.599304437637\n",
      "step: 2729, loss: 0.694119811058\n",
      "step: 2734, loss: 0.686865627766\n",
      "step: 2739, loss: 0.576155662537\n",
      "Checkpoint is saved\n",
      "step: 2744, loss: 0.68663084507\n",
      "step: 2749, loss: 0.630032360554\n",
      "step: 2754, loss: 0.735468506813\n",
      "step: 2759, loss: 0.784315943718\n",
      "Checkpoint is saved\n",
      "step: 2764, loss: 0.590232670307\n",
      "step: 2769, loss: 0.700276017189\n",
      "step: 2774, loss: 0.611932814121\n",
      "step: 2779, loss: 0.649947285652\n",
      "Checkpoint is saved\n",
      "step: 2784, loss: 0.719140648842\n",
      "step: 2789, loss: 0.658006191254\n",
      "step: 2794, loss: 0.630318343639\n",
      "step: 2799, loss: 0.852112352848\n",
      "Checkpoint is saved\n",
      "step: 2804, loss: 0.625001013279\n",
      "step: 2809, loss: 0.553420543671\n",
      "step: 2814, loss: 0.61797606945\n",
      "step: 2819, loss: 0.647525668144\n",
      "Checkpoint is saved\n",
      "step: 2824, loss: 0.614482164383\n",
      "step: 2829, loss: 0.677566468716\n",
      "step: 2834, loss: 0.575129747391\n",
      "step: 2839, loss: 0.559574842453\n",
      "Checkpoint is saved\n",
      "step: 2844, loss: 0.519126772881\n",
      "step: 2849, loss: 0.565838456154\n",
      "step: 2854, loss: 0.610465168953\n",
      "step: 2859, loss: 0.637014389038\n",
      "Checkpoint is saved\n",
      "step: 2864, loss: 0.636349916458\n",
      "step: 2869, loss: 0.728520274162\n",
      "step: 2874, loss: 0.680100083351\n",
      "step: 2879, loss: 0.67812782526\n",
      "Checkpoint is saved\n",
      "step: 2884, loss: 0.842659950256\n",
      "step: 2889, loss: 0.622790336609\n",
      "step: 2894, loss: 0.648193597794\n",
      "step: 2899, loss: 0.570415735245\n",
      "Checkpoint is saved\n",
      "step: 2904, loss: 0.702484548092\n",
      "step: 2909, loss: 0.570255160332\n",
      "step: 2914, loss: 0.518577754498\n",
      "step: 2919, loss: 0.800589621067\n",
      "Checkpoint is saved\n",
      "step: 2924, loss: 0.657638967037\n",
      "step: 2929, loss: 0.673941552639\n",
      "step: 2934, loss: 0.552452325821\n",
      "step: 2939, loss: 0.584912776947\n",
      "Checkpoint is saved\n",
      "step: 2944, loss: 0.527823209763\n",
      "step: 2949, loss: 0.695255041122\n",
      "step: 2954, loss: 0.597456932068\n",
      "step: 2959, loss: 0.616696834564\n",
      "Checkpoint is saved\n",
      "step: 2964, loss: 0.519220232964\n",
      "step: 2969, loss: 0.688585758209\n",
      "step: 2974, loss: 0.631931066513\n",
      "step: 2979, loss: 0.65300142765\n",
      "Checkpoint is saved\n",
      "step: 2984, loss: 0.629164934158\n",
      "step: 2989, loss: 0.613759040833\n",
      "step: 2994, loss: 0.615133166313\n",
      "step: 2999, loss: 0.740402579308\n",
      "Checkpoint is saved\n",
      "step: 3004, loss: 0.546467661858\n",
      "step: 3009, loss: 0.632062792778\n",
      "step: 3014, loss: 0.441099762917\n",
      "step: 3019, loss: 0.609152913094\n",
      "Checkpoint is saved\n",
      "step: 3024, loss: 0.69159913063\n",
      "step: 3029, loss: 0.642834305763\n",
      "step: 3034, loss: 0.596390426159\n",
      "step: 3039, loss: 0.686481535435\n",
      "Checkpoint is saved\n",
      "step: 3044, loss: 0.554154336452\n",
      "step: 3049, loss: 0.564496159554\n",
      "step: 3054, loss: 0.708811581135\n",
      "step: 3059, loss: 0.631775736809\n",
      "Checkpoint is saved\n",
      "step: 3064, loss: 0.618210554123\n",
      "step: 3069, loss: 0.583703577518\n",
      "step: 3074, loss: 0.670306921005\n",
      "step: 3079, loss: 0.73249900341\n",
      "Checkpoint is saved\n",
      "step: 3084, loss: 0.759575068951\n",
      "step: 3089, loss: 0.518199980259\n",
      "step: 3094, loss: 0.65453004837\n",
      "step: 3099, loss: 0.73234885931\n",
      "Checkpoint is saved\n",
      "step: 3104, loss: 0.7104383111\n",
      "step: 3109, loss: 0.638782322407\n",
      "step: 3114, loss: 0.515087485313\n",
      "step: 3119, loss: 0.629910349846\n",
      "Checkpoint is saved\n",
      "step: 3124, loss: 0.542333126068\n",
      "step: 3129, loss: 0.630114853382\n",
      "step: 3134, loss: 0.709577977657\n",
      "step: 3139, loss: 0.455544471741\n",
      "Checkpoint is saved\n",
      "step: 3144, loss: 0.584424853325\n",
      "step: 3149, loss: 0.697474718094\n",
      "step: 3154, loss: 0.616930603981\n",
      "step: 3159, loss: 0.605308353901\n",
      "Checkpoint is saved\n",
      "step: 3164, loss: 0.599463880062\n",
      "step: 3169, loss: 0.61241877079\n",
      "step: 3174, loss: 0.625474393368\n",
      "step: 3179, loss: 0.711592614651\n",
      "Checkpoint is saved\n",
      "step: 3184, loss: 0.616801142693\n",
      "step: 3189, loss: 0.665779948235\n",
      "step: 3194, loss: 0.625808119774\n",
      "step: 3199, loss: 0.488293081522\n",
      "Checkpoint is saved\n",
      "step: 3204, loss: 0.559399306774\n",
      "step: 3209, loss: 0.726774096489\n",
      "step: 3214, loss: 0.62329018116\n",
      "step: 3219, loss: 0.596416354179\n",
      "Checkpoint is saved\n",
      "step: 3224, loss: 0.519365131855\n",
      "step: 3229, loss: 0.689557135105\n",
      "step: 3234, loss: 0.478325992823\n",
      "step: 3239, loss: 0.67699944973\n",
      "Checkpoint is saved\n",
      "step: 3244, loss: 0.646912276745\n",
      "step: 3249, loss: 0.567546248436\n",
      "step: 3254, loss: 0.505832672119\n",
      "step: 3259, loss: 0.567337632179\n",
      "Checkpoint is saved\n",
      "step: 3264, loss: 0.573453009129\n",
      "step: 3269, loss: 0.69576883316\n",
      "step: 3274, loss: 0.565636217594\n",
      "step: 3279, loss: 0.657520771027\n",
      "Checkpoint is saved\n",
      "step: 3284, loss: 0.626342117786\n",
      "step: 3289, loss: 0.536698043346\n",
      "step: 3294, loss: 0.582341432571\n",
      "step: 3299, loss: 0.497841566801\n",
      "Checkpoint is saved\n",
      "step: 3304, loss: 0.592590093613\n",
      "step: 3309, loss: 0.854101061821\n",
      "step: 3314, loss: 0.581751763821\n",
      "step: 3319, loss: 0.540254056454\n",
      "Checkpoint is saved\n",
      "step: 3324, loss: 0.734742343426\n",
      "step: 3329, loss: 0.61759442091\n",
      "step: 3334, loss: 0.565793633461\n",
      "step: 3339, loss: 0.530609548092\n",
      "Checkpoint is saved\n",
      "step: 3344, loss: 0.586396872997\n",
      "step: 3349, loss: 0.583503007889\n",
      "step: 3354, loss: 0.6076567173\n",
      "step: 3359, loss: 0.52051115036\n",
      "Checkpoint is saved\n",
      "step: 3364, loss: 0.591119289398\n",
      "step: 3369, loss: 0.637212693691\n",
      "step: 3374, loss: 0.700426459312\n",
      "step: 3379, loss: 0.627433657646\n",
      "Checkpoint is saved\n",
      "step: 3384, loss: 0.63360029459\n",
      "step: 3389, loss: 0.634075462818\n",
      "step: 3394, loss: 0.822725832462\n",
      "step: 3399, loss: 0.579002022743\n",
      "Checkpoint is saved\n",
      "step: 3404, loss: 0.525582849979\n",
      "step: 3409, loss: 0.68751937151\n",
      "step: 3414, loss: 0.555678248405\n",
      "step: 3419, loss: 0.541592240334\n",
      "Checkpoint is saved\n",
      "step: 3424, loss: 0.623581528664\n",
      "step: 3429, loss: 0.701275408268\n",
      "step: 3434, loss: 0.5873234272\n",
      "step: 3439, loss: 0.542848825455\n",
      "Checkpoint is saved\n",
      "step: 3444, loss: 0.625537514687\n",
      "step: 3449, loss: 0.705772042274\n",
      "step: 3454, loss: 0.596748650074\n",
      "step: 3459, loss: 0.577585577965\n",
      "Checkpoint is saved\n",
      "step: 3464, loss: 0.638042449951\n",
      "step: 3469, loss: 0.79676669836\n",
      "step: 3474, loss: 0.467890620232\n",
      "step: 3479, loss: 0.561839044094\n",
      "Checkpoint is saved\n",
      "step: 3484, loss: 0.638221800327\n",
      "step: 3489, loss: 0.731114864349\n",
      "step: 3494, loss: 0.632292032242\n",
      "step: 3499, loss: 0.471677541733\n",
      "Checkpoint is saved\n",
      "step: 3504, loss: 0.600006341934\n",
      "step: 3509, loss: 0.466184973717\n",
      "step: 3514, loss: 0.64974039793\n",
      "step: 3519, loss: 0.604135751724\n",
      "Checkpoint is saved\n",
      "step: 3524, loss: 0.560836434364\n",
      "step: 3529, loss: 0.481830358505\n",
      "step: 3534, loss: 0.518762588501\n",
      "step: 3539, loss: 0.528302431107\n",
      "Checkpoint is saved\n",
      "step: 3544, loss: 0.534714460373\n",
      "step: 3549, loss: 0.6052120924\n",
      "step: 3554, loss: 0.645439445972\n",
      "step: 3559, loss: 0.631890058517\n",
      "Checkpoint is saved\n",
      "step: 3564, loss: 0.624253511429\n",
      "step: 3569, loss: 0.555608510971\n",
      "step: 3574, loss: 0.605427503586\n",
      "step: 3579, loss: 0.545679330826\n",
      "Checkpoint is saved\n",
      "step: 3584, loss: 0.636404573917\n",
      "step: 3589, loss: 0.661205112934\n",
      "step: 3594, loss: 0.478615909815\n",
      "step: 3599, loss: 0.582749307156\n",
      "Checkpoint is saved\n",
      "step: 3604, loss: 0.507446527481\n",
      "step: 3609, loss: 0.525205433369\n",
      "step: 3614, loss: 0.4484128654\n",
      "step: 3619, loss: 0.592746019363\n",
      "Checkpoint is saved\n",
      "step: 3624, loss: 0.49704438448\n",
      "step: 3629, loss: 0.625249564648\n",
      "step: 3634, loss: 0.654284179211\n",
      "step: 3639, loss: 0.599716603756\n",
      "Checkpoint is saved\n",
      "step: 3644, loss: 0.508358120918\n",
      "step: 3649, loss: 0.46152973175\n",
      "step: 3654, loss: 0.535654842854\n",
      "step: 3659, loss: 0.614918828011\n",
      "Checkpoint is saved\n",
      "step: 3664, loss: 0.517226397991\n",
      "step: 3669, loss: 0.612763524055\n",
      "step: 3674, loss: 0.487287670374\n",
      "step: 3679, loss: 0.520934760571\n",
      "Checkpoint is saved\n",
      "step: 3684, loss: 0.60809648037\n",
      "step: 3689, loss: 0.62522149086\n",
      "step: 3694, loss: 0.495712578297\n",
      "step: 3699, loss: 0.44783782959\n",
      "Checkpoint is saved\n",
      "step: 3704, loss: 0.58697026968\n",
      "step: 3709, loss: 0.645134449005\n",
      "step: 3714, loss: 0.711568534374\n",
      "step: 3719, loss: 0.589084208012\n",
      "Checkpoint is saved\n",
      "step: 3724, loss: 0.720985889435\n",
      "step: 3729, loss: 0.50272756815\n",
      "step: 3734, loss: 0.564787626266\n",
      "step: 3739, loss: 0.629446387291\n",
      "Checkpoint is saved\n",
      "step: 3744, loss: 0.660625100136\n",
      "step: 3749, loss: 0.558590292931\n",
      "step: 3754, loss: 0.593642771244\n",
      "step: 3759, loss: 0.485466271639\n",
      "Checkpoint is saved\n",
      "step: 3764, loss: 0.592984080315\n",
      "step: 3769, loss: 0.583789587021\n",
      "step: 3774, loss: 0.660439372063\n",
      "step: 3779, loss: 0.584276556969\n",
      "Checkpoint is saved\n",
      "step: 3784, loss: 0.656809866428\n",
      "step: 3789, loss: 0.541851401329\n",
      "step: 3794, loss: 0.412361681461\n",
      "step: 3799, loss: 0.711194992065\n",
      "Checkpoint is saved\n",
      "step: 3804, loss: 0.548779726028\n",
      "step: 3809, loss: 0.705281436443\n",
      "step: 3814, loss: 0.558316111565\n",
      "step: 3819, loss: 0.459929466248\n",
      "Checkpoint is saved\n",
      "step: 3824, loss: 0.606160402298\n",
      "step: 3829, loss: 0.532632052898\n",
      "step: 3834, loss: 0.526957452297\n",
      "step: 3839, loss: 0.594167232513\n",
      "Checkpoint is saved\n",
      "step: 3844, loss: 0.461319029331\n",
      "step: 3849, loss: 0.617717325687\n",
      "step: 3854, loss: 0.483680039644\n",
      "step: 3859, loss: 0.480403900146\n",
      "Checkpoint is saved\n",
      "step: 3864, loss: 0.515736401081\n",
      "step: 3869, loss: 0.684445023537\n",
      "step: 3874, loss: 0.51431530714\n",
      "step: 3879, loss: 0.519091069698\n",
      "Checkpoint is saved\n",
      "step: 3884, loss: 0.482767373323\n",
      "step: 3889, loss: 0.616674780846\n",
      "step: 3894, loss: 0.484657078981\n",
      "step: 3899, loss: 0.489016890526\n",
      "Checkpoint is saved\n",
      "step: 3904, loss: 0.517407238483\n",
      "step: 3909, loss: 0.601568460464\n",
      "step: 3914, loss: 0.548779368401\n",
      "step: 3919, loss: 0.601103544235\n",
      "Checkpoint is saved\n",
      "step: 3924, loss: 0.551778078079\n",
      "step: 3929, loss: 0.546084165573\n",
      "step: 3934, loss: 0.637591481209\n",
      "step: 3939, loss: 0.603528261185\n",
      "Checkpoint is saved\n",
      "step: 3944, loss: 0.475627779961\n",
      "step: 3949, loss: 0.487194895744\n",
      "step: 3954, loss: 0.556483983994\n",
      "step: 3959, loss: 0.474605590105\n",
      "Checkpoint is saved\n",
      "step: 3964, loss: 0.514266371727\n",
      "step: 3969, loss: 0.629601955414\n",
      "step: 3974, loss: 0.606746792793\n",
      "step: 3979, loss: 0.578094482422\n",
      "Checkpoint is saved\n",
      "step: 3984, loss: 0.651322722435\n",
      "step: 3989, loss: 0.525530099869\n",
      "step: 3994, loss: 0.550164759159\n",
      "step: 3999, loss: 0.628972411156\n",
      "Checkpoint is saved\n",
      "step: 4004, loss: 0.666171908379\n",
      "step: 4009, loss: 0.440595954657\n",
      "step: 4014, loss: 0.555159687996\n",
      "step: 4019, loss: 0.532981693745\n",
      "Checkpoint is saved\n",
      "step: 4024, loss: 0.538683116436\n",
      "step: 4029, loss: 0.508608937263\n",
      "step: 4034, loss: 0.535157203674\n",
      "step: 4039, loss: 0.545910358429\n",
      "Checkpoint is saved\n",
      "step: 4044, loss: 0.563717484474\n",
      "step: 4049, loss: 0.484121978283\n",
      "step: 4054, loss: 0.578169465065\n",
      "step: 4059, loss: 0.609394013882\n",
      "Checkpoint is saved\n",
      "step: 4064, loss: 0.68764102459\n",
      "step: 4069, loss: 0.562883019447\n",
      "step: 4074, loss: 0.502076566219\n",
      "step: 4079, loss: 0.500663101673\n",
      "Checkpoint is saved\n",
      "step: 4084, loss: 0.621980190277\n",
      "step: 4089, loss: 0.509328305721\n",
      "step: 4094, loss: 0.536335945129\n",
      "step: 4099, loss: 0.680524110794\n",
      "Checkpoint is saved\n",
      "step: 4104, loss: 0.443292081356\n",
      "step: 4109, loss: 0.554795145988\n",
      "step: 4114, loss: 0.588174641132\n",
      "step: 4119, loss: 0.530634522438\n",
      "Checkpoint is saved\n",
      "step: 4124, loss: 0.452589988708\n",
      "step: 4129, loss: 0.456305593252\n",
      "step: 4134, loss: 0.654004335403\n",
      "step: 4139, loss: 0.547600090504\n",
      "Checkpoint is saved\n",
      "step: 4144, loss: 0.508433699608\n",
      "step: 4149, loss: 0.509229183197\n",
      "step: 4154, loss: 0.596821010113\n",
      "step: 4159, loss: 0.482477873564\n",
      "Checkpoint is saved\n",
      "step: 4164, loss: 0.504045307636\n",
      "step: 4169, loss: 0.484582751989\n",
      "step: 4174, loss: 0.515460848808\n",
      "step: 4179, loss: 0.586041688919\n",
      "Checkpoint is saved\n",
      "step: 4184, loss: 0.642253279686\n",
      "step: 4189, loss: 0.504706740379\n",
      "step: 4194, loss: 0.58910793066\n",
      "step: 4199, loss: 0.572956502438\n",
      "Checkpoint is saved\n",
      "step: 4204, loss: 0.564613461494\n",
      "step: 4209, loss: 0.504695057869\n",
      "step: 4214, loss: 0.577812671661\n",
      "step: 4219, loss: 0.621837437153\n",
      "Checkpoint is saved\n",
      "step: 4224, loss: 0.533802807331\n",
      "step: 4229, loss: 0.599321007729\n",
      "step: 4234, loss: 0.684809684753\n",
      "step: 4239, loss: 0.588649630547\n",
      "Checkpoint is saved\n",
      "step: 4244, loss: 0.582879602909\n",
      "step: 4249, loss: 0.434447705746\n",
      "step: 4254, loss: 0.581816792488\n",
      "step: 4259, loss: 0.565411031246\n",
      "Checkpoint is saved\n",
      "step: 4264, loss: 0.57717192173\n",
      "step: 4269, loss: 0.545211791992\n",
      "step: 4274, loss: 0.51965123415\n",
      "step: 4279, loss: 0.604581713676\n",
      "Checkpoint is saved\n",
      "step: 4284, loss: 0.536251783371\n",
      "step: 4289, loss: 0.470836222172\n",
      "step: 4294, loss: 0.613956451416\n",
      "step: 4299, loss: 0.428412795067\n",
      "Checkpoint is saved\n",
      "step: 4304, loss: 0.514105021954\n",
      "step: 4309, loss: 0.602155685425\n",
      "step: 4314, loss: 0.563334703445\n",
      "step: 4319, loss: 0.490066826344\n",
      "Checkpoint is saved\n",
      "step: 4324, loss: 0.686709046364\n",
      "step: 4329, loss: 0.54779368639\n",
      "step: 4334, loss: 0.5812073946\n",
      "step: 4339, loss: 0.630556106567\n",
      "Checkpoint is saved\n",
      "step: 4344, loss: 0.58577644825\n",
      "step: 4349, loss: 0.478419452906\n",
      "step: 4354, loss: 0.619799256325\n",
      "step: 4359, loss: 0.539357185364\n",
      "Checkpoint is saved\n",
      "step: 4364, loss: 0.351353555918\n",
      "step: 4369, loss: 0.556722998619\n",
      "step: 4374, loss: 0.537204027176\n",
      "step: 4379, loss: 0.629418134689\n",
      "Checkpoint is saved\n",
      "step: 4384, loss: 0.507024168968\n",
      "step: 4389, loss: 0.511319041252\n",
      "step: 4394, loss: 0.598679065704\n",
      "step: 4399, loss: 0.534916698933\n",
      "Checkpoint is saved\n",
      "step: 4404, loss: 0.692923426628\n",
      "step: 4409, loss: 0.457616299391\n",
      "step: 4414, loss: 0.590669989586\n",
      "step: 4419, loss: 0.500226736069\n",
      "Checkpoint is saved\n",
      "step: 4424, loss: 0.563731431961\n",
      "step: 4429, loss: 0.675260126591\n",
      "step: 4434, loss: 0.542816221714\n",
      "step: 4439, loss: 0.50754070282\n",
      "Checkpoint is saved\n",
      "step: 4444, loss: 0.552223145962\n",
      "step: 4449, loss: 0.529585123062\n",
      "step: 4454, loss: 0.503986001015\n",
      "step: 4459, loss: 0.573647081852\n",
      "Checkpoint is saved\n",
      "step: 4464, loss: 0.635457992554\n",
      "step: 4469, loss: 0.613058209419\n",
      "step: 4474, loss: 0.522189736366\n",
      "step: 4479, loss: 0.556297242641\n",
      "Checkpoint is saved\n",
      "step: 4484, loss: 0.580749511719\n",
      "step: 4489, loss: 0.484437137842\n",
      "step: 4494, loss: 0.529409646988\n",
      "step: 4499, loss: 0.630278170109\n",
      "Checkpoint is saved\n",
      "step: 4504, loss: 0.52177041769\n",
      "step: 4509, loss: 0.531188249588\n",
      "step: 4514, loss: 0.525175333023\n",
      "step: 4519, loss: 0.504241466522\n",
      "Checkpoint is saved\n",
      "step: 4524, loss: 0.504702329636\n",
      "step: 4529, loss: 0.482979208231\n",
      "step: 4534, loss: 0.561420977116\n",
      "step: 4539, loss: 0.462540984154\n",
      "Checkpoint is saved\n",
      "step: 4544, loss: 0.561065733433\n",
      "step: 4549, loss: 0.488334566355\n",
      "step: 4554, loss: 0.499515414238\n",
      "step: 4559, loss: 0.395691037178\n",
      "Checkpoint is saved\n",
      "step: 4564, loss: 0.574407637119\n",
      "step: 4569, loss: 0.527195692062\n",
      "step: 4574, loss: 0.585466265678\n",
      "step: 4579, loss: 0.568842291832\n",
      "Checkpoint is saved\n",
      "step: 4584, loss: 0.555664896965\n",
      "step: 4589, loss: 0.584633231163\n",
      "step: 4594, loss: 0.516030192375\n",
      "step: 4599, loss: 0.504531502724\n",
      "Checkpoint is saved\n",
      "step: 4604, loss: 0.629883229733\n",
      "step: 4609, loss: 0.58491897583\n",
      "step: 4614, loss: 0.421884924173\n",
      "step: 4619, loss: 0.550007462502\n",
      "Checkpoint is saved\n",
      "step: 4624, loss: 0.599945068359\n",
      "step: 4629, loss: 0.35604852438\n",
      "step: 4634, loss: 0.46263808012\n",
      "step: 4639, loss: 0.520191788673\n",
      "Checkpoint is saved\n",
      "step: 4644, loss: 0.519855797291\n",
      "step: 4649, loss: 0.53305208683\n",
      "step: 4654, loss: 0.578996539116\n",
      "step: 4659, loss: 0.502804577351\n",
      "Checkpoint is saved\n",
      "step: 4664, loss: 0.436937332153\n",
      "step: 4669, loss: 0.532366335392\n",
      "step: 4674, loss: 0.556310653687\n",
      "step: 4679, loss: 0.614431262016\n",
      "Checkpoint is saved\n",
      "step: 4684, loss: 0.558052778244\n",
      "step: 4689, loss: 0.442351281643\n",
      "step: 4694, loss: 0.475973963737\n",
      "step: 4699, loss: 0.589945435524\n",
      "Checkpoint is saved\n",
      "step: 4704, loss: 0.530159175396\n",
      "step: 4709, loss: 0.494911670685\n",
      "step: 4714, loss: 0.583758592606\n",
      "step: 4719, loss: 0.506094455719\n",
      "Checkpoint is saved\n",
      "step: 4724, loss: 0.660904407501\n",
      "step: 4729, loss: 0.466753661633\n",
      "step: 4734, loss: 0.498804152012\n",
      "step: 4739, loss: 0.627263963223\n",
      "Checkpoint is saved\n",
      "step: 4744, loss: 0.500405192375\n",
      "step: 4749, loss: 0.540006816387\n",
      "step: 4754, loss: 0.571380853653\n",
      "step: 4759, loss: 0.467318445444\n",
      "Checkpoint is saved\n",
      "step: 4764, loss: 0.477950245142\n",
      "step: 4769, loss: 0.519060432911\n",
      "step: 4774, loss: 0.475123852491\n",
      "step: 4779, loss: 0.598144054413\n",
      "Checkpoint is saved\n",
      "step: 4784, loss: 0.463172256947\n",
      "step: 4789, loss: 0.616316080093\n",
      "step: 4794, loss: 0.552415847778\n",
      "step: 4799, loss: 0.732084929943\n",
      "Checkpoint is saved\n",
      "step: 4804, loss: 0.535002708435\n",
      "step: 4809, loss: 0.701836585999\n",
      "step: 4814, loss: 0.490845263004\n",
      "step: 4819, loss: 0.542695820332\n",
      "Checkpoint is saved\n",
      "step: 4824, loss: 0.497942596674\n",
      "step: 4829, loss: 0.528853297234\n",
      "step: 4834, loss: 0.498700797558\n",
      "step: 4839, loss: 0.512240588665\n",
      "Checkpoint is saved\n",
      "step: 4844, loss: 0.470825374126\n",
      "step: 4849, loss: 0.525057673454\n",
      "step: 4854, loss: 0.45518130064\n",
      "step: 4859, loss: 0.544790387154\n",
      "Checkpoint is saved\n",
      "step: 4864, loss: 0.540248095989\n",
      "step: 4869, loss: 0.511191010475\n",
      "step: 4874, loss: 0.578793048859\n",
      "step: 4879, loss: 0.482852876186\n",
      "Checkpoint is saved\n",
      "step: 4884, loss: 0.478299945593\n",
      "step: 4889, loss: 0.614404141903\n",
      "step: 4894, loss: 0.524412512779\n",
      "step: 4899, loss: 0.579735517502\n",
      "Checkpoint is saved\n",
      "step: 4904, loss: 0.583057045937\n",
      "step: 4909, loss: 0.544482111931\n",
      "step: 4914, loss: 0.422835886478\n",
      "step: 4919, loss: 0.482456982136\n",
      "Checkpoint is saved\n",
      "step: 4924, loss: 0.558076679707\n",
      "step: 4929, loss: 0.465048134327\n",
      "step: 4934, loss: 0.574888467789\n",
      "step: 4939, loss: 0.455323398113\n",
      "Checkpoint is saved\n",
      "step: 4944, loss: 0.557671308517\n",
      "step: 4949, loss: 0.566533684731\n",
      "step: 4954, loss: 0.554050087929\n",
      "step: 4959, loss: 0.612011611462\n",
      "Checkpoint is saved\n",
      "step: 4964, loss: 0.491936773062\n",
      "step: 4969, loss: 0.54177737236\n",
      "step: 4974, loss: 0.512883424759\n",
      "step: 4979, loss: 0.499632656574\n",
      "Checkpoint is saved\n",
      "step: 4984, loss: 0.575809597969\n",
      "step: 4989, loss: 0.485796928406\n",
      "step: 4994, loss: 0.527940750122\n",
      "step: 4999, loss: 0.596358537674\n",
      "Checkpoint is saved\n",
      "step: 5004, loss: 0.400872468948\n",
      "step: 5009, loss: 0.451931029558\n",
      "step: 5014, loss: 0.617752611637\n",
      "step: 5019, loss: 0.518482029438\n",
      "Checkpoint is saved\n",
      "step: 5024, loss: 0.552311062813\n",
      "step: 5029, loss: 0.543375730515\n",
      "step: 5034, loss: 0.540950775146\n",
      "step: 5039, loss: 0.517158150673\n",
      "Checkpoint is saved\n",
      "step: 5044, loss: 0.556435644627\n",
      "step: 5049, loss: 0.516642093658\n",
      "step: 5054, loss: 0.563926994801\n",
      "step: 5059, loss: 0.598600685596\n",
      "Checkpoint is saved\n",
      "step: 5064, loss: 0.500459372997\n",
      "step: 5069, loss: 0.494380146265\n",
      "step: 5074, loss: 0.558828353882\n",
      "step: 5079, loss: 0.558777570724\n",
      "Checkpoint is saved\n",
      "step: 5084, loss: 0.525521099567\n",
      "step: 5089, loss: 0.520999372005\n",
      "step: 5094, loss: 0.559071958065\n",
      "step: 5099, loss: 0.542838394642\n",
      "Checkpoint is saved\n",
      "step: 5104, loss: 0.469355374575\n",
      "step: 5109, loss: 0.409502595663\n",
      "step: 5114, loss: 0.579894065857\n",
      "step: 5119, loss: 0.459655433893\n",
      "Checkpoint is saved\n",
      "step: 5124, loss: 0.521796941757\n",
      "step: 5129, loss: 0.467183828354\n",
      "step: 5134, loss: 0.534398555756\n",
      "step: 5139, loss: 0.43627473712\n",
      "Checkpoint is saved\n",
      "step: 5144, loss: 0.415327131748\n",
      "step: 5149, loss: 0.534409105778\n",
      "step: 5154, loss: 0.48413798213\n",
      "step: 5159, loss: 0.504504859447\n",
      "Checkpoint is saved\n",
      "step: 5164, loss: 0.614859580994\n",
      "step: 5169, loss: 0.535071909428\n",
      "step: 5174, loss: 0.565588951111\n",
      "step: 5179, loss: 0.388264805079\n",
      "Checkpoint is saved\n",
      "step: 5184, loss: 0.368265748024\n",
      "step: 5189, loss: 0.412922501564\n",
      "step: 5194, loss: 0.457280695438\n",
      "step: 5199, loss: 0.584601581097\n",
      "Checkpoint is saved\n",
      "step: 5204, loss: 0.485931932926\n",
      "step: 5209, loss: 0.56939136982\n",
      "step: 5214, loss: 0.474604666233\n",
      "step: 5219, loss: 0.47915494442\n",
      "Checkpoint is saved\n",
      "step: 5224, loss: 0.479466885328\n",
      "step: 5229, loss: 0.582891285419\n",
      "step: 5234, loss: 0.532015502453\n",
      "step: 5239, loss: 0.415356040001\n",
      "Checkpoint is saved\n",
      "step: 5244, loss: 0.462630987167\n",
      "step: 5249, loss: 0.640805482864\n",
      "step: 5254, loss: 0.551902830601\n",
      "step: 5259, loss: 0.481522232294\n",
      "Checkpoint is saved\n",
      "step: 5264, loss: 0.462424069643\n",
      "step: 5269, loss: 0.475106447935\n",
      "step: 5274, loss: 0.530309677124\n",
      "step: 5279, loss: 0.352807104588\n",
      "Checkpoint is saved\n",
      "step: 5284, loss: 0.577475428581\n",
      "step: 5289, loss: 0.525457978249\n",
      "step: 5294, loss: 0.619024395943\n",
      "step: 5299, loss: 0.461042016745\n",
      "Checkpoint is saved\n",
      "step: 5304, loss: 0.500453472137\n",
      "step: 5309, loss: 0.496715158224\n",
      "step: 5314, loss: 0.476057231426\n",
      "step: 5319, loss: 0.487725794315\n",
      "Checkpoint is saved\n",
      "step: 5324, loss: 0.485177129507\n",
      "step: 5329, loss: 0.460671663284\n",
      "step: 5334, loss: 0.484072059393\n",
      "step: 5339, loss: 0.445744931698\n",
      "Checkpoint is saved\n",
      "step: 5344, loss: 0.41533640027\n",
      "step: 5349, loss: 0.462311387062\n",
      "step: 5354, loss: 0.625122666359\n",
      "step: 5359, loss: 0.344898581505\n",
      "Checkpoint is saved\n",
      "step: 5364, loss: 0.368795990944\n",
      "step: 5369, loss: 0.551332473755\n",
      "step: 5374, loss: 0.583487153053\n",
      "step: 5379, loss: 0.640800774097\n",
      "Checkpoint is saved\n",
      "step: 5384, loss: 0.558358311653\n",
      "step: 5389, loss: 0.464641302824\n",
      "step: 5394, loss: 0.513473749161\n",
      "step: 5399, loss: 0.534946799278\n",
      "Checkpoint is saved\n",
      "step: 5404, loss: 0.446223348379\n",
      "step: 5409, loss: 0.565799593925\n",
      "step: 5414, loss: 0.579157650471\n",
      "step: 5419, loss: 0.508344531059\n",
      "Checkpoint is saved\n",
      "step: 5424, loss: 0.405910462141\n",
      "step: 5429, loss: 0.506604194641\n",
      "step: 5434, loss: 0.522147774696\n",
      "step: 5439, loss: 0.43272420764\n",
      "Checkpoint is saved\n",
      "step: 5444, loss: 0.454601377249\n",
      "step: 5449, loss: 0.618062674999\n",
      "step: 5454, loss: 0.601122379303\n",
      "step: 5459, loss: 0.567963242531\n",
      "Checkpoint is saved\n",
      "step: 5464, loss: 0.550101161003\n",
      "step: 5469, loss: 0.446834653616\n",
      "step: 5474, loss: 0.436565816402\n",
      "step: 5479, loss: 0.573087573051\n",
      "Checkpoint is saved\n",
      "step: 5484, loss: 0.488164931536\n",
      "step: 5489, loss: 0.457179039717\n",
      "step: 5494, loss: 0.568781375885\n",
      "step: 5499, loss: 0.568400502205\n",
      "Checkpoint is saved\n",
      "step: 5504, loss: 0.490350931883\n",
      "step: 5509, loss: 0.532548069954\n",
      "step: 5514, loss: 0.582197189331\n",
      "step: 5519, loss: 0.445058524609\n",
      "Checkpoint is saved\n",
      "step: 5524, loss: 0.43901309371\n",
      "step: 5529, loss: 0.417850285769\n",
      "step: 5534, loss: 0.49202299118\n",
      "step: 5539, loss: 0.609874069691\n",
      "Checkpoint is saved\n",
      "step: 5544, loss: 0.491786122322\n",
      "step: 5549, loss: 0.505090534687\n",
      "step: 5554, loss: 0.435889899731\n",
      "step: 5559, loss: 0.465703815222\n",
      "Checkpoint is saved\n",
      "step: 5564, loss: 0.504438042641\n",
      "step: 5569, loss: 0.605024278164\n",
      "step: 5574, loss: 0.552139341831\n",
      "step: 5579, loss: 0.474326521158\n",
      "Checkpoint is saved\n",
      "step: 5584, loss: 0.562245309353\n",
      "step: 5589, loss: 0.493326425552\n",
      "step: 5594, loss: 0.405051469803\n",
      "step: 5599, loss: 0.521752119064\n",
      "Checkpoint is saved\n",
      "step: 5604, loss: 0.508360266685\n",
      "step: 5609, loss: 0.469542145729\n",
      "step: 5614, loss: 0.543038606644\n",
      "step: 5619, loss: 0.499844461679\n",
      "Checkpoint is saved\n",
      "step: 5624, loss: 0.403012186289\n",
      "step: 5629, loss: 0.419250249863\n",
      "step: 5634, loss: 0.59649348259\n",
      "step: 5639, loss: 0.535079658031\n",
      "Checkpoint is saved\n",
      "step: 5644, loss: 0.556386470795\n",
      "step: 5649, loss: 0.369006365538\n",
      "step: 5654, loss: 0.4791046381\n",
      "step: 5659, loss: 0.51118093729\n",
      "Checkpoint is saved\n",
      "step: 5664, loss: 0.593616127968\n",
      "step: 5669, loss: 0.482272714376\n",
      "step: 5674, loss: 0.516709923744\n",
      "step: 5679, loss: 0.58147084713\n",
      "Checkpoint is saved\n",
      "step: 5684, loss: 0.520561397076\n",
      "step: 5689, loss: 0.510032236576\n",
      "step: 5694, loss: 0.522969901562\n",
      "step: 5699, loss: 0.505987524986\n",
      "Checkpoint is saved\n",
      "step: 5704, loss: 0.390001177788\n",
      "step: 5709, loss: 0.570730566978\n",
      "step: 5714, loss: 0.480238050222\n",
      "step: 5719, loss: 0.537801384926\n",
      "Checkpoint is saved\n",
      "step: 5724, loss: 0.430684685707\n",
      "step: 5729, loss: 0.530752897263\n",
      "step: 5734, loss: 0.503742456436\n",
      "step: 5739, loss: 0.507163107395\n",
      "Checkpoint is saved\n",
      "step: 5744, loss: 0.521377503872\n",
      "step: 5749, loss: 0.569895446301\n",
      "step: 5754, loss: 0.515213012695\n",
      "step: 5759, loss: 0.417210191488\n",
      "Checkpoint is saved\n",
      "step: 5764, loss: 0.665747642517\n",
      "step: 5769, loss: 0.687533736229\n",
      "step: 5774, loss: 0.487230807543\n",
      "step: 5779, loss: 0.456143140793\n",
      "Checkpoint is saved\n",
      "step: 5784, loss: 0.569997668266\n",
      "step: 5789, loss: 0.476770192385\n",
      "step: 5794, loss: 0.447598874569\n",
      "step: 5799, loss: 0.504599571228\n",
      "Checkpoint is saved\n",
      "step: 5804, loss: 0.391428530216\n",
      "step: 5809, loss: 0.579389691353\n",
      "step: 5814, loss: 0.377445936203\n",
      "step: 5819, loss: 0.536828219891\n",
      "Checkpoint is saved\n",
      "step: 5824, loss: 0.567539930344\n",
      "step: 5829, loss: 0.517556905746\n",
      "step: 5834, loss: 0.484244257212\n",
      "step: 5839, loss: 0.515723049641\n",
      "Checkpoint is saved\n",
      "step: 5844, loss: 0.525399029255\n",
      "step: 5849, loss: 0.513970136642\n",
      "step: 5854, loss: 0.546689450741\n",
      "step: 5859, loss: 0.501332163811\n",
      "Checkpoint is saved\n",
      "step: 5864, loss: 0.451409220695\n",
      "step: 5869, loss: 0.4694891572\n",
      "step: 5874, loss: 0.531826078892\n",
      "step: 5879, loss: 0.610463023186\n",
      "Checkpoint is saved\n",
      "step: 5884, loss: 0.583408534527\n",
      "step: 5889, loss: 0.69511115551\n",
      "step: 5894, loss: 0.477050453424\n",
      "step: 5899, loss: 0.466158598661\n",
      "Checkpoint is saved\n",
      "step: 5904, loss: 0.505173325539\n",
      "step: 5909, loss: 0.530448615551\n",
      "step: 5914, loss: 0.400551736355\n",
      "step: 5919, loss: 0.576797366142\n",
      "Checkpoint is saved\n",
      "step: 5924, loss: 0.52137118578\n",
      "step: 5929, loss: 0.66781437397\n",
      "step: 5934, loss: 0.482960611582\n",
      "step: 5939, loss: 0.488850086927\n",
      "Checkpoint is saved\n",
      "step: 5944, loss: 0.460662424564\n",
      "step: 5949, loss: 0.508984684944\n",
      "step: 5954, loss: 0.488845765591\n",
      "step: 5959, loss: 0.577165961266\n",
      "Checkpoint is saved\n",
      "step: 5964, loss: 0.394362360239\n",
      "step: 5969, loss: 0.455694943666\n",
      "step: 5974, loss: 0.498869985342\n",
      "step: 5979, loss: 0.431069582701\n",
      "Checkpoint is saved\n",
      "step: 5984, loss: 0.416277587414\n",
      "step: 5989, loss: 0.420299530029\n",
      "step: 5994, loss: 0.536196470261\n",
      "step: 5999, loss: 0.401425093412\n",
      "Checkpoint is saved\n",
      "step: 6004, loss: 0.542736053467\n",
      "step: 6009, loss: 0.429256170988\n",
      "step: 6014, loss: 0.418247550726\n",
      "step: 6019, loss: 0.48007017374\n",
      "Checkpoint is saved\n",
      "step: 6024, loss: 0.507270336151\n",
      "step: 6029, loss: 0.594808578491\n",
      "step: 6034, loss: 0.463287353516\n",
      "step: 6039, loss: 0.361366450787\n",
      "Checkpoint is saved\n",
      "step: 6044, loss: 0.445360273123\n",
      "step: 6049, loss: 0.530415058136\n",
      "step: 6054, loss: 0.506963253021\n",
      "step: 6059, loss: 0.54406607151\n",
      "Checkpoint is saved\n",
      "step: 6064, loss: 0.416816174984\n",
      "step: 6069, loss: 0.458100497723\n",
      "step: 6074, loss: 0.554021835327\n",
      "step: 6079, loss: 0.553354501724\n",
      "Checkpoint is saved\n",
      "step: 6084, loss: 0.387943804264\n",
      "step: 6089, loss: 0.522491812706\n",
      "step: 6094, loss: 0.459090769291\n",
      "step: 6099, loss: 0.521636545658\n",
      "Checkpoint is saved\n",
      "step: 6104, loss: 0.505219578743\n",
      "step: 6109, loss: 0.487559527159\n",
      "step: 6114, loss: 0.494083434343\n",
      "step: 6119, loss: 0.430924028158\n",
      "Checkpoint is saved\n",
      "step: 6124, loss: 0.470792651176\n",
      "step: 6129, loss: 0.493011295795\n",
      "step: 6134, loss: 0.490003615618\n",
      "step: 6139, loss: 0.511569857597\n",
      "Checkpoint is saved\n",
      "step: 6144, loss: 0.431720048189\n",
      "step: 6149, loss: 0.353988528252\n",
      "step: 6154, loss: 0.412588536739\n",
      "step: 6159, loss: 0.461821377277\n",
      "Checkpoint is saved\n",
      "step: 6164, loss: 0.505989551544\n",
      "step: 6169, loss: 0.550658941269\n",
      "step: 6174, loss: 0.572452545166\n",
      "step: 6179, loss: 0.528450965881\n",
      "Checkpoint is saved\n",
      "step: 6184, loss: 0.577559769154\n",
      "step: 6189, loss: 0.509106040001\n",
      "step: 6194, loss: 0.496786862612\n",
      "step: 6199, loss: 0.393118709326\n",
      "Checkpoint is saved\n",
      "step: 6204, loss: 0.536895096302\n",
      "step: 6209, loss: 0.48066920042\n",
      "step: 6214, loss: 0.457569271326\n",
      "step: 6219, loss: 0.498211741447\n",
      "Checkpoint is saved\n",
      "step: 6224, loss: 0.458884894848\n",
      "step: 6229, loss: 0.438709825277\n",
      "step: 6234, loss: 0.551843762398\n",
      "step: 6239, loss: 0.561432182789\n",
      "Checkpoint is saved\n",
      "step: 6244, loss: 0.498421490192\n",
      "step: 6249, loss: 0.479986220598\n",
      "step: 6254, loss: 0.601504743099\n",
      "step: 6259, loss: 0.42333060503\n",
      "Checkpoint is saved\n",
      "step: 6264, loss: 0.546624422073\n",
      "step: 6269, loss: 0.46188339591\n",
      "step: 6274, loss: 0.451481640339\n",
      "step: 6279, loss: 0.597428560257\n",
      "Checkpoint is saved\n",
      "step: 6284, loss: 0.57131934166\n",
      "step: 6289, loss: 0.573298454285\n",
      "step: 6294, loss: 0.598058700562\n",
      "step: 6299, loss: 0.418315410614\n",
      "Checkpoint is saved\n",
      "step: 6304, loss: 0.675003051758\n",
      "step: 6309, loss: 0.460526764393\n",
      "step: 6314, loss: 0.463226437569\n",
      "step: 6319, loss: 0.445449113846\n",
      "Checkpoint is saved\n",
      "step: 6324, loss: 0.578818798065\n",
      "step: 6329, loss: 0.537609696388\n",
      "step: 6334, loss: 0.546840071678\n",
      "step: 6339, loss: 0.566584706306\n",
      "Checkpoint is saved\n",
      "step: 6344, loss: 0.471076190472\n",
      "step: 6349, loss: 0.373836815357\n",
      "step: 6354, loss: 0.538680791855\n",
      "step: 6359, loss: 0.495389044285\n",
      "Checkpoint is saved\n",
      "step: 6364, loss: 0.523583173752\n",
      "step: 6369, loss: 0.51534640789\n",
      "step: 6374, loss: 0.395175367594\n",
      "step: 6379, loss: 0.448223829269\n",
      "Checkpoint is saved\n",
      "step: 6384, loss: 0.57692527771\n",
      "step: 6389, loss: 0.597316384315\n",
      "step: 6394, loss: 0.501718401909\n",
      "step: 6399, loss: 0.50025677681\n",
      "Checkpoint is saved\n",
      "step: 6404, loss: 0.490336805582\n",
      "step: 6409, loss: 0.496287435293\n",
      "step: 6414, loss: 0.508044421673\n",
      "step: 6419, loss: 0.464770972729\n",
      "Checkpoint is saved\n",
      "step: 6424, loss: 0.500937461853\n",
      "step: 6429, loss: 0.462332159281\n",
      "step: 6434, loss: 0.517138063908\n",
      "step: 6439, loss: 0.514054000378\n",
      "Checkpoint is saved\n",
      "step: 6444, loss: 0.541638016701\n",
      "step: 6449, loss: 0.482942640781\n",
      "step: 6454, loss: 0.491230517626\n",
      "step: 6459, loss: 0.459439575672\n",
      "Checkpoint is saved\n",
      "step: 6464, loss: 0.496182918549\n",
      "step: 6469, loss: 0.430516600609\n",
      "step: 6474, loss: 0.423084318638\n",
      "step: 6479, loss: 0.526607513428\n",
      "Checkpoint is saved\n",
      "step: 6484, loss: 0.398754239082\n",
      "step: 6489, loss: 0.517667770386\n",
      "step: 6494, loss: 0.403357535601\n",
      "step: 6499, loss: 0.49598929286\n",
      "Checkpoint is saved\n",
      "step: 6504, loss: 0.57443344593\n",
      "step: 6509, loss: 0.402409136295\n",
      "step: 6514, loss: 0.50885540247\n",
      "step: 6519, loss: 0.528835356236\n",
      "Checkpoint is saved\n",
      "step: 6524, loss: 0.330496251583\n",
      "step: 6529, loss: 0.404679447412\n",
      "step: 6534, loss: 0.411048144102\n",
      "step: 6539, loss: 0.530735254288\n",
      "Checkpoint is saved\n",
      "step: 6544, loss: 0.60383272171\n",
      "step: 6549, loss: 0.449326872826\n",
      "step: 6554, loss: 0.503591239452\n",
      "step: 6559, loss: 0.416662216187\n",
      "Checkpoint is saved\n",
      "step: 6564, loss: 0.607576429844\n",
      "step: 6569, loss: 0.42917573452\n",
      "step: 6574, loss: 0.577326595783\n",
      "step: 6579, loss: 0.313479840755\n",
      "Checkpoint is saved\n",
      "step: 6584, loss: 0.425686210394\n",
      "step: 6589, loss: 0.49281424284\n",
      "step: 6594, loss: 0.451560199261\n",
      "step: 6599, loss: 0.556915998459\n",
      "Checkpoint is saved\n",
      "step: 6604, loss: 0.498184561729\n",
      "step: 6609, loss: 0.479907810688\n",
      "step: 6614, loss: 0.439491271973\n",
      "step: 6619, loss: 0.427080512047\n",
      "Checkpoint is saved\n",
      "step: 6624, loss: 0.492769241333\n",
      "step: 6629, loss: 0.449422061443\n",
      "step: 6634, loss: 0.458109676838\n",
      "step: 6639, loss: 0.48895907402\n",
      "Checkpoint is saved\n",
      "step: 6644, loss: 0.441641449928\n",
      "step: 6649, loss: 0.476477086544\n",
      "step: 6654, loss: 0.349899142981\n",
      "step: 6659, loss: 0.493293970823\n",
      "Checkpoint is saved\n",
      "step: 6664, loss: 0.487842559814\n",
      "step: 6669, loss: 0.576534986496\n",
      "step: 6674, loss: 0.624599814415\n",
      "step: 6679, loss: 0.373555183411\n",
      "Checkpoint is saved\n",
      "step: 6684, loss: 0.449139714241\n",
      "step: 6689, loss: 0.520349919796\n",
      "step: 6694, loss: 0.515999734402\n",
      "step: 6699, loss: 0.482697308064\n",
      "Checkpoint is saved\n",
      "step: 6704, loss: 0.480830878019\n",
      "step: 6709, loss: 0.432467848063\n",
      "step: 6714, loss: 0.489402651787\n",
      "step: 6719, loss: 0.439686655998\n",
      "Checkpoint is saved\n",
      "step: 6724, loss: 0.488299369812\n",
      "step: 6729, loss: 0.56079852581\n",
      "step: 6734, loss: 0.558292627335\n",
      "step: 6739, loss: 0.463917255402\n",
      "Checkpoint is saved\n",
      "step: 6744, loss: 0.486596375704\n",
      "step: 6749, loss: 0.473392605782\n",
      "step: 6754, loss: 0.49663066864\n",
      "step: 6759, loss: 0.506708264351\n",
      "Checkpoint is saved\n",
      "step: 6764, loss: 0.352177798748\n",
      "step: 6769, loss: 0.50405818224\n",
      "step: 6774, loss: 0.414154648781\n",
      "step: 6779, loss: 0.366352915764\n",
      "Checkpoint is saved\n",
      "step: 6784, loss: 0.495746076107\n",
      "step: 6789, loss: 0.510037243366\n",
      "step: 6794, loss: 0.598050415516\n",
      "step: 6799, loss: 0.608757317066\n",
      "Checkpoint is saved\n",
      "step: 6804, loss: 0.544896483421\n",
      "step: 6809, loss: 0.3739554286\n",
      "step: 6814, loss: 0.372740745544\n",
      "step: 6819, loss: 0.527631700039\n",
      "Checkpoint is saved\n",
      "step: 6824, loss: 0.473236262798\n",
      "step: 6829, loss: 0.448536396027\n",
      "step: 6834, loss: 0.67053258419\n",
      "step: 6839, loss: 0.420530438423\n",
      "Checkpoint is saved\n",
      "step: 6844, loss: 0.500805854797\n",
      "step: 6849, loss: 0.483802497387\n",
      "step: 6854, loss: 0.391467601061\n",
      "step: 6859, loss: 0.498679131269\n",
      "Checkpoint is saved\n",
      "step: 6864, loss: 0.485026746988\n",
      "step: 6869, loss: 0.442865908146\n",
      "step: 6874, loss: 0.432348251343\n",
      "step: 6879, loss: 0.551913976669\n",
      "Checkpoint is saved\n",
      "step: 6884, loss: 0.499056279659\n",
      "step: 6889, loss: 0.519926011562\n",
      "step: 6894, loss: 0.577942073345\n",
      "step: 6899, loss: 0.515328407288\n",
      "Checkpoint is saved\n",
      "step: 6904, loss: 0.49421453476\n",
      "step: 6909, loss: 0.457956939936\n",
      "step: 6914, loss: 0.467517495155\n",
      "step: 6919, loss: 0.457801729441\n",
      "Checkpoint is saved\n",
      "step: 6924, loss: 0.461674332619\n",
      "step: 6929, loss: 0.388265252113\n",
      "step: 6934, loss: 0.511079251766\n",
      "step: 6939, loss: 0.426999866962\n",
      "Checkpoint is saved\n",
      "step: 6944, loss: 0.464445114136\n",
      "step: 6949, loss: 0.515809237957\n",
      "step: 6954, loss: 0.493995428085\n",
      "step: 6959, loss: 0.529524147511\n",
      "Checkpoint is saved\n",
      "step: 6964, loss: 0.491346180439\n",
      "step: 6969, loss: 0.519925832748\n",
      "step: 6974, loss: 0.49012619257\n",
      "step: 6979, loss: 0.397323787212\n",
      "Checkpoint is saved\n",
      "step: 6984, loss: 0.529383540154\n",
      "step: 6989, loss: 0.444844871759\n",
      "step: 6994, loss: 0.482132971287\n",
      "step: 6999, loss: 0.482825607061\n",
      "Checkpoint is saved\n",
      "step: 7004, loss: 0.403110802174\n",
      "step: 7009, loss: 0.409651637077\n",
      "step: 7014, loss: 0.530223011971\n",
      "step: 7019, loss: 0.52311873436\n",
      "Checkpoint is saved\n",
      "step: 7024, loss: 0.448034286499\n",
      "step: 7029, loss: 0.480272948742\n",
      "step: 7034, loss: 0.491610348225\n",
      "step: 7039, loss: 0.496276885271\n",
      "Checkpoint is saved\n",
      "step: 7044, loss: 0.461937308311\n",
      "step: 7049, loss: 0.457256019115\n",
      "step: 7054, loss: 0.484872192144\n",
      "step: 7059, loss: 0.482759237289\n",
      "Checkpoint is saved\n",
      "step: 7064, loss: 0.470540851355\n",
      "step: 7069, loss: 0.420685589314\n",
      "step: 7074, loss: 0.472655653954\n",
      "step: 7079, loss: 0.471292406321\n",
      "Checkpoint is saved\n",
      "step: 7084, loss: 0.492458939552\n",
      "step: 7089, loss: 0.565616309643\n",
      "step: 7094, loss: 0.368584722281\n",
      "step: 7099, loss: 0.507002174854\n",
      "Checkpoint is saved\n",
      "step: 7104, loss: 0.427536547184\n",
      "step: 7109, loss: 0.502042531967\n",
      "step: 7114, loss: 0.478359341621\n",
      "step: 7119, loss: 0.474650919437\n",
      "Checkpoint is saved\n",
      "step: 7124, loss: 0.532977461815\n",
      "step: 7129, loss: 0.470865577459\n",
      "step: 7134, loss: 0.515535593033\n",
      "step: 7139, loss: 0.491317451\n",
      "Checkpoint is saved\n",
      "step: 7144, loss: 0.484806507826\n",
      "step: 7149, loss: 0.379467368126\n",
      "step: 7154, loss: 0.515345573425\n",
      "step: 7159, loss: 0.506803095341\n",
      "Checkpoint is saved\n",
      "step: 7164, loss: 0.429096341133\n",
      "step: 7169, loss: 0.375885874033\n",
      "step: 7174, loss: 0.446212053299\n",
      "step: 7179, loss: 0.38057756424\n",
      "Checkpoint is saved\n",
      "step: 7184, loss: 0.522429168224\n",
      "step: 7189, loss: 0.573501348495\n",
      "step: 7194, loss: 0.48344796896\n",
      "step: 7199, loss: 0.47439545393\n",
      "Checkpoint is saved\n",
      "step: 7204, loss: 0.471182763577\n",
      "step: 7209, loss: 0.49128600955\n",
      "step: 7214, loss: 0.440096735954\n",
      "step: 7219, loss: 0.483914762735\n",
      "Checkpoint is saved\n",
      "step: 7224, loss: 0.503350377083\n",
      "step: 7229, loss: 0.427349805832\n",
      "step: 7234, loss: 0.375117599964\n",
      "step: 7239, loss: 0.378579676151\n",
      "Checkpoint is saved\n",
      "step: 7244, loss: 0.616429150105\n",
      "step: 7249, loss: 0.425204902887\n",
      "step: 7254, loss: 0.336304754019\n",
      "step: 7259, loss: 0.532625973225\n",
      "Checkpoint is saved\n",
      "step: 7264, loss: 0.5682528615\n",
      "step: 7269, loss: 0.583179056644\n",
      "step: 7274, loss: 0.597979903221\n",
      "step: 7279, loss: 0.498112261295\n",
      "Checkpoint is saved\n",
      "step: 7284, loss: 0.398901760578\n",
      "step: 7289, loss: 0.530331015587\n",
      "step: 7294, loss: 0.500326871872\n",
      "step: 7299, loss: 0.483438402414\n",
      "Checkpoint is saved\n",
      "step: 7304, loss: 0.471096098423\n",
      "step: 7309, loss: 0.424936145544\n",
      "step: 7314, loss: 0.528516411781\n",
      "step: 7319, loss: 0.502958059311\n",
      "Checkpoint is saved\n",
      "step: 7324, loss: 0.486253559589\n",
      "step: 7329, loss: 0.58639729023\n",
      "step: 7334, loss: 0.501116096973\n",
      "step: 7339, loss: 0.426388174295\n",
      "Checkpoint is saved\n",
      "step: 7344, loss: 0.563870489597\n",
      "step: 7349, loss: 0.437454074621\n",
      "step: 7354, loss: 0.488549649715\n",
      "step: 7359, loss: 0.473980545998\n",
      "Checkpoint is saved\n",
      "step: 7364, loss: 0.482623815536\n",
      "step: 7369, loss: 0.467337071896\n",
      "step: 7374, loss: 0.395191311836\n",
      "step: 7379, loss: 0.458223283291\n",
      "Checkpoint is saved\n",
      "step: 7384, loss: 0.569505274296\n",
      "step: 7389, loss: 0.495251506567\n",
      "step: 7394, loss: 0.470086634159\n",
      "step: 7399, loss: 0.383190691471\n",
      "Checkpoint is saved\n",
      "step: 7404, loss: 0.345635652542\n",
      "step: 7409, loss: 0.466499447823\n",
      "step: 7414, loss: 0.465481370687\n",
      "step: 7419, loss: 0.597779452801\n",
      "Checkpoint is saved\n",
      "step: 7424, loss: 0.520785033703\n",
      "step: 7429, loss: 0.39923787117\n",
      "step: 7434, loss: 0.506935238838\n",
      "step: 7439, loss: 0.498736590147\n",
      "Checkpoint is saved\n",
      "step: 7444, loss: 0.52774810791\n",
      "step: 7449, loss: 0.430691421032\n",
      "step: 7454, loss: 0.414857059717\n",
      "step: 7459, loss: 0.45218193531\n",
      "Checkpoint is saved\n",
      "step: 7464, loss: 0.542612314224\n",
      "step: 7469, loss: 0.45435076952\n",
      "step: 7474, loss: 0.500337839127\n",
      "step: 7479, loss: 0.35392343998\n",
      "Checkpoint is saved\n",
      "step: 7484, loss: 0.544191360474\n",
      "step: 7489, loss: 0.496281147003\n",
      "step: 7494, loss: 0.467867225409\n",
      "step: 7499, loss: 0.487009644508\n",
      "Checkpoint is saved\n",
      "step: 7504, loss: 0.392937064171\n",
      "step: 7509, loss: 0.56484824419\n",
      "step: 7514, loss: 0.486523717642\n",
      "step: 7519, loss: 0.423697888851\n",
      "Checkpoint is saved\n",
      "step: 7524, loss: 0.462591737509\n",
      "step: 7529, loss: 0.519534111023\n",
      "step: 7534, loss: 0.396466910839\n",
      "step: 7539, loss: 0.45038241148\n",
      "Checkpoint is saved\n",
      "step: 7544, loss: 0.513205885887\n",
      "step: 7549, loss: 0.516952514648\n",
      "step: 7554, loss: 0.481881052256\n",
      "step: 7559, loss: 0.506913304329\n",
      "Checkpoint is saved\n",
      "step: 7564, loss: 0.496925264597\n",
      "step: 7569, loss: 0.531994104385\n",
      "step: 7574, loss: 0.453538835049\n",
      "step: 7579, loss: 0.448959231377\n",
      "Checkpoint is saved\n",
      "step: 7584, loss: 0.514329135418\n",
      "step: 7589, loss: 0.491068542004\n",
      "step: 7594, loss: 0.495007306337\n",
      "step: 7599, loss: 0.50582909584\n",
      "Checkpoint is saved\n",
      "step: 7604, loss: 0.420318007469\n",
      "step: 7609, loss: 0.499499112368\n",
      "step: 7614, loss: 0.576296448708\n",
      "step: 7619, loss: 0.542499542236\n",
      "Checkpoint is saved\n",
      "step: 7624, loss: 0.56276756525\n",
      "step: 7629, loss: 0.502560913563\n",
      "step: 7634, loss: 0.523337602615\n",
      "step: 7639, loss: 0.526183605194\n",
      "Checkpoint is saved\n",
      "step: 7644, loss: 0.48066931963\n",
      "step: 7649, loss: 0.530598640442\n",
      "step: 7654, loss: 0.500452280045\n",
      "step: 7659, loss: 0.446305006742\n",
      "Checkpoint is saved\n",
      "step: 7664, loss: 0.473720103502\n",
      "step: 7669, loss: 0.524257063866\n",
      "step: 7674, loss: 0.462211996317\n",
      "step: 7679, loss: 0.419003069401\n",
      "Checkpoint is saved\n",
      "step: 7684, loss: 0.473303377628\n",
      "step: 7689, loss: 0.451164066792\n",
      "step: 7694, loss: 0.475334405899\n",
      "step: 7699, loss: 0.530675649643\n",
      "Checkpoint is saved\n",
      "step: 7704, loss: 0.426267981529\n",
      "step: 7709, loss: 0.514408648014\n",
      "step: 7714, loss: 0.467975080013\n",
      "step: 7719, loss: 0.477780073881\n",
      "Checkpoint is saved\n",
      "step: 7724, loss: 0.487858951092\n",
      "step: 7729, loss: 0.434149295092\n",
      "step: 7734, loss: 0.51272714138\n",
      "step: 7739, loss: 0.448195070028\n",
      "Checkpoint is saved\n",
      "step: 7744, loss: 0.557544052601\n",
      "step: 7749, loss: 0.386803418398\n",
      "step: 7754, loss: 0.485251188278\n",
      "step: 7759, loss: 0.528631865978\n",
      "Checkpoint is saved\n",
      "step: 7764, loss: 0.485234618187\n",
      "step: 7769, loss: 0.394800245762\n",
      "step: 7774, loss: 0.439322471619\n",
      "step: 7779, loss: 0.45756188035\n",
      "Checkpoint is saved\n",
      "step: 7784, loss: 0.355994731188\n",
      "step: 7789, loss: 0.474739670753\n",
      "step: 7794, loss: 0.474156707525\n",
      "step: 7799, loss: 0.489610284567\n",
      "Checkpoint is saved\n",
      "step: 7804, loss: 0.444794148207\n",
      "step: 7809, loss: 0.460151076317\n",
      "step: 7814, loss: 0.537801802158\n",
      "step: 7819, loss: 0.505302131176\n",
      "Checkpoint is saved\n",
      "step: 7824, loss: 0.436634689569\n",
      "step: 7829, loss: 0.466327488422\n",
      "step: 7834, loss: 0.317651242018\n",
      "step: 7839, loss: 0.504952669144\n",
      "Checkpoint is saved\n",
      "step: 7844, loss: 0.418992102146\n",
      "step: 7849, loss: 0.470455557108\n",
      "step: 7854, loss: 0.402613610029\n",
      "step: 7859, loss: 0.388059258461\n",
      "Checkpoint is saved\n",
      "step: 7864, loss: 0.402513980865\n",
      "step: 7869, loss: 0.491897821426\n",
      "step: 7874, loss: 0.469733774662\n",
      "step: 7879, loss: 0.431288659573\n",
      "Checkpoint is saved\n",
      "step: 7884, loss: 0.499056220055\n",
      "step: 7889, loss: 0.527541637421\n",
      "step: 7894, loss: 0.504571914673\n",
      "step: 7899, loss: 0.522111058235\n",
      "Checkpoint is saved\n",
      "step: 7904, loss: 0.443346887827\n",
      "step: 7909, loss: 0.567685067654\n",
      "step: 7914, loss: 0.403486639261\n",
      "step: 7919, loss: 0.457582682371\n",
      "Checkpoint is saved\n",
      "step: 7924, loss: 0.430738478899\n",
      "step: 7929, loss: 0.523670375347\n",
      "step: 7934, loss: 0.454789817333\n",
      "step: 7939, loss: 0.544880270958\n",
      "Checkpoint is saved\n",
      "step: 7944, loss: 0.520132303238\n",
      "step: 7949, loss: 0.345549762249\n",
      "step: 7954, loss: 0.527353167534\n",
      "step: 7959, loss: 0.380770385265\n",
      "Checkpoint is saved\n",
      "step: 7964, loss: 0.48577016592\n",
      "step: 7969, loss: 0.471647948027\n",
      "step: 7974, loss: 0.432915329933\n",
      "step: 7979, loss: 0.389679908752\n",
      "Checkpoint is saved\n",
      "step: 7984, loss: 0.35177347064\n",
      "step: 7989, loss: 0.454836547375\n",
      "step: 7994, loss: 0.46512272954\n",
      "step: 7999, loss: 0.561605215073\n",
      "Checkpoint is saved\n",
      "step: 8004, loss: 0.573958098888\n",
      "step: 8009, loss: 0.481084346771\n",
      "step: 8014, loss: 0.415423631668\n",
      "step: 8019, loss: 0.491438627243\n",
      "Checkpoint is saved\n",
      "step: 8024, loss: 0.443642646074\n",
      "step: 8029, loss: 0.384145766497\n",
      "step: 8034, loss: 0.533092975616\n",
      "step: 8039, loss: 0.503570914268\n",
      "Checkpoint is saved\n",
      "step: 8044, loss: 0.33888271451\n",
      "step: 8049, loss: 0.360684216022\n",
      "step: 8054, loss: 0.559640586376\n",
      "step: 8059, loss: 0.421893119812\n",
      "Checkpoint is saved\n",
      "step: 8064, loss: 0.424718737602\n",
      "step: 8069, loss: 0.360625177622\n",
      "step: 8074, loss: 0.575756430626\n",
      "step: 8079, loss: 0.401704013348\n",
      "Checkpoint is saved\n",
      "step: 8084, loss: 0.53876376152\n",
      "step: 8089, loss: 0.428440839052\n",
      "step: 8094, loss: 0.458378821611\n",
      "step: 8099, loss: 0.564125239849\n",
      "Checkpoint is saved\n",
      "step: 8104, loss: 0.443506538868\n",
      "step: 8109, loss: 0.399394899607\n",
      "step: 8114, loss: 0.364115267992\n",
      "step: 8119, loss: 0.458729147911\n",
      "Checkpoint is saved\n",
      "step: 8124, loss: 0.526670694351\n",
      "step: 8129, loss: 0.501200079918\n",
      "step: 8134, loss: 0.488392710686\n",
      "step: 8139, loss: 0.468693435192\n",
      "Checkpoint is saved\n",
      "step: 8144, loss: 0.379929453135\n",
      "step: 8149, loss: 0.451126396656\n",
      "step: 8154, loss: 0.571000814438\n",
      "step: 8159, loss: 0.509332597256\n",
      "Checkpoint is saved\n",
      "step: 8164, loss: 0.525171220303\n",
      "step: 8169, loss: 0.45559695363\n",
      "step: 8174, loss: 0.450662255287\n",
      "step: 8179, loss: 0.331467330456\n",
      "Checkpoint is saved\n",
      "step: 8184, loss: 0.593466639519\n",
      "step: 8189, loss: 0.482256919146\n",
      "step: 8194, loss: 0.509258747101\n",
      "step: 8199, loss: 0.495997041464\n",
      "Checkpoint is saved\n",
      "step: 8204, loss: 0.468227833509\n",
      "step: 8209, loss: 0.530447304249\n",
      "step: 8214, loss: 0.488649368286\n",
      "step: 8219, loss: 0.551918148994\n",
      "Checkpoint is saved\n",
      "step: 8224, loss: 0.446498274803\n",
      "step: 8229, loss: 0.450253814459\n",
      "step: 8234, loss: 0.532926857471\n",
      "step: 8239, loss: 0.446047723293\n",
      "Checkpoint is saved\n",
      "step: 8244, loss: 0.471205860376\n",
      "step: 8249, loss: 0.405478477478\n",
      "step: 8254, loss: 0.545068144798\n",
      "step: 8259, loss: 0.477664381266\n",
      "Checkpoint is saved\n",
      "step: 8264, loss: 0.43061619997\n",
      "step: 8269, loss: 0.485297620296\n",
      "step: 8274, loss: 0.443024128675\n",
      "step: 8279, loss: 0.50467902422\n",
      "Checkpoint is saved\n",
      "step: 8284, loss: 0.473770260811\n",
      "step: 8289, loss: 0.509624481201\n",
      "step: 8294, loss: 0.358253240585\n",
      "step: 8299, loss: 0.493736177683\n",
      "Checkpoint is saved\n",
      "step: 8304, loss: 0.50518733263\n",
      "step: 8309, loss: 0.539258301258\n",
      "step: 8314, loss: 0.429131984711\n",
      "step: 8319, loss: 0.472301274538\n",
      "Checkpoint is saved\n",
      "step: 8324, loss: 0.492757946253\n",
      "step: 8329, loss: 0.455273747444\n",
      "step: 8334, loss: 0.561919629574\n",
      "step: 8339, loss: 0.38932916522\n",
      "Checkpoint is saved\n",
      "step: 8344, loss: 0.435634851456\n",
      "step: 8349, loss: 0.534313440323\n",
      "step: 8354, loss: 0.422888875008\n",
      "step: 8359, loss: 0.480732798576\n",
      "Checkpoint is saved\n",
      "step: 8364, loss: 0.465074390173\n",
      "step: 8369, loss: 0.561469316483\n",
      "step: 8374, loss: 0.536932587624\n",
      "step: 8379, loss: 0.551800429821\n",
      "Checkpoint is saved\n",
      "step: 8384, loss: 0.439309477806\n",
      "step: 8389, loss: 0.522072136402\n",
      "step: 8394, loss: 0.503207504749\n",
      "step: 8399, loss: 0.499094754457\n",
      "Checkpoint is saved\n",
      "step: 8404, loss: 0.542175829411\n",
      "step: 8409, loss: 0.446322023869\n",
      "step: 8414, loss: 0.503641963005\n",
      "step: 8419, loss: 0.567633390427\n",
      "Checkpoint is saved\n",
      "step: 8424, loss: 0.46853980422\n",
      "step: 8429, loss: 0.447204709053\n",
      "step: 8434, loss: 0.405531913042\n",
      "step: 8439, loss: 0.46579515934\n",
      "Checkpoint is saved\n",
      "step: 8444, loss: 0.424773573875\n",
      "step: 8449, loss: 0.417648732662\n",
      "step: 8454, loss: 0.391767710447\n",
      "step: 8459, loss: 0.47733014822\n",
      "Checkpoint is saved\n",
      "step: 8464, loss: 0.326167315245\n",
      "step: 8469, loss: 0.438028395176\n",
      "step: 8474, loss: 0.531155526638\n",
      "step: 8479, loss: 0.453335106373\n",
      "Checkpoint is saved\n",
      "step: 8484, loss: 0.42075279355\n",
      "step: 8489, loss: 0.441798657179\n",
      "step: 8494, loss: 0.454169780016\n",
      "step: 8499, loss: 0.448025822639\n",
      "Checkpoint is saved\n",
      "step: 8504, loss: 0.486971884966\n",
      "step: 8509, loss: 0.3419508636\n",
      "step: 8514, loss: 0.559150218964\n",
      "step: 8519, loss: 0.339061737061\n",
      "Checkpoint is saved\n",
      "step: 8524, loss: 0.502825498581\n",
      "step: 8529, loss: 0.60823237896\n",
      "step: 8534, loss: 0.460453391075\n",
      "step: 8539, loss: 0.503093123436\n",
      "Checkpoint is saved\n",
      "step: 8544, loss: 0.512916684151\n",
      "step: 8549, loss: 0.533027946949\n",
      "step: 8554, loss: 0.461196810007\n",
      "step: 8559, loss: 0.451154798269\n",
      "Checkpoint is saved\n",
      "step: 8564, loss: 0.415488213301\n",
      "step: 8569, loss: 0.532885909081\n",
      "step: 8574, loss: 0.361727744341\n",
      "step: 8579, loss: 0.416771829128\n",
      "Checkpoint is saved\n",
      "step: 8584, loss: 0.37141931057\n",
      "step: 8589, loss: 0.425099611282\n",
      "step: 8594, loss: 0.550419688225\n",
      "step: 8599, loss: 0.46211451292\n",
      "Checkpoint is saved\n",
      "step: 8604, loss: 0.491645097733\n",
      "step: 8609, loss: 0.443793922663\n",
      "step: 8614, loss: 0.438074856997\n",
      "step: 8619, loss: 0.416315019131\n",
      "Checkpoint is saved\n",
      "step: 8624, loss: 0.460489481688\n",
      "step: 8629, loss: 0.515025496483\n",
      "step: 8634, loss: 0.389208436012\n",
      "step: 8639, loss: 0.411888241768\n",
      "Checkpoint is saved\n",
      "step: 8644, loss: 0.438695281744\n",
      "step: 8649, loss: 0.440065532923\n",
      "step: 8654, loss: 0.393067955971\n",
      "step: 8659, loss: 0.495889306068\n",
      "Checkpoint is saved\n",
      "step: 8664, loss: 0.496078163385\n",
      "step: 8669, loss: 0.36797106266\n",
      "step: 8674, loss: 0.458340734243\n",
      "step: 8679, loss: 0.491132348776\n",
      "Checkpoint is saved\n",
      "step: 8684, loss: 0.360051691532\n",
      "step: 8689, loss: 0.5085760355\n",
      "step: 8694, loss: 0.419236212969\n",
      "step: 8699, loss: 0.382059514523\n",
      "Checkpoint is saved\n",
      "step: 8704, loss: 0.451794147491\n",
      "step: 8709, loss: 0.448018670082\n",
      "step: 8714, loss: 0.538761973381\n",
      "step: 8719, loss: 0.55108910799\n",
      "Checkpoint is saved\n",
      "step: 8724, loss: 0.448388040066\n",
      "step: 8729, loss: 0.516088485718\n",
      "step: 8734, loss: 0.497974634171\n",
      "step: 8739, loss: 0.450016200542\n",
      "Checkpoint is saved\n",
      "step: 8744, loss: 0.431920945644\n",
      "step: 8749, loss: 0.465609878302\n",
      "step: 8754, loss: 0.492188841105\n",
      "step: 8759, loss: 0.553265333176\n",
      "Checkpoint is saved\n",
      "step: 8764, loss: 0.461776852608\n",
      "step: 8769, loss: 0.390729993582\n",
      "step: 8774, loss: 0.450649082661\n",
      "step: 8779, loss: 0.421851336956\n",
      "Checkpoint is saved\n",
      "step: 8784, loss: 0.412888228893\n",
      "step: 8789, loss: 0.393012315035\n",
      "step: 8794, loss: 0.428762048483\n",
      "step: 8799, loss: 0.392799198627\n",
      "Checkpoint is saved\n",
      "step: 8804, loss: 0.413019657135\n",
      "step: 8809, loss: 0.55434679985\n",
      "step: 8814, loss: 0.485881358385\n",
      "step: 8819, loss: 0.414968639612\n",
      "Checkpoint is saved\n",
      "step: 8824, loss: 0.392991900444\n",
      "step: 8829, loss: 0.422410428524\n",
      "step: 8834, loss: 0.403784692287\n",
      "step: 8839, loss: 0.413171201944\n",
      "Checkpoint is saved\n",
      "step: 8844, loss: 0.379991650581\n",
      "step: 8849, loss: 0.452810406685\n",
      "step: 8854, loss: 0.400293320417\n",
      "step: 8859, loss: 0.405324161053\n",
      "Checkpoint is saved\n",
      "step: 8864, loss: 0.376632332802\n",
      "step: 8869, loss: 0.360999435186\n",
      "step: 8874, loss: 0.471360564232\n",
      "step: 8879, loss: 0.502947568893\n",
      "Checkpoint is saved\n",
      "step: 8884, loss: 0.460316985846\n",
      "step: 8889, loss: 0.445169836283\n",
      "step: 8894, loss: 0.514730095863\n",
      "step: 8899, loss: 0.386214256287\n",
      "Checkpoint is saved\n",
      "step: 8904, loss: 0.428610265255\n",
      "step: 8909, loss: 0.378940165043\n",
      "step: 8914, loss: 0.457298338413\n",
      "step: 8919, loss: 0.434321939945\n",
      "Checkpoint is saved\n",
      "step: 8924, loss: 0.397848755121\n",
      "step: 8929, loss: 0.356069862843\n",
      "step: 8934, loss: 0.409698367119\n",
      "step: 8939, loss: 0.47277700901\n",
      "Checkpoint is saved\n",
      "step: 8944, loss: 0.468993723392\n",
      "step: 8949, loss: 0.434115767479\n",
      "step: 8954, loss: 0.415005773306\n",
      "step: 8959, loss: 0.44460862875\n",
      "Checkpoint is saved\n",
      "step: 8964, loss: 0.372522234917\n",
      "step: 8969, loss: 0.655782699585\n",
      "step: 8974, loss: 0.568691849709\n",
      "step: 8979, loss: 0.436886787415\n",
      "Checkpoint is saved\n",
      "step: 8984, loss: 0.501175940037\n",
      "step: 8989, loss: 0.598150968552\n",
      "step: 8994, loss: 0.578290879726\n",
      "step: 8999, loss: 0.403791546822\n",
      "Checkpoint is saved\n",
      "step: 9004, loss: 0.542992532253\n",
      "step: 9009, loss: 0.430592417717\n",
      "step: 9014, loss: 0.352090358734\n",
      "step: 9019, loss: 0.559537291527\n",
      "Checkpoint is saved\n",
      "step: 9024, loss: 0.43421202898\n",
      "step: 9029, loss: 0.443214058876\n",
      "step: 9034, loss: 0.442909359932\n",
      "step: 9039, loss: 0.482210576534\n",
      "Checkpoint is saved\n",
      "step: 9044, loss: 0.421709030867\n",
      "step: 9049, loss: 0.5538392663\n",
      "step: 9054, loss: 0.422164440155\n",
      "step: 9059, loss: 0.561562418938\n",
      "Checkpoint is saved\n",
      "step: 9064, loss: 0.449427306652\n",
      "step: 9069, loss: 0.415650248528\n",
      "step: 9074, loss: 0.436851263046\n",
      "step: 9079, loss: 0.39907798171\n",
      "Checkpoint is saved\n",
      "step: 9084, loss: 0.374146461487\n",
      "step: 9089, loss: 0.423537909985\n",
      "step: 9094, loss: 0.48013395071\n",
      "step: 9099, loss: 0.496303498745\n",
      "Checkpoint is saved\n",
      "step: 9104, loss: 0.427907943726\n",
      "step: 9109, loss: 0.467759817839\n",
      "step: 9114, loss: 0.365693718195\n",
      "step: 9119, loss: 0.576202392578\n",
      "Checkpoint is saved\n",
      "step: 9124, loss: 0.521264612675\n",
      "step: 9129, loss: 0.413829863071\n",
      "step: 9134, loss: 0.476290911436\n",
      "step: 9139, loss: 0.512000799179\n",
      "Checkpoint is saved\n",
      "step: 9144, loss: 0.49563446641\n",
      "step: 9149, loss: 0.428084313869\n",
      "step: 9154, loss: 0.419740438461\n",
      "step: 9159, loss: 0.40050971508\n",
      "Checkpoint is saved\n",
      "step: 9164, loss: 0.472633957863\n",
      "step: 9169, loss: 0.590396881104\n",
      "step: 9174, loss: 0.438874006271\n",
      "step: 9179, loss: 0.492688894272\n",
      "Checkpoint is saved\n",
      "step: 9184, loss: 0.398435354233\n",
      "step: 9189, loss: 0.450049698353\n",
      "step: 9194, loss: 0.457231163979\n",
      "step: 9199, loss: 0.476265251637\n",
      "Checkpoint is saved\n",
      "step: 9204, loss: 0.4049051404\n",
      "step: 9209, loss: 0.406162828207\n",
      "step: 9214, loss: 0.379024595022\n",
      "step: 9219, loss: 0.444176971912\n",
      "Checkpoint is saved\n",
      "step: 9224, loss: 0.381835460663\n",
      "step: 9229, loss: 0.459537625313\n",
      "step: 9234, loss: 0.45876967907\n",
      "step: 9239, loss: 0.543682098389\n",
      "Checkpoint is saved\n",
      "step: 9244, loss: 0.459549367428\n",
      "step: 9249, loss: 0.457015216351\n",
      "step: 9254, loss: 0.377168416977\n",
      "step: 9259, loss: 0.339831352234\n",
      "Checkpoint is saved\n",
      "step: 9264, loss: 0.470833867788\n",
      "step: 9269, loss: 0.44507804513\n",
      "step: 9274, loss: 0.40309804678\n",
      "step: 9279, loss: 0.585229158401\n",
      "Checkpoint is saved\n",
      "step: 9284, loss: 0.432236641645\n",
      "step: 9289, loss: 0.342001527548\n",
      "step: 9294, loss: 0.443431258202\n",
      "step: 9299, loss: 0.393782377243\n",
      "Checkpoint is saved\n",
      "step: 9304, loss: 0.507955431938\n",
      "step: 9309, loss: 0.444802552462\n",
      "step: 9314, loss: 0.47690141201\n",
      "step: 9319, loss: 0.522128522396\n",
      "Checkpoint is saved\n",
      "step: 9324, loss: 0.46231418848\n",
      "step: 9329, loss: 0.410302609205\n",
      "step: 9334, loss: 0.390890330076\n",
      "step: 9339, loss: 0.411865323782\n",
      "Checkpoint is saved\n",
      "step: 9344, loss: 0.477069735527\n",
      "step: 9349, loss: 0.575552225113\n",
      "step: 9354, loss: 0.445349127054\n",
      "step: 9359, loss: 0.39863473177\n",
      "Checkpoint is saved\n",
      "step: 9364, loss: 0.515288114548\n",
      "step: 9369, loss: 0.51363158226\n",
      "step: 9374, loss: 0.48335057497\n",
      "step: 9379, loss: 0.428114950657\n",
      "Checkpoint is saved\n",
      "step: 9384, loss: 0.464055687189\n",
      "step: 9389, loss: 0.419976085424\n",
      "step: 9394, loss: 0.298235535622\n",
      "step: 9399, loss: 0.430154025555\n",
      "Checkpoint is saved\n",
      "step: 9404, loss: 0.415214657784\n",
      "step: 9409, loss: 0.534187555313\n",
      "step: 9414, loss: 0.440494507551\n",
      "step: 9419, loss: 0.527089774609\n",
      "Checkpoint is saved\n",
      "step: 9424, loss: 0.386940300465\n",
      "step: 9429, loss: 0.477455168962\n",
      "step: 9434, loss: 0.345443069935\n",
      "step: 9439, loss: 0.441621363163\n",
      "Checkpoint is saved\n",
      "step: 9444, loss: 0.412046790123\n",
      "step: 9449, loss: 0.494428753853\n",
      "step: 9454, loss: 0.401594489813\n",
      "step: 9459, loss: 0.399636983871\n",
      "Checkpoint is saved\n",
      "step: 9464, loss: 0.429527908564\n",
      "step: 9469, loss: 0.573507845402\n",
      "step: 9474, loss: 0.41077837348\n",
      "step: 9479, loss: 0.390024185181\n",
      "Checkpoint is saved\n",
      "step: 9484, loss: 0.407701134682\n",
      "step: 9489, loss: 0.481820017099\n",
      "step: 9494, loss: 0.443466424942\n",
      "step: 9499, loss: 0.458221793175\n",
      "Checkpoint is saved\n",
      "step: 9504, loss: 0.368312776089\n",
      "step: 9509, loss: 0.474303126335\n",
      "step: 9514, loss: 0.400205790997\n",
      "step: 9519, loss: 0.451582968235\n",
      "Checkpoint is saved\n",
      "step: 9524, loss: 0.39640390873\n",
      "step: 9529, loss: 0.470225155354\n",
      "step: 9534, loss: 0.357107162476\n",
      "step: 9539, loss: 0.471025466919\n",
      "Checkpoint is saved\n",
      "step: 9544, loss: 0.499983787537\n",
      "step: 9549, loss: 0.477372378111\n",
      "step: 9554, loss: 0.452338933945\n",
      "step: 9559, loss: 0.494474172592\n",
      "Checkpoint is saved\n",
      "step: 9564, loss: 0.383185982704\n",
      "step: 9569, loss: 0.419665843248\n",
      "step: 9574, loss: 0.419668912888\n",
      "step: 9579, loss: 0.408056676388\n",
      "Checkpoint is saved\n",
      "step: 9584, loss: 0.458263337612\n",
      "step: 9589, loss: 0.439420223236\n",
      "step: 9594, loss: 0.437084794044\n",
      "step: 9599, loss: 0.415892064571\n",
      "Checkpoint is saved\n",
      "step: 9604, loss: 0.349491715431\n",
      "step: 9609, loss: 0.432847797871\n",
      "step: 9614, loss: 0.390150904655\n",
      "step: 9619, loss: 0.382961630821\n",
      "Checkpoint is saved\n",
      "step: 9624, loss: 0.427330374718\n",
      "step: 9629, loss: 0.394360184669\n",
      "step: 9634, loss: 0.596489429474\n",
      "step: 9639, loss: 0.446797549725\n",
      "Checkpoint is saved\n",
      "step: 9644, loss: 0.500346541405\n",
      "step: 9649, loss: 0.432995200157\n",
      "step: 9654, loss: 0.475773960352\n",
      "step: 9659, loss: 0.424261510372\n",
      "Checkpoint is saved\n",
      "step: 9664, loss: 0.493332445621\n",
      "step: 9669, loss: 0.366315186024\n",
      "step: 9674, loss: 0.564515590668\n",
      "step: 9679, loss: 0.49979531765\n",
      "Checkpoint is saved\n",
      "step: 9684, loss: 0.465449571609\n",
      "step: 9689, loss: 0.428847491741\n",
      "step: 9694, loss: 0.374333977699\n",
      "step: 9699, loss: 0.510895133018\n",
      "Checkpoint is saved\n",
      "step: 9704, loss: 0.402139902115\n",
      "step: 9709, loss: 0.464308977127\n",
      "step: 9714, loss: 0.54209280014\n",
      "step: 9719, loss: 0.38231036067\n",
      "Checkpoint is saved\n",
      "step: 9724, loss: 0.35985404253\n",
      "step: 9729, loss: 0.368907481432\n",
      "step: 9734, loss: 0.412065327168\n",
      "step: 9739, loss: 0.437911003828\n",
      "Checkpoint is saved\n",
      "step: 9744, loss: 0.483171343803\n",
      "step: 9749, loss: 0.462485909462\n",
      "step: 9754, loss: 0.382678091526\n",
      "step: 9759, loss: 0.398009538651\n",
      "Checkpoint is saved\n",
      "step: 9764, loss: 0.431383639574\n",
      "step: 9769, loss: 0.413622558117\n",
      "step: 9774, loss: 0.488080501556\n",
      "step: 9779, loss: 0.501964986324\n",
      "Checkpoint is saved\n",
      "step: 9784, loss: 0.454035639763\n",
      "step: 9789, loss: 0.467028826475\n",
      "step: 9794, loss: 0.512368440628\n",
      "step: 9799, loss: 0.501174211502\n",
      "Checkpoint is saved\n",
      "step: 9804, loss: 0.380854427814\n",
      "step: 9809, loss: 0.532566070557\n",
      "step: 9814, loss: 0.399831444025\n",
      "step: 9819, loss: 0.323340058327\n",
      "Checkpoint is saved\n",
      "step: 9824, loss: 0.382813692093\n",
      "step: 9829, loss: 0.453522920609\n",
      "step: 9834, loss: 0.346554875374\n",
      "step: 9839, loss: 0.478167951107\n",
      "Checkpoint is saved\n",
      "step: 9844, loss: 0.36113345623\n",
      "step: 9849, loss: 0.387384206057\n",
      "step: 9854, loss: 0.50275349617\n",
      "step: 9859, loss: 0.492148995399\n",
      "Checkpoint is saved\n",
      "step: 9864, loss: 0.399822801352\n",
      "step: 9869, loss: 0.481905341148\n",
      "step: 9874, loss: 0.427810698748\n",
      "step: 9879, loss: 0.409106314182\n",
      "Checkpoint is saved\n",
      "step: 9884, loss: 0.523263216019\n",
      "step: 9889, loss: 0.469888448715\n",
      "step: 9894, loss: 0.535091280937\n",
      "step: 9899, loss: 0.413178503513\n",
      "Checkpoint is saved\n",
      "step: 9904, loss: 0.427842855453\n",
      "step: 9909, loss: 0.441452711821\n",
      "step: 9914, loss: 0.462649583817\n",
      "step: 9919, loss: 0.438506126404\n",
      "Checkpoint is saved\n",
      "step: 9924, loss: 0.559422135353\n",
      "step: 9929, loss: 0.43295699358\n",
      "step: 9934, loss: 0.379654169083\n",
      "step: 9939, loss: 0.493013709784\n",
      "Checkpoint is saved\n",
      "step: 9944, loss: 0.51218688488\n",
      "step: 9949, loss: 0.496885955334\n",
      "step: 9954, loss: 0.467294931412\n",
      "step: 9959, loss: 0.647021114826\n",
      "Checkpoint is saved\n",
      "step: 9964, loss: 0.449302196503\n",
      "step: 9969, loss: 0.497288912535\n",
      "step: 9974, loss: 0.46579682827\n",
      "step: 9979, loss: 0.428186029196\n",
      "Checkpoint is saved\n",
      "step: 9984, loss: 0.434142291546\n",
      "step: 9989, loss: 0.526878356934\n",
      "step: 9994, loss: 0.551744580269\n",
      "step: 9999, loss: 0.413198173046\n",
      "Checkpoint is saved\n",
      "Training time for 10000 steps: 29503.766459s\n"
     ]
    }
   ],
   "source": [
    "# let's train the model\n",
    "\n",
    "# we will use this list to plot losses through steps\n",
    "losses = []\n",
    "\n",
    "# save a checkpoint so we can restore the model later \n",
    "saver = tf.train.Saver()\n",
    "\n",
    "print '------------------TRAINING------------------'\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    t = time.time()\n",
    "    for step in range(steps):\n",
    "        feed = feed_dict(X_train, Y_train)\n",
    "            \n",
    "        backward_step(sess, feed)\n",
    "        \n",
    "        if step % 5 == 4 or step == 0:\n",
    "            loss_value = sess.run(loss, feed_dict = feed)\n",
    "            print 'step: {}, loss: {}'.format(step, loss_value)\n",
    "            losses.append(loss_value)\n",
    "        \n",
    "        if step % 20 == 19:\n",
    "            saver.save(sess, 'checkpoints/', global_step=step)\n",
    "            print 'Checkpoint is saved'\n",
    "            \n",
    "    print 'Training time for {} steps: {}s'.format(steps, time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbIAAAEkCAYAAABKTLRCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10582a910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot losses\n",
    "\n",
    "with plt.style.context('fivethirtyeight'):\n",
    "    plt.plot(losses, linewidth = 1)\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Losses')\n",
    "    plt.ylim((0, 12))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/-9999\n",
      "1.\n",
      "--------------------------------\n",
      "Cameron is expected to give a speech setting out the government’s future strategy\n",
      "La gestión pública tiene que dar un discurso para la estrategia futura \n",
      "--------------------------------\n",
      "2.\n",
      "--------------------------------\n",
      "What we ought to be doing is to get behind the Muslim communities\n",
      "Lo que tenemos que hacer es que sea el problema de las comunidades comunitarias \n",
      "--------------------------------\n",
      "3.\n",
      "--------------------------------\n",
      "Cameron is due to use a speech to warn young Britons tempted to join IS fighters\n",
      "El europeo está emplear utilizar un discurso para el <ukn> de <ukn> <ukn> <ukn> \n",
      "--------------------------------\n",
      "4.\n",
      "--------------------------------\n",
      "The prime minister warned of the dangers posed by those who quietly condone IS militants extremist ideology\n",
      "El Ministro del Sr Evans ha dicho que por los que se ha hecho \n",
      "--------------------------------\n",
      "5.\n",
      "--------------------------------\n",
      "There are two kinds of damages\n",
      "Hay dos tipos de crédito \n",
      "--------------------------------\n",
      "6.\n",
      "--------------------------------\n",
      "The first is compensatory meaning money to pay for the actual cost of an injury or loss\n",
      "El primero es el dinero del dinero para pagar el coste de un beneficio \n",
      "--------------------------------\n",
      "7.\n",
      "--------------------------------\n",
      "The second is punitive or exemplary meaning an amount of money that is more than the actual damages\n",
      "El segundo es que es un cierto papel de dinero que es un cierto dinero \n",
      "--------------------------------\n",
      "8.\n",
      "--------------------------------\n",
      "A deportation has certain consequences regarding the number of years within which a deportee may not legally immigrate\n",
      "Una condición que se ha hecho algunas consecuencias sobre años dentro de un posible plazo \n",
      "--------------------------------\n",
      "9.\n",
      "--------------------------------\n",
      "The UK could decide to back out at any point in the negotiations\n",
      "La decisión podría decidir decidir a la cuestión en las negociaciones \n",
      "--------------------------------\n",
      "10.\n",
      "--------------------------------\n",
      "The terms of the Brexit are still unclear\n",
      "Los términos del <ukn> están todavía claro \n",
      "--------------------------------\n",
      "11.\n",
      "--------------------------------\n",
      "Brussels is likely to make an example of them to deter other member states from following suit\n",
      "Bruselas se puede realizar por ejemplo en su corazón otros Estados miembro de otros Estados \n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "# let's test the model\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    # placeholders\n",
    "    encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'encoder{}'.format(i)) for i in range(input_seq_len)]\n",
    "    decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'decoder{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "    # output projection\n",
    "    size = 512\n",
    "    w_t = tf.get_variable('proj_w', [es_vocab_size, size], tf.float32)\n",
    "    b = tf.get_variable('proj_b', [es_vocab_size], tf.float32)\n",
    "    w = tf.transpose(w_t)\n",
    "    output_projection = (w, b)\n",
    "    \n",
    "    # change the model so that output at time t can be fed as input at time t+1\n",
    "    outputs, states = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "                                                encoder_inputs,\n",
    "                                                decoder_inputs,\n",
    "                                                tf.contrib.rnn.BasicLSTMCell(size),\n",
    "                                                num_encoder_symbols = en_vocab_size,\n",
    "                                                num_decoder_symbols = es_vocab_size,\n",
    "                                                embedding_size = 100,\n",
    "                                                feed_previous = True, # <-----this is changed----->\n",
    "                                                output_projection = output_projection,\n",
    "                                                dtype = tf.float32)\n",
    "    \n",
    "    # ops for projecting outputs\n",
    "    outputs_proj = [tf.matmul(outputs[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "\n",
    "    # let's translate these sentences     \n",
    "    en_sentences = ['Cameron is expected to give a speech setting out the government’s future strategy',\n",
    "                    \n",
    "                   'What we ought to be doing is to get behind the Muslim communities',\n",
    "                    \n",
    "                   'Cameron is due to use a speech to warn young Britons tempted to join IS fighters',\n",
    "                    \n",
    "                   'The prime minister warned of the dangers posed by those who quietly condone IS militants extremist ideology',\n",
    "                    \n",
    "                   'There are two kinds of damages', \n",
    "                   'The first is compensatory meaning money to pay for the actual cost of an injury or loss', \n",
    "                    'The second is punitive or exemplary meaning an amount of money that is more than the actual damages',\n",
    "                    \n",
    "                   'A deportation has certain consequences regarding the number of years within which a deportee may not legally immigrate',\n",
    "                    'The UK could decide to back out at any point in the negotiations',\n",
    "                    'The terms of the Brexit are still unclear',\n",
    "                    'Brussels is likely to make an example of them to deter other member states from following suit']\n",
    "    \n",
    "    en_sentences_encoded = [[en_word2idx.get(word, 0) for word in en_sentence.split()] for en_sentence in en_sentences]\n",
    "    \n",
    "    # padding to fit encoder input\n",
    "    for i in range(len(en_sentences_encoded)):\n",
    "        en_sentences_encoded[i] += (15 - len(en_sentences_encoded[i])) * [en_word2idx['<pad>']]\n",
    "    \n",
    "    # restore all variables - use the last checkpoint saved\n",
    "    saver = tf.train.Saver()\n",
    "    path = tf.train.latest_checkpoint('checkpoints')\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # restore\n",
    "        saver.restore(sess, path)\n",
    "        \n",
    "        # feed data into placeholders\n",
    "        feed = {}\n",
    "        for i in range(input_seq_len):\n",
    "            feed[encoder_inputs[i].name] = np.array([en_sentences_encoded[j][i] for j in range(len(en_sentences_encoded))], dtype = np.int32)\n",
    "            \n",
    "        feed[decoder_inputs[0].name] = np.array([es_word2idx['<go>']] * len(en_sentences_encoded), dtype = np.int32)\n",
    "        \n",
    "        # translate\n",
    "        output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
    "        \n",
    "        # decode seq.\n",
    "        for i in range(len(en_sentences_encoded)):\n",
    "            print '{}.\\n--------------------------------'.format(i+1)\n",
    "            ouput_seq = [output_sequences[j][i] for j in range(output_seq_len)]\n",
    "            #decode output sequence\n",
    "            words = decode_output(ouput_seq)\n",
    "        \n",
    "            print en_sentences[i]\n",
    "            for i in range(len(words)):\n",
    "                if words[i] not in ['<eos>', '<pad>', '<go>']:\n",
    "                    print words[i],\n",
    "            \n",
    "            print '\\n--------------------------------'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### BLEU index calculatios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31440065102\n",
      "0.560790023999\n",
      "0.149141479683\n",
      "0.468881665708\n",
      "0.668740304976\n",
      "0.319912215057\n",
      "0.277051940573\n",
      "0.509333091785\n",
      "0.382979567374\n",
      "0.314898888038\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "hypothesis1 = 'La gestión pública tiene que dar un discurso para la estrategia futura'.split()\n",
    "hypothesis2 = 'Lo que tenemos que hacer es que sea el problema de las comunidades comunitarias'.split()\n",
    "hypothesis3 = 'El europeo está emplear utilizar un discurso para el <ukn> de <ukn> <ukn> <ukn>'.split()\n",
    "hypothesis4 = 'El Ministro del Sr Evans ha dicho que por los que se ha hecho'.split()\n",
    "hypothesis5 = 'Hay dos tipos de crédito'.split()\n",
    "hypothesis6 = 'El primero es el dinero del dinero para pagar el coste de un beneficio'.split()\n",
    "hypothesis7 = 'El segundo es que es un cierto papel de dinero que es un cierto dinero'.split()\n",
    "hypothesis8 = 'Una condición que se ha hecho algunas consecuencias sobre años dentro de un posible plazo'.split()\n",
    "hypothesis__ = 'La decisión podría decidir decidir a la cuestión en las negociaciones'.split()\n",
    "hypothesis9  = 'Los términos del <ukn> están todavía claro'.split()\n",
    "hypothesis10 = 'Bruselas se puede realizar por ejemplo en su corazón otros Estados miembro de otros Estados'.split()\n",
    "\n",
    "reference1 = 'Se espera que Cameron pronuncie un discurso exponiendo la estrategia futura del gobierno'.split()\n",
    "reference2 = 'Lo que deberíamos hacer es apoyar a las comunidades musulmanas'.split()\n",
    "reference3 = \"Cameron tiene programado un discurso para advertir a los jóvenes británicos tentados a unirse con los combatientes del Estado Islámico\".split()\n",
    "reference4 = 'El primer ministro advirtió sobre los peligros que plantean aquellos que calladamente toleran la ideología extremista de los militantes del EI'.split()                \n",
    "reference5 = 'Hay dos tipos de daños'.split()\n",
    "reference6 = 'El primero es el dinero compensatorio que significa dinero para pagar el costo real de una lesión o pérdida'.split()\n",
    "reference7 = \"El segundo es punitivo o ejemplar que significa cantidad de dinero mayor que los daños reales\".split()\n",
    "reference8 = 'Una deportación tiene ciertas consecuencias con respecto a la cantidad de años durante los cuales un deportado no puede inmigrar legalmente'.split()\n",
    "reference9 = 'Los términos del Brexit aún no están claros'.split()\n",
    "reference10 = 'Es probable que Bruselas haga un ejemplo de ellos para disuadir a otros Estados miembros de seguir su ejemplo'.split()\n",
    "\n",
    "bleuScores = []\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference1], hypothesis1)\n",
    "bleuScores.append(BLEUscore)\n",
    "print BLEUscore\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference2], hypothesis2)\n",
    "bleuScores.append(BLEUscore)\n",
    "print BLEUscore\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference3], hypothesis3)\n",
    "bleuScores.append(BLEUscore)\n",
    "print BLEUscore\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference4], hypothesis4)\n",
    "bleuScores.append(BLEUscore)\n",
    "print BLEUscore\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference5], hypothesis5)\n",
    "bleuScores.append(BLEUscore)\n",
    "print BLEUscore\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference6], hypothesis6)\n",
    "bleuScores.append(BLEUscore)\n",
    "print BLEUscore\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference7], hypothesis7)\n",
    "bleuScores.append(BLEUscore)\n",
    "print BLEUscore\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference8], hypothesis8)\n",
    "bleuScores.append(BLEUscore)\n",
    "print BLEUscore\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference9], hypothesis9)\n",
    "bleuScores.append(BLEUscore)\n",
    "print BLEUscore\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference10], hypothesis10)\n",
    "bleuScores.append(BLEUscore)\n",
    "print BLEUscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3144006510201675, 0.5607900239988007, 0.14914147968282956, 0.46888166570791445, 0.668740304976422, 0.31991221505676887, 0.27705194057255156, 0.5093330917854971, 0.38297956737438044, 0.3148988880384631]\n"
     ]
    }
   ],
   "source": [
    "print bleuScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " average BLEU score is:  0.396612982821\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print \"average BLEU score is: \", np.mean(bleuScores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The average BLEU score is 0.396612982821\n",
    "We used on 2 millions sentences from parlamet parallel corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
