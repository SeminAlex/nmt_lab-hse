{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle as pickle\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readed en0\n",
      "readed es0\n",
      "readed en1\n",
      "readed es1\n",
      "readed en2\n",
      "readed es2\n",
      "readed en3\n",
      "readed es3\n"
     ]
    }
   ],
   "source": [
    "#dataRead\n",
    "\n",
    "en_sentences = []\n",
    "es_sentences = []\n",
    "\n",
    "for i in range(4):\n",
    "    with open(\"dataset/europarl-v7.es-en-\" + str(i) +\".en\", 'r') as en_file:\n",
    "        for en_line in en_file:\n",
    "            en_sentences.append(en_line)\n",
    "    print \"readed en\" + str(i)\n",
    "    with open(\"dataset/europarl-v7.es-en-\" + str(i) +\".es\", 'r') as es_file:\n",
    "        for es_line in es_file:\n",
    "            es_sentences.append(es_line)\n",
    "    print \"readed es\" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apart from anything else, they divisively create social problems for workers in the shipping industry and residents of island regions.\n",
      "\n",
      "Aparte de todo, crean, generando discrepancias, problemas sociales para los trabajadores del sector naviero y los residentes de las regiones insulares.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = 999999\n",
    "print en_sentences[index]\n",
    "print es_sentences[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataPreparation\n",
    "def create_dataset(source_sentences,target_sentences):\n",
    "    source_vocab_dict = Counter(word.strip(',.\" ;:)(][?!') for sentence in source_sentences for word in sentence.split())\n",
    "    target_vocab_dict = Counter(word.strip(',.\" ;:)(][?!') for sentence in target_sentences for word in sentence.split())\n",
    "\n",
    "    source_vocab = map(lambda x: x[0], sorted(source_vocab_dict.items(), key = lambda x: -x[1]))\n",
    "    target_vocab = map(lambda x: x[0], sorted(target_vocab_dict.items(), key = lambda x: -x[1]))\n",
    "    \n",
    "    source_vocab = source_vocab[:20000]\n",
    "    target_vocab = target_vocab[:30000]\n",
    "    \n",
    "    start_idx = 2\n",
    "    source_word2idx = dict([(word, idx+start_idx) for idx, word in enumerate(source_vocab)])\n",
    "    source_word2idx['<ukn>'] = 0\n",
    "    source_word2idx['<pad>'] = 1\n",
    "    source_idx2word = dict([(idx, word) for word, idx in source_word2idx.iteritems()])\n",
    "    \n",
    "    start_idx = 4\n",
    "    target_word2idx = dict([(word, idx+start_idx) for idx, word in enumerate(target_vocab)])\n",
    "    target_word2idx['<ukn>'] = 0\n",
    "    target_word2idx['<go>']  = 1\n",
    "    target_word2idx['<eos>'] = 2\n",
    "    target_word2idx['<pad>'] = 3\n",
    "    \n",
    "    target_idx2word = dict([(idx, word) for word, idx in target_word2idx.iteritems()])\n",
    "    x = [[source_word2idx.get(word.strip(',.\" ;:)(][?!'), 0) for word in sentence.split()] for sentence in source_sentences]\n",
    "    y = [[target_word2idx.get(word.strip(',.\" ;:)(][?!'), 0) for word in sentence.split()] for sentence in target_sentences]\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        n1 = len(x[i])\n",
    "        n2 = len(y[i])\n",
    "        n = n1 if n1 < n2 else n2 \n",
    "        if abs(n1 - n2) <= 0.3 * n:\n",
    "            if n1 <= 15 and n2 <= 15:\n",
    "                X.append(x[i])\n",
    "                Y.append(y[i])\n",
    "    return X, Y, source_word2idx, source_idx2word, source_vocab, target_word2idx, target_idx2word, target_vocab\n",
    "\n",
    "def save_dataset(file_path, obj):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(obj, f, -1)\n",
    "\n",
    "def read_dataset(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#en_sentences = ['hello my friend', 'we need to reboot the server']\n",
    "#es_sentences = ['hola mi amigo', 'necesitamos reiniciar el servidor']\n",
    "save_dataset('./data.pkl', create_dataset(en_sentences, es_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "X, Y, en_word2idx, en_idx2word, en_vocab, es_word2idx, es_idx2word, es_vocab = read_dataset('data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence in English - encoded: [8425, 3, 2, 1695]\n",
      "Sentence in Spanish - encoded: [10624, 13, 575, 4, 1420]\n",
      "Decoded:\n",
      "------------------------\n",
      "Resumption of the session \n",
      "\n",
      "Reanudación del período de sesiones\n"
     ]
    }
   ],
   "source": [
    "#CHECK THAT WORKs\n",
    "print 'Sentence in English - encoded:', X[0]\n",
    "print 'Sentence in Spanish - encoded:', Y[0]\n",
    "print 'Decoded:\\n------------------------'\n",
    "\n",
    "for i in range(len(X[0])):\n",
    "    print en_idx2word[X[0][i]],\n",
    "    \n",
    "print '\\n'\n",
    "\n",
    "for i in range(len(Y[0])):\n",
    "    print es_idx2word[Y[0][i]],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data processing\n",
    "\n",
    "# data padding\n",
    "def data_padding(x, y, length = 15):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = x[i] + (length - len(x[i])) * [en_word2idx['<pad>']]\n",
    "        y[i] = [es_word2idx['<go>']] + y[i] + [es_word2idx['<eos>']] + (length-len(y[i])) * [es_word2idx['<pad>']]\n",
    "\n",
    "data_padding(X, Y)\n",
    "print X\n",
    "\n",
    "# data splitting\n",
    "X_train,  X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1)\n",
    "\n",
    "del X\n",
    "del Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model\n",
    "\n",
    "input_seq_len = 15\n",
    "output_seq_len = 17\n",
    "en_vocab_size = len(en_vocab) + 2 # + <pad>, <ukn>\n",
    "es_vocab_size = len(es_vocab) + 4 # + <pad>, <ukn>, <eos>, <go>\n",
    "\n",
    "# placeholders\n",
    "encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'encoder{}'.format(i)) for i in range(input_seq_len)]\n",
    "decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'decoder{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "targets = [decoder_inputs[i+1] for i in range(output_seq_len-1)]\n",
    "# add one more target\n",
    "targets.append(tf.placeholder(dtype = tf.int32, shape = [None], name = 'last_target'))\n",
    "target_weights = [tf.placeholder(dtype = tf.float32, shape = [None], name = 'target_w{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "# output projection\n",
    "size = 512\n",
    "w_t = tf.get_variable('proj_w', [es_vocab_size, size], tf.float32)\n",
    "b = tf.get_variable('proj_b', [es_vocab_size], tf.float32)\n",
    "w = tf.transpose(w_t)\n",
    "output_projection = (w, b)\n",
    "\n",
    "outputs, states = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "                                            encoder_inputs,\n",
    "                                            decoder_inputs,\n",
    "                                            tf.contrib.rnn.BasicLSTMCell(size),\n",
    "                                            num_encoder_symbols = en_vocab_size,\n",
    "                                            num_decoder_symbols = es_vocab_size,\n",
    "                                            embedding_size = 100,\n",
    "                                            feed_previous = False,\n",
    "                                            output_projection = output_projection,\n",
    "                                            dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our loss function\n",
    "\n",
    "# sampled softmax loss - returns: A batch_size 1-D tensor of per-example sampled softmax losses\n",
    "def sampled_loss(labels, logits):\n",
    "    return tf.nn.sampled_softmax_loss(\n",
    "                        weights = w_t,\n",
    "                        biases = b,\n",
    "                        labels = tf.reshape(labels, [-1, 1]),\n",
    "                        inputs = logits,\n",
    "                        num_sampled = 512,\n",
    "                        num_classes = es_vocab_size)\n",
    "\n",
    "# Weighted cross-entropy loss for a sequence of logits\n",
    "loss = tf.contrib.legacy_seq2seq.sequence_loss(outputs, targets, target_weights, softmax_loss_function = sampled_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define some helper functions\n",
    "\n",
    "# simple softmax function\n",
    "def softmax(x):\n",
    "    n = np.max(x)\n",
    "    e_x = np.exp(x - n)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# feed data into placeholders\n",
    "def feed_dict(x, y, batch_size = 64):\n",
    "    feed = {}\n",
    "    \n",
    "    idxes = np.random.choice(len(x), size = batch_size, replace = False)\n",
    "    \n",
    "    for i in range(input_seq_len):\n",
    "        feed[encoder_inputs[i].name] = np.array([x[j][i] for j in idxes], dtype = np.int32)\n",
    "        \n",
    "    for i in range(output_seq_len):\n",
    "        feed[decoder_inputs[i].name] = np.array([y[j][i] for j in idxes], dtype = np.int32)\n",
    "        \n",
    "    feed[targets[len(targets)-1].name] = np.full(shape = [batch_size], fill_value = es_word2idx['<pad>'], dtype = np.int32)\n",
    "    \n",
    "    for i in range(output_seq_len-1):\n",
    "        batch_weights = np.ones(batch_size, dtype = np.float32)\n",
    "        target = feed[decoder_inputs[i+1].name]\n",
    "        for j in range(batch_size):\n",
    "            if target[j] == es_word2idx['<pad>']:\n",
    "                batch_weights[j] = 0.0\n",
    "        feed[target_weights[i].name] = batch_weights\n",
    "        \n",
    "    feed[target_weights[output_seq_len-1].name] = np.zeros(batch_size, dtype = np.float32)\n",
    "    \n",
    "    return feed\n",
    "\n",
    "# decode output sequence\n",
    "def decode_output(output_seq):\n",
    "    words = []\n",
    "    for i in range(output_seq_len):\n",
    "        smax = softmax(output_seq[i])\n",
    "        idx = np.argmax(smax)\n",
    "        words.append(es_idx2word[idx])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ops and hyperparameters\n",
    "learning_rate = 5e-3\n",
    "batch_size = 64\n",
    "steps = 10000\n",
    "\n",
    "# ops for projecting outputs\n",
    "outputs_proj = [tf.matmul(outputs[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "\n",
    "# training op\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# init op\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# forward step\n",
    "def forward_step(sess, feed):\n",
    "    output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
    "    return output_sequences\n",
    "\n",
    "# training step\n",
    "def backward_step(sess, feed):\n",
    "    sess.run(optimizer, feed_dict = feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------TRAINING------------------\n",
      "step: 0, loss: 8.9650554657\n",
      "step: 4, loss: 9.09259319305\n",
      "step: 9, loss: 8.96600341797\n",
      "step: 14, loss: 9.08163356781\n",
      "step: 19, loss: 9.04584121704\n",
      "Checkpoint is saved\n",
      "step: 24, loss: 8.9775428772\n",
      "step: 29, loss: 8.96202087402\n",
      "step: 34, loss: 9.04169273376\n",
      "step: 39, loss: 8.95137405396\n",
      "Checkpoint is saved\n",
      "step: 44, loss: 8.84370326996\n",
      "step: 49, loss: 8.92049789429\n",
      "step: 54, loss: 8.74224281311\n",
      "step: 59, loss: 8.30894088745\n",
      "Checkpoint is saved\n",
      "step: 64, loss: 7.69854354858\n",
      "step: 69, loss: 6.97393226624\n",
      "step: 74, loss: 7.30864953995\n",
      "step: 79, loss: 6.32648658752\n",
      "Checkpoint is saved\n",
      "step: 84, loss: 6.11800193787\n",
      "step: 89, loss: 6.25610637665\n",
      "step: 94, loss: 6.08073997498\n",
      "step: 99, loss: 6.0511264801\n",
      "Checkpoint is saved\n",
      "step: 104, loss: 6.38327121735\n",
      "step: 109, loss: 5.84017515182\n",
      "step: 114, loss: 5.84495449066\n",
      "step: 119, loss: 5.73464488983\n",
      "Checkpoint is saved\n",
      "step: 124, loss: 5.48550701141\n",
      "step: 129, loss: 5.4876832962\n",
      "step: 134, loss: 5.95776224136\n",
      "step: 139, loss: 5.37716579437\n",
      "Checkpoint is saved\n",
      "step: 144, loss: 6.37731838226\n",
      "step: 149, loss: 5.29277992249\n",
      "step: 154, loss: 5.33879470825\n",
      "step: 159, loss: 5.16882944107\n",
      "Checkpoint is saved\n",
      "step: 164, loss: 5.07839584351\n",
      "step: 169, loss: 5.00301885605\n",
      "step: 174, loss: 5.14625406265\n",
      "step: 179, loss: 5.28105449677\n",
      "Checkpoint is saved\n",
      "step: 184, loss: 4.91368436813\n",
      "step: 189, loss: 5.35933637619\n",
      "step: 194, loss: 4.79110002518\n",
      "step: 199, loss: 4.7047662735\n",
      "Checkpoint is saved\n",
      "step: 204, loss: 4.53891849518\n",
      "step: 209, loss: 4.75942897797\n",
      "step: 214, loss: 4.64212608337\n",
      "step: 219, loss: 4.76759004593\n",
      "Checkpoint is saved\n",
      "step: 224, loss: 4.52348947525\n",
      "step: 229, loss: 4.01287794113\n",
      "step: 234, loss: 4.14737319946\n",
      "step: 239, loss: 3.95242357254\n",
      "Checkpoint is saved\n",
      "step: 244, loss: 4.48842954636\n",
      "step: 249, loss: 3.95320224762\n",
      "step: 254, loss: 3.8589861393\n",
      "step: 259, loss: 3.61796998978\n",
      "Checkpoint is saved\n",
      "step: 264, loss: 3.7663564682\n",
      "step: 269, loss: 3.78564596176\n",
      "step: 274, loss: 3.35341930389\n",
      "step: 279, loss: 3.53601551056\n",
      "Checkpoint is saved\n",
      "step: 284, loss: 3.9609837532\n",
      "step: 289, loss: 3.76328039169\n",
      "step: 294, loss: 3.58888292313\n",
      "step: 299, loss: 3.52340459824\n",
      "Checkpoint is saved\n",
      "step: 304, loss: 3.81747055054\n",
      "step: 309, loss: 3.65629005432\n",
      "step: 314, loss: 3.42579460144\n",
      "step: 319, loss: 3.11258840561\n",
      "Checkpoint is saved\n",
      "step: 324, loss: 3.38104271889\n",
      "step: 329, loss: 3.03279733658\n",
      "step: 334, loss: 3.15929627419\n",
      "step: 339, loss: 2.7646317482\n",
      "Checkpoint is saved\n",
      "step: 344, loss: 3.0671749115\n",
      "step: 349, loss: 3.0868396759\n",
      "step: 354, loss: 3.54534864426\n",
      "step: 359, loss: 3.21301436424\n",
      "Checkpoint is saved\n",
      "step: 364, loss: 2.7688369751\n",
      "step: 369, loss: 3.00003886223\n",
      "step: 374, loss: 3.14465546608\n",
      "step: 379, loss: 2.75990700722\n",
      "Checkpoint is saved\n",
      "step: 384, loss: 2.91661024094\n",
      "step: 389, loss: 2.84761667252\n",
      "step: 394, loss: 2.90990281105\n",
      "step: 399, loss: 2.88645076752\n",
      "Checkpoint is saved\n",
      "step: 404, loss: 2.86371541023\n",
      "step: 409, loss: 2.44995832443\n",
      "step: 414, loss: 2.78647255898\n",
      "step: 419, loss: 2.44207859039\n",
      "Checkpoint is saved\n",
      "step: 424, loss: 2.77891802788\n",
      "step: 429, loss: 2.45855426788\n",
      "step: 434, loss: 2.73725342751\n",
      "step: 439, loss: 2.57606220245\n",
      "Checkpoint is saved\n",
      "step: 444, loss: 2.23702335358\n",
      "step: 449, loss: 2.29312610626\n",
      "step: 454, loss: 2.34238004684\n",
      "step: 459, loss: 2.33773612976\n",
      "Checkpoint is saved\n",
      "step: 464, loss: 2.62192964554\n",
      "step: 469, loss: 2.01441025734\n",
      "step: 474, loss: 1.96460866928\n",
      "step: 479, loss: 2.27501153946\n",
      "Checkpoint is saved\n",
      "step: 484, loss: 2.15898108482\n",
      "step: 489, loss: 2.41390371323\n",
      "step: 494, loss: 1.98770332336\n",
      "step: 499, loss: 2.14561605453\n",
      "Checkpoint is saved\n",
      "step: 504, loss: 2.00014853477\n",
      "step: 509, loss: 1.96183621883\n",
      "step: 514, loss: 2.5506606102\n",
      "step: 519, loss: 1.91621518135\n",
      "Checkpoint is saved\n",
      "step: 524, loss: 2.01576280594\n",
      "step: 529, loss: 2.12286758423\n",
      "step: 534, loss: 2.1236178875\n",
      "step: 539, loss: 2.19372057915\n",
      "Checkpoint is saved\n",
      "step: 544, loss: 1.77194094658\n",
      "step: 549, loss: 2.26102972031\n",
      "step: 554, loss: 1.82494902611\n",
      "step: 559, loss: 2.06247138977\n",
      "Checkpoint is saved\n",
      "step: 564, loss: 1.88836622238\n",
      "step: 569, loss: 2.08454322815\n",
      "step: 574, loss: 1.83027327061\n",
      "step: 579, loss: 1.97589278221\n",
      "Checkpoint is saved\n",
      "step: 584, loss: 1.91477680206\n",
      "step: 589, loss: 1.77446424961\n",
      "step: 594, loss: 1.71821808815\n",
      "step: 599, loss: 1.78221607208\n",
      "Checkpoint is saved\n",
      "step: 604, loss: 1.77133870125\n",
      "step: 609, loss: 1.96844410896\n",
      "step: 614, loss: 1.85254776478\n",
      "step: 619, loss: 1.84432339668\n",
      "Checkpoint is saved\n",
      "step: 624, loss: 1.64931964874\n",
      "step: 629, loss: 2.21129322052\n",
      "step: 634, loss: 1.66816926003\n",
      "step: 639, loss: 1.95866060257\n",
      "Checkpoint is saved\n",
      "step: 644, loss: 1.83723676205\n",
      "step: 649, loss: 1.95064735413\n",
      "step: 654, loss: 1.40708434582\n",
      "step: 659, loss: 1.58996617794\n",
      "Checkpoint is saved\n",
      "step: 664, loss: 1.85596990585\n",
      "step: 669, loss: 1.81077218056\n",
      "step: 674, loss: 1.53613317013\n",
      "step: 679, loss: 1.83291018009\n",
      "Checkpoint is saved\n",
      "step: 684, loss: 1.61691975594\n",
      "step: 689, loss: 1.57243585587\n",
      "step: 694, loss: 1.62682318687\n",
      "step: 699, loss: 1.99172496796\n",
      "Checkpoint is saved\n",
      "step: 704, loss: 1.72041654587\n",
      "step: 709, loss: 1.70867967606\n",
      "step: 714, loss: 1.87890505791\n",
      "step: 719, loss: 1.56214845181\n",
      "Checkpoint is saved\n",
      "step: 724, loss: 1.79141616821\n",
      "step: 729, loss: 1.74061846733\n",
      "step: 734, loss: 1.45749378204\n",
      "step: 739, loss: 1.29757106304\n",
      "Checkpoint is saved\n",
      "step: 744, loss: 1.72047972679\n",
      "step: 749, loss: 1.85052239895\n",
      "step: 754, loss: 1.58217024803\n",
      "step: 759, loss: 1.3992010355\n",
      "Checkpoint is saved\n",
      "step: 764, loss: 1.35177230835\n",
      "step: 769, loss: 1.38100934029\n",
      "step: 774, loss: 1.88886928558\n",
      "step: 779, loss: 1.49679899216\n",
      "Checkpoint is saved\n",
      "step: 784, loss: 1.48363828659\n",
      "step: 789, loss: 1.33165621758\n",
      "step: 794, loss: 1.5928401947\n",
      "step: 799, loss: 1.29865288734\n",
      "Checkpoint is saved\n",
      "step: 804, loss: 1.58947587013\n",
      "step: 809, loss: 1.40903067589\n",
      "step: 814, loss: 1.28847301006\n",
      "step: 819, loss: 1.33510160446\n",
      "Checkpoint is saved\n",
      "step: 824, loss: 1.60296809673\n",
      "step: 829, loss: 1.20667362213\n",
      "step: 834, loss: 1.49770712852\n",
      "step: 839, loss: 1.33688211441\n",
      "Checkpoint is saved\n",
      "step: 844, loss: 1.52961707115\n",
      "step: 849, loss: 1.48677444458\n",
      "step: 854, loss: 1.3789576292\n",
      "step: 859, loss: 1.33724093437\n",
      "Checkpoint is saved\n",
      "step: 864, loss: 1.57022547722\n",
      "step: 869, loss: 1.30129337311\n",
      "step: 874, loss: 1.49850618839\n",
      "step: 879, loss: 1.21114933491\n",
      "Checkpoint is saved\n",
      "step: 884, loss: 1.1982998848\n",
      "step: 889, loss: 1.52756071091\n",
      "step: 894, loss: 1.4643406868\n",
      "step: 899, loss: 1.46144723892\n",
      "Checkpoint is saved\n",
      "step: 904, loss: 1.42141270638\n",
      "step: 909, loss: 1.28829860687\n",
      "step: 914, loss: 1.40643525124\n",
      "step: 919, loss: 1.17361545563\n",
      "Checkpoint is saved\n",
      "step: 924, loss: 1.45689487457\n",
      "step: 929, loss: 1.29146742821\n",
      "step: 934, loss: 1.62030053139\n",
      "step: 939, loss: 1.39700210094\n",
      "Checkpoint is saved\n",
      "step: 944, loss: 1.1205072403\n",
      "step: 949, loss: 1.35507774353\n",
      "step: 954, loss: 1.28263664246\n",
      "step: 959, loss: 1.34377098083\n",
      "Checkpoint is saved\n",
      "step: 964, loss: 1.12943172455\n",
      "step: 969, loss: 1.11587071419\n",
      "step: 974, loss: 1.11186313629\n",
      "step: 979, loss: 1.66475951672\n",
      "Checkpoint is saved\n",
      "step: 984, loss: 0.999220490456\n",
      "step: 989, loss: 1.11343884468\n",
      "step: 994, loss: 1.19580364227\n",
      "step: 999, loss: 1.23788070679\n",
      "Checkpoint is saved\n",
      "step: 1004, loss: 1.15025138855\n",
      "step: 1009, loss: 1.18976807594\n",
      "step: 1014, loss: 1.22422218323\n",
      "step: 1019, loss: 1.2071570158\n",
      "Checkpoint is saved\n",
      "step: 1024, loss: 1.33793401718\n",
      "step: 1029, loss: 1.41899681091\n",
      "step: 1034, loss: 1.23779177666\n",
      "step: 1039, loss: 1.08775091171\n",
      "Checkpoint is saved\n",
      "step: 1044, loss: 1.23043513298\n",
      "step: 1049, loss: 1.54579126835\n",
      "step: 1054, loss: 1.09218263626\n",
      "step: 1059, loss: 1.23198866844\n",
      "Checkpoint is saved\n",
      "step: 1064, loss: 1.22510075569\n",
      "step: 1069, loss: 1.19251334667\n",
      "step: 1074, loss: 1.21494102478\n",
      "step: 1079, loss: 1.23155891895\n",
      "Checkpoint is saved\n",
      "step: 1084, loss: 1.07537770271\n",
      "step: 1089, loss: 1.12022662163\n",
      "step: 1094, loss: 1.52556991577\n",
      "step: 1099, loss: 1.17567300797\n",
      "Checkpoint is saved\n",
      "step: 1104, loss: 1.01845526695\n",
      "step: 1109, loss: 1.21878230572\n",
      "step: 1114, loss: 1.13158917427\n",
      "step: 1119, loss: 1.09903430939\n",
      "Checkpoint is saved\n",
      "step: 1124, loss: 1.15282189846\n",
      "step: 1129, loss: 0.922705292702\n",
      "step: 1134, loss: 1.36027050018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1139, loss: 1.01748478413\n",
      "Checkpoint is saved\n",
      "step: 1144, loss: 1.16948473454\n",
      "step: 1149, loss: 1.19262623787\n",
      "step: 1154, loss: 1.06632900238\n",
      "step: 1159, loss: 1.20566475391\n",
      "Checkpoint is saved\n",
      "step: 1164, loss: 1.260160923\n",
      "step: 1169, loss: 1.1423137188\n",
      "step: 1174, loss: 1.12538409233\n",
      "step: 1179, loss: 0.951745986938\n",
      "Checkpoint is saved\n",
      "step: 1184, loss: 1.10071599483\n",
      "step: 1189, loss: 1.23123860359\n",
      "step: 1194, loss: 1.28469920158\n",
      "step: 1199, loss: 1.06936144829\n",
      "Checkpoint is saved\n",
      "step: 1204, loss: 1.14399838448\n",
      "step: 1209, loss: 0.9406504035\n",
      "step: 1214, loss: 1.11304366589\n",
      "step: 1219, loss: 1.05500853062\n",
      "Checkpoint is saved\n",
      "step: 1224, loss: 1.23689055443\n",
      "step: 1229, loss: 1.10858821869\n",
      "step: 1234, loss: 1.08985579014\n",
      "step: 1239, loss: 1.02006995678\n",
      "Checkpoint is saved\n",
      "step: 1244, loss: 1.10317599773\n",
      "step: 1249, loss: 0.88196849823\n",
      "step: 1254, loss: 1.03918588161\n",
      "step: 1259, loss: 1.03878998756\n",
      "Checkpoint is saved\n",
      "step: 1264, loss: 1.03968441486\n",
      "step: 1269, loss: 0.95664614439\n",
      "step: 1274, loss: 1.01464736462\n",
      "step: 1279, loss: 0.938502669334\n",
      "Checkpoint is saved\n",
      "step: 1284, loss: 0.934983730316\n",
      "step: 1289, loss: 1.04776620865\n",
      "step: 1294, loss: 0.855628728867\n",
      "step: 1299, loss: 0.893618106842\n",
      "Checkpoint is saved\n",
      "step: 1304, loss: 0.886786878109\n",
      "step: 1309, loss: 0.904984235764\n",
      "step: 1314, loss: 1.33204340935\n",
      "step: 1319, loss: 0.941603541374\n",
      "Checkpoint is saved\n",
      "step: 1324, loss: 0.870582222939\n",
      "step: 1329, loss: 0.938957929611\n",
      "step: 1334, loss: 1.20555019379\n",
      "step: 1339, loss: 0.97885197401\n",
      "Checkpoint is saved\n",
      "step: 1344, loss: 0.992077410221\n",
      "step: 1349, loss: 0.994935750961\n",
      "step: 1354, loss: 1.12226200104\n",
      "step: 1359, loss: 1.03164994717\n",
      "Checkpoint is saved\n",
      "step: 1364, loss: 1.03787338734\n",
      "step: 1369, loss: 1.02622413635\n",
      "step: 1374, loss: 0.89716386795\n",
      "step: 1379, loss: 0.967522978783\n",
      "Checkpoint is saved\n",
      "step: 1384, loss: 0.937291264534\n",
      "step: 1389, loss: 0.957510411739\n",
      "step: 1394, loss: 0.869899272919\n",
      "step: 1399, loss: 1.03030323982\n",
      "Checkpoint is saved\n",
      "step: 1404, loss: 1.17875671387\n",
      "step: 1409, loss: 1.00460338593\n",
      "step: 1414, loss: 1.03094065189\n",
      "step: 1419, loss: 0.848665177822\n",
      "Checkpoint is saved\n",
      "step: 1424, loss: 1.00866770744\n",
      "step: 1429, loss: 0.895425915718\n",
      "step: 1434, loss: 1.08766222\n",
      "step: 1439, loss: 1.00789010525\n",
      "Checkpoint is saved\n",
      "step: 1444, loss: 1.20743095875\n",
      "step: 1449, loss: 0.937864899635\n",
      "step: 1454, loss: 1.04308629036\n",
      "step: 1459, loss: 0.975180983543\n",
      "Checkpoint is saved\n",
      "step: 1464, loss: 1.02454221249\n",
      "step: 1469, loss: 1.08178853989\n",
      "step: 1474, loss: 1.10988521576\n",
      "step: 1479, loss: 1.10038113594\n",
      "Checkpoint is saved\n",
      "step: 1484, loss: 0.775191068649\n",
      "step: 1489, loss: 0.982281506062\n",
      "step: 1494, loss: 0.923675537109\n",
      "step: 1499, loss: 0.978844881058\n",
      "Checkpoint is saved\n",
      "step: 1504, loss: 0.915410637856\n",
      "step: 1509, loss: 1.05905902386\n",
      "step: 1514, loss: 0.738393306732\n",
      "step: 1519, loss: 0.817822635174\n",
      "Checkpoint is saved\n",
      "step: 1524, loss: 1.28650331497\n",
      "step: 1529, loss: 0.810752868652\n",
      "step: 1534, loss: 0.953008711338\n",
      "step: 1539, loss: 0.823464214802\n",
      "Checkpoint is saved\n",
      "step: 1544, loss: 0.967173457146\n",
      "step: 1549, loss: 0.781935691833\n",
      "step: 1554, loss: 0.785630345345\n",
      "step: 1559, loss: 1.12101173401\n",
      "Checkpoint is saved\n",
      "step: 1564, loss: 0.916214764118\n",
      "step: 1569, loss: 0.825114250183\n",
      "step: 1574, loss: 0.943802952766\n",
      "step: 1579, loss: 0.925549924374\n",
      "Checkpoint is saved\n",
      "step: 1584, loss: 1.25430262089\n",
      "step: 1589, loss: 0.869099020958\n",
      "step: 1594, loss: 0.940875768661\n",
      "step: 1599, loss: 0.84735417366\n",
      "Checkpoint is saved\n",
      "step: 1604, loss: 1.00972819328\n",
      "step: 1609, loss: 0.834512710571\n",
      "step: 1614, loss: 0.954483628273\n",
      "step: 1619, loss: 0.762866556644\n",
      "Checkpoint is saved\n",
      "step: 1624, loss: 0.985873699188\n",
      "step: 1629, loss: 1.0376765728\n",
      "step: 1634, loss: 0.963401436806\n",
      "step: 1639, loss: 0.86214774847\n",
      "Checkpoint is saved\n",
      "step: 1644, loss: 0.798957169056\n",
      "step: 1649, loss: 0.938171386719\n",
      "step: 1654, loss: 1.02772164345\n",
      "step: 1659, loss: 0.949646472931\n",
      "Checkpoint is saved\n",
      "step: 1664, loss: 0.955730676651\n",
      "step: 1669, loss: 0.936390340328\n",
      "step: 1674, loss: 0.695230007172\n",
      "step: 1679, loss: 0.813620209694\n",
      "Checkpoint is saved\n",
      "step: 1684, loss: 0.812908709049\n",
      "step: 1689, loss: 0.90572977066\n",
      "step: 1694, loss: 0.930181801319\n",
      "step: 1699, loss: 0.8236348629\n",
      "Checkpoint is saved\n",
      "step: 1704, loss: 0.921928286552\n",
      "step: 1709, loss: 0.778254985809\n",
      "step: 1714, loss: 0.703472852707\n",
      "step: 1719, loss: 0.969012200832\n",
      "Checkpoint is saved\n",
      "step: 1724, loss: 0.880083560944\n",
      "step: 1729, loss: 0.88882547617\n",
      "step: 1734, loss: 0.920682251453\n",
      "step: 1739, loss: 0.79346036911\n",
      "Checkpoint is saved\n",
      "step: 1744, loss: 0.667808532715\n",
      "step: 1749, loss: 0.820096969604\n",
      "step: 1754, loss: 0.932645082474\n",
      "step: 1759, loss: 0.961649417877\n",
      "Checkpoint is saved\n",
      "step: 1764, loss: 0.792287349701\n",
      "step: 1769, loss: 0.85337215662\n",
      "step: 1774, loss: 0.877933621407\n",
      "step: 1779, loss: 0.91823977232\n",
      "Checkpoint is saved\n",
      "step: 1784, loss: 0.647333145142\n",
      "step: 1789, loss: 0.841610014439\n",
      "step: 1794, loss: 0.768955290318\n",
      "step: 1799, loss: 0.641664862633\n",
      "Checkpoint is saved\n",
      "step: 1804, loss: 0.830578804016\n",
      "step: 1809, loss: 1.06863951683\n",
      "step: 1814, loss: 0.768411755562\n",
      "step: 1819, loss: 0.84395813942\n",
      "Checkpoint is saved\n",
      "step: 1824, loss: 0.800232410431\n",
      "step: 1829, loss: 0.628239274025\n",
      "step: 1834, loss: 0.780473470688\n",
      "step: 1839, loss: 0.795886099339\n",
      "Checkpoint is saved\n",
      "step: 1844, loss: 0.680709660053\n",
      "step: 1849, loss: 0.735145330429\n",
      "step: 1854, loss: 0.783382773399\n",
      "step: 1859, loss: 0.884350419044\n",
      "Checkpoint is saved\n",
      "step: 1864, loss: 0.842352032661\n",
      "step: 1869, loss: 0.882169783115\n",
      "step: 1874, loss: 0.844899117947\n",
      "step: 1879, loss: 0.798497796059\n",
      "Checkpoint is saved\n",
      "step: 1884, loss: 0.676165819168\n",
      "step: 1889, loss: 0.756681501865\n",
      "step: 1894, loss: 0.724797010422\n",
      "step: 1899, loss: 0.797459065914\n",
      "Checkpoint is saved\n",
      "step: 1904, loss: 0.767322003841\n",
      "step: 1909, loss: 0.697946608067\n",
      "step: 1914, loss: 0.797465443611\n",
      "step: 1919, loss: 0.754182934761\n",
      "Checkpoint is saved\n",
      "step: 1924, loss: 0.698167443275\n",
      "step: 1929, loss: 0.703909218311\n",
      "step: 1934, loss: 0.802441954613\n",
      "step: 1939, loss: 0.699179291725\n",
      "Checkpoint is saved\n",
      "step: 1944, loss: 0.75602465868\n",
      "step: 1949, loss: 0.625219643116\n",
      "step: 1954, loss: 0.838775217533\n",
      "step: 1959, loss: 0.807995080948\n",
      "Checkpoint is saved\n",
      "step: 1964, loss: 0.747001171112\n",
      "step: 1969, loss: 0.920379579067\n",
      "step: 1974, loss: 0.76118016243\n",
      "step: 1979, loss: 0.651897907257\n",
      "Checkpoint is saved\n",
      "step: 1984, loss: 0.87543296814\n",
      "step: 1989, loss: 0.807802379131\n",
      "step: 1994, loss: 0.804295539856\n",
      "step: 1999, loss: 0.80711042881\n",
      "Checkpoint is saved\n",
      "step: 2004, loss: 0.959323406219\n",
      "step: 2009, loss: 0.85634291172\n",
      "step: 2014, loss: 0.744298994541\n",
      "step: 2019, loss: 0.757491827011\n",
      "Checkpoint is saved\n",
      "step: 2024, loss: 0.727530241013\n",
      "step: 2029, loss: 0.724536418915\n",
      "step: 2034, loss: 0.796659708023\n",
      "step: 2039, loss: 0.678312420845\n",
      "Checkpoint is saved\n",
      "step: 2044, loss: 0.756510853767\n",
      "step: 2049, loss: 0.76389747858\n",
      "step: 2054, loss: 0.829505205154\n",
      "step: 2059, loss: 0.823472082615\n",
      "Checkpoint is saved\n",
      "step: 2064, loss: 0.765379190445\n",
      "step: 2069, loss: 0.644353151321\n",
      "step: 2074, loss: 0.745546758175\n",
      "step: 2079, loss: 0.843418061733\n",
      "Checkpoint is saved\n",
      "step: 2084, loss: 0.724864244461\n",
      "step: 2089, loss: 0.80943274498\n",
      "step: 2094, loss: 0.817204594612\n",
      "step: 2099, loss: 0.924366474152\n",
      "Checkpoint is saved\n",
      "step: 2104, loss: 0.827936172485\n",
      "step: 2109, loss: 0.818680644035\n",
      "step: 2114, loss: 0.60629594326\n",
      "step: 2119, loss: 0.636027753353\n",
      "Checkpoint is saved\n",
      "step: 2124, loss: 0.690325021744\n",
      "step: 2129, loss: 0.60761654377\n",
      "step: 2134, loss: 0.696227908134\n",
      "step: 2139, loss: 0.756756782532\n",
      "Checkpoint is saved\n",
      "step: 2144, loss: 0.882017433643\n",
      "step: 2149, loss: 0.755919337273\n",
      "step: 2154, loss: 0.762078940868\n",
      "step: 2159, loss: 0.672665834427\n",
      "Checkpoint is saved\n",
      "step: 2164, loss: 0.718514561653\n",
      "step: 2169, loss: 0.668504536152\n",
      "step: 2174, loss: 0.699176073074\n",
      "step: 2179, loss: 0.707272171974\n",
      "Checkpoint is saved\n",
      "step: 2184, loss: 0.736656069756\n",
      "step: 2189, loss: 0.791448354721\n",
      "step: 2194, loss: 0.850361466408\n",
      "step: 2199, loss: 0.845227718353\n",
      "Checkpoint is saved\n",
      "step: 2204, loss: 0.798032641411\n",
      "step: 2209, loss: 0.740918159485\n",
      "step: 2214, loss: 0.731688439846\n",
      "step: 2219, loss: 0.738865554333\n",
      "Checkpoint is saved\n",
      "step: 2224, loss: 0.941562592983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2229, loss: 0.758251368999\n",
      "step: 2234, loss: 0.848917841911\n",
      "step: 2239, loss: 0.770509541035\n",
      "Checkpoint is saved\n",
      "step: 2244, loss: 0.752563357353\n",
      "step: 2249, loss: 0.728128612041\n",
      "step: 2254, loss: 0.742596507072\n",
      "step: 2259, loss: 0.757927536964\n",
      "Checkpoint is saved\n",
      "step: 2264, loss: 0.787601351738\n",
      "step: 2269, loss: 0.654601573944\n",
      "step: 2274, loss: 0.813274979591\n",
      "step: 2279, loss: 0.719276428223\n",
      "Checkpoint is saved\n",
      "step: 2284, loss: 0.70891970396\n",
      "step: 2289, loss: 0.527855038643\n",
      "step: 2294, loss: 0.691080212593\n",
      "step: 2299, loss: 0.747519493103\n",
      "Checkpoint is saved\n",
      "step: 2304, loss: 0.852898955345\n",
      "step: 2309, loss: 0.805727481842\n",
      "step: 2314, loss: 0.707306265831\n",
      "step: 2319, loss: 0.63568842411\n",
      "Checkpoint is saved\n",
      "step: 2324, loss: 0.782061338425\n",
      "step: 2329, loss: 0.97177284956\n",
      "step: 2334, loss: 0.79211306572\n",
      "step: 2339, loss: 0.749987840652\n",
      "Checkpoint is saved\n",
      "step: 2344, loss: 0.68276655674\n",
      "step: 2349, loss: 0.685160577297\n",
      "step: 2354, loss: 0.694991111755\n",
      "step: 2359, loss: 0.755885124207\n",
      "Checkpoint is saved\n",
      "step: 2364, loss: 0.773347556591\n",
      "step: 2369, loss: 0.808686435223\n",
      "step: 2374, loss: 0.709432125092\n",
      "step: 2379, loss: 0.758565425873\n",
      "Checkpoint is saved\n",
      "step: 2384, loss: 0.652181863785\n",
      "step: 2389, loss: 0.737961888313\n",
      "step: 2394, loss: 0.686049938202\n",
      "step: 2399, loss: 0.602388381958\n",
      "Checkpoint is saved\n",
      "step: 2404, loss: 0.806983470917\n",
      "step: 2409, loss: 0.689781844616\n",
      "step: 2414, loss: 0.850750207901\n",
      "step: 2419, loss: 0.626288890839\n",
      "Checkpoint is saved\n",
      "step: 2424, loss: 0.779193758965\n",
      "step: 2429, loss: 0.876234412193\n",
      "step: 2434, loss: 0.646088659763\n",
      "step: 2439, loss: 0.753326296806\n",
      "Checkpoint is saved\n",
      "step: 2444, loss: 0.675346255302\n",
      "step: 2449, loss: 0.687493562698\n",
      "step: 2454, loss: 0.649382054806\n",
      "step: 2459, loss: 0.598340749741\n",
      "Checkpoint is saved\n",
      "step: 2464, loss: 0.696566343307\n",
      "step: 2469, loss: 0.78656065464\n",
      "step: 2474, loss: 0.855830550194\n",
      "step: 2479, loss: 0.746598899364\n",
      "Checkpoint is saved\n",
      "step: 2484, loss: 0.727890312672\n",
      "step: 2489, loss: 0.534801125526\n",
      "step: 2494, loss: 0.788023233414\n",
      "step: 2499, loss: 0.791080474854\n",
      "Checkpoint is saved\n",
      "step: 2504, loss: 0.694777667522\n",
      "step: 2509, loss: 0.62617701292\n",
      "step: 2514, loss: 0.720274984837\n",
      "step: 2519, loss: 0.737549066544\n",
      "Checkpoint is saved\n",
      "step: 2524, loss: 0.769190311432\n",
      "step: 2529, loss: 0.534996211529\n",
      "step: 2534, loss: 0.6376953125\n",
      "step: 2539, loss: 0.626858651638\n",
      "Checkpoint is saved\n",
      "step: 2544, loss: 0.595271885395\n",
      "step: 2549, loss: 0.780637145042\n",
      "step: 2554, loss: 0.651740312576\n",
      "step: 2559, loss: 0.572954416275\n",
      "Checkpoint is saved\n",
      "step: 2564, loss: 0.562088370323\n",
      "step: 2569, loss: 0.628822207451\n",
      "step: 2574, loss: 0.812255501747\n",
      "step: 2579, loss: 0.650549769402\n",
      "Checkpoint is saved\n",
      "step: 2584, loss: 0.708948791027\n",
      "step: 2589, loss: 0.684061169624\n",
      "step: 2594, loss: 0.64434492588\n",
      "step: 2599, loss: 0.590446710587\n",
      "Checkpoint is saved\n",
      "step: 2604, loss: 0.652312874794\n",
      "step: 2609, loss: 0.704329431057\n",
      "step: 2614, loss: 0.664357662201\n",
      "step: 2619, loss: 0.655375182629\n",
      "Checkpoint is saved\n",
      "step: 2624, loss: 0.786998271942\n",
      "step: 2629, loss: 0.635079145432\n",
      "step: 2634, loss: 0.522879004478\n",
      "step: 2639, loss: 0.708438754082\n",
      "Checkpoint is saved\n",
      "step: 2644, loss: 0.502569973469\n",
      "step: 2649, loss: 0.643200874329\n",
      "step: 2654, loss: 0.668840408325\n",
      "step: 2659, loss: 0.550134599209\n",
      "Checkpoint is saved\n",
      "step: 2664, loss: 0.640127599239\n",
      "step: 2669, loss: 0.638610601425\n",
      "step: 2674, loss: 0.724937677383\n",
      "step: 2679, loss: 0.690999984741\n",
      "Checkpoint is saved\n",
      "step: 2684, loss: 0.606803119183\n",
      "step: 2689, loss: 0.57490336895\n",
      "step: 2694, loss: 0.607626199722\n",
      "step: 2699, loss: 0.601369440556\n",
      "Checkpoint is saved\n",
      "step: 2704, loss: 0.556693851948\n",
      "step: 2709, loss: 0.685240149498\n",
      "step: 2714, loss: 0.683801054955\n",
      "step: 2719, loss: 0.589291989803\n",
      "Checkpoint is saved\n",
      "step: 2724, loss: 0.622469127178\n",
      "step: 2729, loss: 0.649905562401\n",
      "step: 2734, loss: 0.693816363811\n",
      "step: 2739, loss: 0.664607286453\n",
      "Checkpoint is saved\n",
      "step: 2744, loss: 0.576741337776\n",
      "step: 2749, loss: 0.684509932995\n",
      "step: 2754, loss: 0.543193101883\n",
      "step: 2759, loss: 0.502763867378\n",
      "Checkpoint is saved\n",
      "step: 2764, loss: 0.523649096489\n",
      "step: 2769, loss: 0.583503127098\n",
      "step: 2774, loss: 0.651218414307\n",
      "step: 2779, loss: 0.745921373367\n",
      "Checkpoint is saved\n",
      "step: 2784, loss: 0.595863580704\n",
      "step: 2789, loss: 0.647769987583\n",
      "step: 2794, loss: 0.596192359924\n",
      "step: 2799, loss: 0.475969910622\n",
      "Checkpoint is saved\n",
      "step: 2804, loss: 0.690269589424\n",
      "step: 2809, loss: 0.670135498047\n",
      "step: 2814, loss: 0.678528904915\n",
      "step: 2819, loss: 0.65626013279\n",
      "Checkpoint is saved\n",
      "step: 2824, loss: 0.631917357445\n",
      "step: 2829, loss: 0.552395522594\n",
      "step: 2834, loss: 0.667723119259\n",
      "step: 2839, loss: 0.727911174297\n",
      "Checkpoint is saved\n",
      "step: 2844, loss: 0.428760737181\n",
      "step: 2849, loss: 0.564188301563\n",
      "step: 2854, loss: 0.573705494404\n",
      "step: 2859, loss: 0.617704093456\n",
      "Checkpoint is saved\n",
      "step: 2864, loss: 0.675166368484\n",
      "step: 2869, loss: 0.769926667213\n",
      "step: 2874, loss: 0.811773657799\n",
      "step: 2879, loss: 0.606576383114\n",
      "Checkpoint is saved\n",
      "step: 2884, loss: 0.663621902466\n",
      "step: 2889, loss: 0.632812976837\n",
      "step: 2894, loss: 0.592017889023\n",
      "step: 2899, loss: 0.625584959984\n",
      "Checkpoint is saved\n",
      "step: 2904, loss: 0.669555842876\n",
      "step: 2909, loss: 0.608003735542\n",
      "step: 2914, loss: 0.636006116867\n",
      "step: 2919, loss: 0.706018328667\n",
      "Checkpoint is saved\n",
      "step: 2924, loss: 0.607296943665\n",
      "step: 2929, loss: 0.647897720337\n",
      "step: 2934, loss: 0.641978740692\n",
      "step: 2939, loss: 0.672124445438\n",
      "Checkpoint is saved\n",
      "step: 2944, loss: 0.59427511692\n",
      "step: 2949, loss: 0.680013775826\n",
      "step: 2954, loss: 0.562531173229\n",
      "step: 2959, loss: 0.635048866272\n",
      "Checkpoint is saved\n",
      "step: 2964, loss: 0.765783190727\n",
      "step: 2969, loss: 0.71403169632\n",
      "step: 2974, loss: 0.581393420696\n",
      "step: 2979, loss: 0.610229074955\n",
      "Checkpoint is saved\n",
      "step: 2984, loss: 0.563566863537\n",
      "step: 2989, loss: 0.601070523262\n",
      "step: 2994, loss: 0.518516242504\n",
      "step: 2999, loss: 0.845376610756\n",
      "Checkpoint is saved\n",
      "step: 3004, loss: 0.637175321579\n",
      "step: 3009, loss: 0.525556087494\n",
      "step: 3014, loss: 0.741138696671\n",
      "step: 3019, loss: 0.699736714363\n",
      "Checkpoint is saved\n",
      "step: 3024, loss: 0.651997447014\n",
      "step: 3029, loss: 0.658279120922\n",
      "step: 3034, loss: 0.509468436241\n",
      "step: 3039, loss: 0.716169297695\n",
      "Checkpoint is saved\n",
      "step: 3044, loss: 0.627090156078\n",
      "step: 3049, loss: 0.586991786957\n",
      "step: 3054, loss: 0.546021223068\n",
      "step: 3059, loss: 0.636554002762\n",
      "Checkpoint is saved\n",
      "step: 3064, loss: 0.611268341541\n",
      "step: 3069, loss: 0.66116476059\n",
      "step: 3074, loss: 0.545659542084\n",
      "step: 3079, loss: 0.511827349663\n",
      "Checkpoint is saved\n",
      "step: 3084, loss: 0.607665836811\n",
      "step: 3089, loss: 0.617318093777\n",
      "step: 3094, loss: 0.615823149681\n",
      "step: 3099, loss: 0.636763453484\n",
      "Checkpoint is saved\n",
      "step: 3104, loss: 0.537535786629\n",
      "step: 3109, loss: 0.726003289223\n",
      "step: 3114, loss: 0.560200691223\n",
      "step: 3119, loss: 0.585059583187\n",
      "Checkpoint is saved\n",
      "step: 3124, loss: 0.648560702801\n",
      "step: 3129, loss: 0.701724767685\n",
      "step: 3134, loss: 0.63386631012\n",
      "step: 3139, loss: 0.753263831139\n",
      "Checkpoint is saved\n",
      "step: 3144, loss: 0.616289377213\n",
      "step: 3149, loss: 0.60237121582\n",
      "step: 3154, loss: 0.604664802551\n",
      "step: 3159, loss: 0.652690410614\n",
      "Checkpoint is saved\n",
      "step: 3164, loss: 0.520191609859\n",
      "step: 3169, loss: 0.595525503159\n",
      "step: 3174, loss: 0.503495633602\n",
      "step: 3179, loss: 0.630372166634\n",
      "Checkpoint is saved\n",
      "step: 3184, loss: 0.621134877205\n",
      "step: 3189, loss: 0.482646226883\n",
      "step: 3194, loss: 0.620609641075\n",
      "step: 3199, loss: 0.472305238247\n",
      "Checkpoint is saved\n",
      "step: 3204, loss: 0.606593251228\n",
      "step: 3209, loss: 0.561448812485\n",
      "step: 3214, loss: 0.522345304489\n",
      "step: 3219, loss: 0.648016452789\n",
      "Checkpoint is saved\n",
      "step: 3224, loss: 0.542209506035\n",
      "step: 3229, loss: 0.637277662754\n",
      "step: 3234, loss: 0.505999565125\n",
      "step: 3239, loss: 0.568000853062\n",
      "Checkpoint is saved\n",
      "step: 3244, loss: 0.688589453697\n",
      "step: 3249, loss: 0.648669600487\n",
      "step: 3254, loss: 0.559383034706\n",
      "step: 3259, loss: 0.698803186417\n",
      "Checkpoint is saved\n",
      "step: 3264, loss: 0.622582376003\n",
      "step: 3269, loss: 0.581756293774\n",
      "step: 3274, loss: 0.608488023281\n",
      "step: 3279, loss: 0.618964791298\n",
      "Checkpoint is saved\n",
      "step: 3284, loss: 0.471164703369\n",
      "step: 3289, loss: 0.74640417099\n",
      "step: 3294, loss: 0.572031259537\n",
      "step: 3299, loss: 0.638191640377\n",
      "Checkpoint is saved\n",
      "step: 3304, loss: 0.662996292114\n",
      "step: 3309, loss: 0.515912890434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3314, loss: 0.572088837624\n",
      "step: 3319, loss: 0.568711996078\n",
      "Checkpoint is saved\n",
      "step: 3324, loss: 0.60239136219\n",
      "step: 3329, loss: 0.642002165318\n",
      "step: 3334, loss: 0.679989099503\n",
      "step: 3339, loss: 0.702555060387\n",
      "Checkpoint is saved\n",
      "step: 3344, loss: 0.636394202709\n",
      "step: 3349, loss: 0.566143870354\n",
      "step: 3354, loss: 0.521510481834\n",
      "step: 3359, loss: 0.580823302269\n",
      "Checkpoint is saved\n",
      "step: 3364, loss: 0.662432670593\n",
      "step: 3369, loss: 0.601620435715\n",
      "step: 3374, loss: 0.522187232971\n",
      "step: 3379, loss: 0.584721744061\n",
      "Checkpoint is saved\n",
      "step: 3384, loss: 0.505610227585\n",
      "step: 3389, loss: 0.646488785744\n",
      "step: 3394, loss: 0.607615053654\n",
      "step: 3399, loss: 0.665447413921\n",
      "Checkpoint is saved\n",
      "step: 3404, loss: 0.605726242065\n",
      "step: 3409, loss: 0.570219099522\n",
      "step: 3414, loss: 0.583872199059\n",
      "step: 3419, loss: 0.63631439209\n",
      "Checkpoint is saved\n",
      "step: 3424, loss: 0.678439617157\n",
      "step: 3429, loss: 0.519179522991\n",
      "step: 3434, loss: 0.656097352505\n",
      "step: 3439, loss: 0.547735989094\n",
      "Checkpoint is saved\n",
      "step: 3444, loss: 0.418126165867\n",
      "step: 3449, loss: 0.709856987\n",
      "step: 3454, loss: 0.575628876686\n",
      "step: 3459, loss: 0.652327120304\n",
      "Checkpoint is saved\n",
      "step: 3464, loss: 0.557412445545\n",
      "step: 3469, loss: 0.633296608925\n",
      "step: 3474, loss: 0.523744881153\n",
      "step: 3479, loss: 0.62731307745\n",
      "Checkpoint is saved\n",
      "step: 3484, loss: 0.723893523216\n",
      "step: 3489, loss: 0.567179501057\n",
      "step: 3494, loss: 0.640362024307\n",
      "step: 3499, loss: 0.559080243111\n",
      "Checkpoint is saved\n",
      "step: 3504, loss: 0.53841984272\n",
      "step: 3509, loss: 0.568952083588\n",
      "step: 3514, loss: 0.681192874908\n",
      "step: 3519, loss: 0.621473014355\n",
      "Checkpoint is saved\n",
      "step: 3524, loss: 0.425185233355\n",
      "step: 3529, loss: 0.624502539635\n",
      "step: 3534, loss: 0.554553449154\n",
      "step: 3539, loss: 0.518030107021\n",
      "Checkpoint is saved\n",
      "step: 3544, loss: 0.562817692757\n",
      "step: 3549, loss: 0.516055464745\n",
      "step: 3554, loss: 0.620536088943\n",
      "step: 3559, loss: 0.735377669334\n",
      "Checkpoint is saved\n",
      "step: 3564, loss: 0.543935775757\n",
      "step: 3569, loss: 0.677819848061\n",
      "step: 3574, loss: 0.693696379662\n",
      "step: 3579, loss: 0.581872344017\n",
      "Checkpoint is saved\n",
      "step: 3584, loss: 0.669261574745\n",
      "step: 3589, loss: 0.674177467823\n",
      "step: 3594, loss: 0.511783123016\n",
      "step: 3599, loss: 0.543609142303\n",
      "Checkpoint is saved\n",
      "step: 3604, loss: 0.5527780056\n",
      "step: 3609, loss: 0.511541128159\n",
      "step: 3614, loss: 0.450749188662\n",
      "step: 3619, loss: 0.534659743309\n",
      "Checkpoint is saved\n",
      "step: 3624, loss: 0.622663855553\n",
      "step: 3629, loss: 0.65940195322\n",
      "step: 3634, loss: 0.440999090672\n",
      "step: 3639, loss: 0.649341225624\n",
      "Checkpoint is saved\n",
      "step: 3644, loss: 0.681562066078\n",
      "step: 3649, loss: 0.611099302769\n",
      "step: 3654, loss: 0.566838622093\n",
      "step: 3659, loss: 0.520253479481\n",
      "Checkpoint is saved\n",
      "step: 3664, loss: 0.527669131756\n",
      "step: 3669, loss: 0.525443077087\n",
      "step: 3674, loss: 0.517016947269\n",
      "step: 3679, loss: 0.62656211853\n",
      "Checkpoint is saved\n",
      "step: 3684, loss: 0.517887294292\n",
      "step: 3689, loss: 0.522984743118\n",
      "step: 3694, loss: 0.62385892868\n",
      "step: 3699, loss: 0.645336508751\n",
      "Checkpoint is saved\n",
      "step: 3704, loss: 0.503601789474\n",
      "step: 3709, loss: 0.60443931818\n",
      "step: 3714, loss: 0.520047605038\n",
      "step: 3719, loss: 0.680312097073\n",
      "Checkpoint is saved\n",
      "step: 3724, loss: 0.637432694435\n",
      "step: 3729, loss: 0.53186404705\n",
      "step: 3734, loss: 0.59279358387\n",
      "step: 3739, loss: 0.623590826988\n",
      "Checkpoint is saved\n",
      "step: 3744, loss: 0.533331036568\n",
      "step: 3749, loss: 0.538631141186\n",
      "step: 3754, loss: 0.51741951704\n",
      "step: 3759, loss: 0.601528942585\n",
      "Checkpoint is saved\n",
      "step: 3764, loss: 0.521696329117\n",
      "step: 3769, loss: 0.567914247513\n",
      "step: 3774, loss: 0.642186164856\n",
      "step: 3779, loss: 0.57996237278\n",
      "Checkpoint is saved\n",
      "step: 3784, loss: 0.593671739101\n",
      "step: 3789, loss: 0.495665937662\n",
      "step: 3794, loss: 0.440398037434\n",
      "step: 3799, loss: 0.669238269329\n",
      "Checkpoint is saved\n",
      "step: 3804, loss: 0.66688221693\n",
      "step: 3809, loss: 0.522351026535\n",
      "step: 3814, loss: 0.627315282822\n",
      "step: 3819, loss: 0.568627476692\n",
      "Checkpoint is saved\n",
      "step: 3824, loss: 0.616154193878\n",
      "step: 3829, loss: 0.525995373726\n",
      "step: 3834, loss: 0.623209238052\n",
      "step: 3839, loss: 0.626127123833\n",
      "Checkpoint is saved\n",
      "step: 3844, loss: 0.520168781281\n",
      "step: 3849, loss: 0.617077827454\n",
      "step: 3854, loss: 0.595125257969\n",
      "step: 3859, loss: 0.608192682266\n",
      "Checkpoint is saved\n",
      "step: 3864, loss: 0.716376185417\n",
      "step: 3869, loss: 0.626716911793\n",
      "step: 3874, loss: 0.60942286253\n",
      "step: 3879, loss: 0.507828772068\n",
      "Checkpoint is saved\n",
      "step: 3884, loss: 0.509356856346\n",
      "step: 3889, loss: 0.541715145111\n",
      "step: 3894, loss: 0.698161959648\n",
      "step: 3899, loss: 0.592841982841\n",
      "Checkpoint is saved\n",
      "step: 3904, loss: 0.57428753376\n",
      "step: 3909, loss: 0.529075741768\n",
      "step: 3914, loss: 0.532164692879\n",
      "step: 3919, loss: 0.642589569092\n",
      "Checkpoint is saved\n",
      "step: 3924, loss: 0.575177669525\n",
      "step: 3929, loss: 0.616862475872\n",
      "step: 3934, loss: 0.572818994522\n",
      "step: 3939, loss: 0.565346956253\n",
      "Checkpoint is saved\n",
      "step: 3944, loss: 0.576340317726\n",
      "step: 3949, loss: 0.723433017731\n",
      "step: 3954, loss: 0.567456245422\n",
      "step: 3959, loss: 0.603401660919\n",
      "Checkpoint is saved\n",
      "step: 3964, loss: 0.576566636562\n",
      "step: 3969, loss: 0.562540531158\n",
      "step: 3974, loss: 0.529069602489\n",
      "step: 3979, loss: 0.73944914341\n",
      "Checkpoint is saved\n",
      "step: 3984, loss: 0.692852854729\n",
      "step: 3989, loss: 0.68743032217\n",
      "step: 3994, loss: 0.523557305336\n",
      "step: 3999, loss: 0.356623470783\n",
      "Checkpoint is saved\n",
      "step: 4004, loss: 0.531898498535\n",
      "step: 4009, loss: 0.526961684227\n",
      "step: 4014, loss: 0.585639894009\n",
      "step: 4019, loss: 0.388386189938\n",
      "Checkpoint is saved\n",
      "step: 4024, loss: 0.403374999762\n",
      "step: 4029, loss: 0.47149258852\n",
      "step: 4034, loss: 0.50630402565\n",
      "step: 4039, loss: 0.642908871174\n",
      "Checkpoint is saved\n",
      "step: 4044, loss: 0.536296367645\n",
      "step: 4049, loss: 0.467786490917\n",
      "step: 4054, loss: 0.594552516937\n",
      "step: 4059, loss: 0.602097511292\n",
      "Checkpoint is saved\n",
      "step: 4064, loss: 0.602743387222\n",
      "step: 4069, loss: 0.454421401024\n",
      "step: 4074, loss: 0.5408462286\n",
      "step: 4079, loss: 0.557138860226\n",
      "Checkpoint is saved\n",
      "step: 4084, loss: 0.682706952095\n",
      "step: 4089, loss: 0.443723231554\n",
      "step: 4094, loss: 0.554658532143\n",
      "step: 4099, loss: 0.515871882439\n",
      "Checkpoint is saved\n",
      "step: 4104, loss: 0.551375508308\n",
      "step: 4109, loss: 0.649288833141\n",
      "step: 4114, loss: 0.486610233784\n",
      "step: 4119, loss: 0.722503185272\n",
      "Checkpoint is saved\n",
      "step: 4124, loss: 0.586831152439\n",
      "step: 4129, loss: 0.521660447121\n",
      "step: 4134, loss: 0.55838239193\n",
      "step: 4139, loss: 0.498411953449\n",
      "Checkpoint is saved\n",
      "step: 4144, loss: 0.505744576454\n",
      "step: 4149, loss: 0.528722703457\n",
      "step: 4154, loss: 0.479110151529\n",
      "step: 4159, loss: 0.468838691711\n",
      "Checkpoint is saved\n",
      "step: 4164, loss: 0.612587213516\n",
      "step: 4169, loss: 0.408858299255\n",
      "step: 4174, loss: 0.578382194042\n",
      "step: 4179, loss: 0.592378497124\n",
      "Checkpoint is saved\n",
      "step: 4184, loss: 0.546201586723\n",
      "step: 4189, loss: 0.64629638195\n",
      "step: 4194, loss: 0.598945021629\n",
      "step: 4199, loss: 0.569506525993\n",
      "Checkpoint is saved\n",
      "step: 4204, loss: 0.490462869406\n",
      "step: 4209, loss: 0.586818397045\n",
      "step: 4214, loss: 0.710114836693\n",
      "step: 4219, loss: 0.614216387272\n",
      "Checkpoint is saved\n",
      "step: 4224, loss: 0.485582172871\n",
      "step: 4229, loss: 0.647896468639\n",
      "step: 4234, loss: 0.612080037594\n",
      "step: 4239, loss: 0.559428334236\n",
      "Checkpoint is saved\n",
      "step: 4244, loss: 0.575415432453\n",
      "step: 4249, loss: 0.659384608269\n",
      "step: 4254, loss: 0.575529336929\n",
      "step: 4259, loss: 0.642677485943\n",
      "Checkpoint is saved\n",
      "step: 4264, loss: 0.535653233528\n",
      "step: 4269, loss: 0.436163067818\n",
      "step: 4274, loss: 0.514871954918\n",
      "step: 4279, loss: 0.560814619064\n",
      "Checkpoint is saved\n",
      "step: 4284, loss: 0.48960518837\n",
      "step: 4289, loss: 0.478031516075\n",
      "step: 4294, loss: 0.588671863079\n",
      "step: 4299, loss: 0.677182137966\n",
      "Checkpoint is saved\n",
      "step: 4304, loss: 0.632069349289\n",
      "step: 4309, loss: 0.67426431179\n",
      "step: 4314, loss: 0.645708918571\n",
      "step: 4319, loss: 0.503635525703\n",
      "Checkpoint is saved\n",
      "step: 4324, loss: 0.512833178043\n",
      "step: 4329, loss: 0.515398204327\n",
      "step: 4334, loss: 0.602530896664\n",
      "step: 4339, loss: 0.467014789581\n",
      "Checkpoint is saved\n",
      "step: 4344, loss: 0.54309284687\n",
      "step: 4349, loss: 0.604811966419\n",
      "step: 4354, loss: 0.522363722324\n",
      "step: 4359, loss: 0.572999835014\n",
      "Checkpoint is saved\n",
      "step: 4364, loss: 0.484511315823\n",
      "step: 4369, loss: 0.538717806339\n",
      "step: 4374, loss: 0.396398276091\n",
      "step: 4379, loss: 0.680704593658\n",
      "Checkpoint is saved\n",
      "step: 4384, loss: 0.601667881012\n",
      "step: 4389, loss: 0.553395092487\n",
      "step: 4394, loss: 0.502148270607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4399, loss: 0.505971431732\n",
      "Checkpoint is saved\n",
      "step: 4404, loss: 0.793555378914\n",
      "step: 4409, loss: 0.572595596313\n",
      "step: 4414, loss: 0.601051092148\n",
      "step: 4419, loss: 0.623666644096\n",
      "Checkpoint is saved\n",
      "step: 4424, loss: 0.606485009193\n",
      "step: 4429, loss: 0.544421195984\n",
      "step: 4434, loss: 0.585327506065\n",
      "step: 4439, loss: 0.552182555199\n",
      "Checkpoint is saved\n",
      "step: 4444, loss: 0.565583646297\n",
      "step: 4449, loss: 0.40988856554\n",
      "step: 4454, loss: 0.467222541571\n",
      "step: 4459, loss: 0.526784062386\n",
      "Checkpoint is saved\n",
      "step: 4464, loss: 0.540553569794\n",
      "step: 4469, loss: 0.59456461668\n",
      "step: 4474, loss: 0.534188807011\n",
      "step: 4479, loss: 0.490632265806\n",
      "Checkpoint is saved\n",
      "step: 4484, loss: 0.448954820633\n",
      "step: 4489, loss: 0.563126683235\n",
      "step: 4494, loss: 0.502766907215\n",
      "step: 4499, loss: 0.445716947317\n",
      "Checkpoint is saved\n",
      "step: 4504, loss: 0.58954757452\n",
      "step: 4509, loss: 0.704829990864\n",
      "step: 4514, loss: 0.430272072554\n",
      "step: 4519, loss: 0.525730609894\n",
      "Checkpoint is saved\n",
      "step: 4524, loss: 0.369942545891\n",
      "step: 4529, loss: 0.600350856781\n",
      "step: 4534, loss: 0.5822955966\n",
      "step: 4539, loss: 0.490937292576\n",
      "Checkpoint is saved\n",
      "step: 4544, loss: 0.572124123573\n",
      "step: 4549, loss: 0.646216154099\n",
      "step: 4554, loss: 0.570836305618\n",
      "step: 4559, loss: 0.601557612419\n",
      "Checkpoint is saved\n",
      "step: 4564, loss: 0.625394940376\n",
      "step: 4569, loss: 0.446454942226\n",
      "step: 4574, loss: 0.466261327267\n",
      "step: 4579, loss: 0.5973123312\n",
      "Checkpoint is saved\n",
      "step: 4584, loss: 0.602684319019\n",
      "step: 4589, loss: 0.525321364403\n",
      "step: 4594, loss: 0.515559494495\n",
      "step: 4599, loss: 0.550659000874\n",
      "Checkpoint is saved\n",
      "step: 4604, loss: 0.443928271532\n",
      "step: 4609, loss: 0.514492630959\n",
      "step: 4614, loss: 0.525259077549\n",
      "step: 4619, loss: 0.511233270168\n",
      "Checkpoint is saved\n",
      "step: 4624, loss: 0.599543809891\n",
      "step: 4629, loss: 0.549888968468\n",
      "step: 4634, loss: 0.70829230547\n",
      "step: 4639, loss: 0.624418854713\n",
      "Checkpoint is saved\n",
      "step: 4644, loss: 0.440140038729\n",
      "step: 4649, loss: 0.486459314823\n",
      "step: 4654, loss: 0.46454834938\n",
      "step: 4659, loss: 0.582554101944\n",
      "Checkpoint is saved\n",
      "step: 4664, loss: 0.722310841084\n",
      "step: 4669, loss: 0.475912004709\n",
      "step: 4674, loss: 0.488009572029\n",
      "step: 4679, loss: 0.558936953545\n",
      "Checkpoint is saved\n",
      "step: 4684, loss: 0.546986222267\n",
      "step: 4689, loss: 0.59456217289\n",
      "step: 4694, loss: 0.645295500755\n",
      "step: 4699, loss: 0.59011644125\n",
      "Checkpoint is saved\n",
      "step: 4704, loss: 0.480849683285\n",
      "step: 4709, loss: 0.672387719154\n",
      "step: 4714, loss: 0.608907222748\n",
      "step: 4719, loss: 0.583071827888\n",
      "Checkpoint is saved\n",
      "step: 4724, loss: 0.580894470215\n",
      "step: 4729, loss: 0.517759561539\n",
      "step: 4734, loss: 0.520378112793\n",
      "step: 4739, loss: 0.419684529305\n",
      "Checkpoint is saved\n",
      "step: 4744, loss: 0.599441111088\n",
      "step: 4749, loss: 0.391355633736\n",
      "step: 4754, loss: 0.601906299591\n",
      "step: 4759, loss: 0.553289532661\n",
      "Checkpoint is saved\n",
      "step: 4764, loss: 0.480416506529\n",
      "step: 4769, loss: 0.513134777546\n",
      "step: 4774, loss: 0.540254354477\n",
      "step: 4779, loss: 0.53109061718\n",
      "Checkpoint is saved\n",
      "step: 4784, loss: 0.542988657951\n",
      "step: 4789, loss: 0.515461802483\n",
      "step: 4794, loss: 0.578900754452\n",
      "step: 4799, loss: 0.601524472237\n",
      "Checkpoint is saved\n",
      "step: 4804, loss: 0.554110884666\n",
      "step: 4809, loss: 0.486616879702\n",
      "step: 4814, loss: 0.533841073513\n",
      "step: 4819, loss: 0.564463317394\n",
      "Checkpoint is saved\n",
      "step: 4824, loss: 0.567653119564\n",
      "step: 4829, loss: 0.564290821552\n",
      "step: 4834, loss: 0.603845477104\n",
      "step: 4839, loss: 0.462129592896\n",
      "Checkpoint is saved\n",
      "step: 4844, loss: 0.466958016157\n",
      "step: 4849, loss: 0.566272974014\n",
      "step: 4854, loss: 0.534782767296\n",
      "step: 4859, loss: 0.46233433485\n",
      "Checkpoint is saved\n",
      "step: 4864, loss: 0.519719362259\n",
      "step: 4869, loss: 0.496104776859\n",
      "step: 4874, loss: 0.675799608231\n",
      "step: 4879, loss: 0.627312898636\n",
      "Checkpoint is saved\n",
      "step: 4884, loss: 0.618943929672\n",
      "step: 4889, loss: 0.504649162292\n",
      "step: 4894, loss: 0.630071043968\n",
      "step: 4899, loss: 0.477316081524\n",
      "Checkpoint is saved\n",
      "step: 4904, loss: 0.625685572624\n",
      "step: 4909, loss: 0.533668160439\n",
      "step: 4914, loss: 0.506728768349\n",
      "step: 4919, loss: 0.558419406414\n",
      "Checkpoint is saved\n",
      "step: 4924, loss: 0.49344265461\n",
      "step: 4929, loss: 0.417737871408\n",
      "step: 4934, loss: 0.514594912529\n",
      "step: 4939, loss: 0.60197108984\n",
      "Checkpoint is saved\n",
      "step: 4944, loss: 0.529443264008\n",
      "step: 4949, loss: 0.480400085449\n",
      "step: 4954, loss: 0.481327533722\n",
      "step: 4959, loss: 0.447951495647\n",
      "Checkpoint is saved\n",
      "step: 4964, loss: 0.547610163689\n",
      "step: 4969, loss: 0.520230412483\n",
      "step: 4974, loss: 0.643736839294\n",
      "step: 4979, loss: 0.621367812157\n",
      "Checkpoint is saved\n",
      "step: 4984, loss: 0.525592684746\n",
      "step: 4989, loss: 0.426433384418\n",
      "step: 4994, loss: 0.543283522129\n",
      "step: 4999, loss: 0.571853339672\n",
      "Checkpoint is saved\n",
      "step: 5004, loss: 0.480245411396\n",
      "step: 5009, loss: 0.623048067093\n",
      "step: 5014, loss: 0.548197209835\n",
      "step: 5019, loss: 0.49756065011\n",
      "Checkpoint is saved\n",
      "step: 5024, loss: 0.553300976753\n",
      "step: 5029, loss: 0.558459043503\n",
      "step: 5034, loss: 0.555650949478\n",
      "step: 5039, loss: 0.541170477867\n",
      "Checkpoint is saved\n",
      "step: 5044, loss: 0.609102368355\n",
      "step: 5049, loss: 0.573649644852\n",
      "step: 5054, loss: 0.522476613522\n",
      "step: 5059, loss: 0.572638332844\n",
      "Checkpoint is saved\n",
      "step: 5064, loss: 0.468512654305\n",
      "step: 5069, loss: 0.467659950256\n",
      "step: 5074, loss: 0.549119949341\n",
      "step: 5079, loss: 0.620196580887\n",
      "Checkpoint is saved\n",
      "step: 5084, loss: 0.522349298\n",
      "step: 5089, loss: 0.543090105057\n",
      "step: 5094, loss: 0.527630090714\n",
      "step: 5099, loss: 0.652991294861\n",
      "Checkpoint is saved\n",
      "step: 5104, loss: 0.451719403267\n",
      "step: 5109, loss: 0.629413187504\n",
      "step: 5114, loss: 0.545761466026\n",
      "step: 5119, loss: 0.503542363644\n",
      "Checkpoint is saved\n",
      "step: 5124, loss: 0.494140416384\n",
      "step: 5129, loss: 0.530661821365\n",
      "step: 5134, loss: 0.503501653671\n",
      "step: 5139, loss: 0.617478311062\n",
      "Checkpoint is saved\n",
      "step: 5144, loss: 0.434054136276\n",
      "step: 5149, loss: 0.492934644222\n",
      "step: 5154, loss: 0.506772279739\n",
      "step: 5159, loss: 0.459380805492\n",
      "Checkpoint is saved\n",
      "step: 5164, loss: 0.478182941675\n",
      "step: 5169, loss: 0.433565080166\n",
      "step: 5174, loss: 0.417822510004\n",
      "step: 5179, loss: 0.423408269882\n",
      "Checkpoint is saved\n",
      "step: 5184, loss: 0.470476478338\n",
      "step: 5189, loss: 0.609840869904\n",
      "step: 5194, loss: 0.540602564812\n",
      "step: 5199, loss: 0.588041305542\n",
      "Checkpoint is saved\n",
      "step: 5204, loss: 0.612272024155\n",
      "step: 5209, loss: 0.48103043437\n",
      "step: 5214, loss: 0.430144518614\n",
      "step: 5219, loss: 0.510195732117\n",
      "Checkpoint is saved\n",
      "step: 5224, loss: 0.51921916008\n",
      "step: 5229, loss: 0.524357438087\n",
      "step: 5234, loss: 0.521381020546\n",
      "step: 5239, loss: 0.486744761467\n",
      "Checkpoint is saved\n",
      "step: 5244, loss: 0.605509638786\n",
      "step: 5249, loss: 0.517412722111\n",
      "step: 5254, loss: 0.502361953259\n",
      "step: 5259, loss: 0.563228309155\n",
      "Checkpoint is saved\n",
      "step: 5264, loss: 0.567993164062\n",
      "step: 5269, loss: 0.551429569721\n",
      "step: 5274, loss: 0.487638831139\n",
      "step: 5279, loss: 0.469872772694\n",
      "Checkpoint is saved\n",
      "step: 5284, loss: 0.461331546307\n",
      "step: 5289, loss: 0.615656554699\n",
      "step: 5294, loss: 0.50250852108\n",
      "step: 5299, loss: 0.546579241753\n",
      "Checkpoint is saved\n",
      "step: 5304, loss: 0.475533604622\n",
      "step: 5309, loss: 0.577359557152\n",
      "step: 5314, loss: 0.422443211079\n",
      "step: 5319, loss: 0.588972568512\n",
      "Checkpoint is saved\n",
      "step: 5324, loss: 0.480927914381\n",
      "step: 5329, loss: 0.505067229271\n",
      "step: 5334, loss: 0.521998286247\n",
      "step: 5339, loss: 0.560003340244\n",
      "Checkpoint is saved\n",
      "step: 5344, loss: 0.429090678692\n",
      "step: 5349, loss: 0.543114185333\n",
      "step: 5354, loss: 0.595765888691\n",
      "step: 5359, loss: 0.581803798676\n",
      "Checkpoint is saved\n",
      "step: 5364, loss: 0.591719150543\n",
      "step: 5369, loss: 0.495360434055\n",
      "step: 5374, loss: 0.549982309341\n",
      "step: 5379, loss: 0.522126019001\n",
      "Checkpoint is saved\n",
      "step: 5384, loss: 0.426940917969\n",
      "step: 5389, loss: 0.43852442503\n",
      "step: 5394, loss: 0.610851466656\n",
      "step: 5399, loss: 0.454984366894\n",
      "Checkpoint is saved\n",
      "step: 5404, loss: 0.546964645386\n",
      "step: 5409, loss: 0.495393693447\n",
      "step: 5414, loss: 0.52368247509\n",
      "step: 5419, loss: 0.417755097151\n",
      "Checkpoint is saved\n",
      "step: 5424, loss: 0.604795455933\n",
      "step: 5429, loss: 0.549077570438\n",
      "step: 5434, loss: 0.643530070782\n",
      "step: 5439, loss: 0.576270222664\n",
      "Checkpoint is saved\n",
      "step: 5444, loss: 0.531142115593\n",
      "step: 5449, loss: 0.385562419891\n",
      "step: 5454, loss: 0.55234670639\n",
      "step: 5459, loss: 0.531179726124\n",
      "Checkpoint is saved\n",
      "step: 5464, loss: 0.425799846649\n",
      "step: 5469, loss: 0.577093601227\n",
      "step: 5474, loss: 0.53621327877\n",
      "step: 5479, loss: 0.461513221264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint is saved\n",
      "step: 5484, loss: 0.606736898422\n",
      "step: 5489, loss: 0.524172067642\n",
      "step: 5494, loss: 0.505207419395\n",
      "step: 5499, loss: 0.535045742989\n",
      "Checkpoint is saved\n",
      "step: 5504, loss: 0.502351760864\n",
      "step: 5509, loss: 0.403441429138\n",
      "step: 5514, loss: 0.553533315659\n",
      "step: 5519, loss: 0.592334449291\n",
      "Checkpoint is saved\n",
      "step: 5524, loss: 0.569378256798\n",
      "step: 5529, loss: 0.605040132999\n",
      "step: 5534, loss: 0.538051843643\n",
      "step: 5539, loss: 0.432314276695\n",
      "Checkpoint is saved\n",
      "step: 5544, loss: 0.531026780605\n",
      "step: 5549, loss: 0.578982114792\n",
      "step: 5554, loss: 0.483246684074\n",
      "step: 5559, loss: 0.466720581055\n",
      "Checkpoint is saved\n",
      "step: 5564, loss: 0.403963565826\n",
      "step: 5569, loss: 0.517579376698\n",
      "step: 5574, loss: 0.509555101395\n",
      "step: 5579, loss: 0.498215079308\n",
      "Checkpoint is saved\n",
      "step: 5584, loss: 0.449390619993\n",
      "step: 5589, loss: 0.47673946619\n",
      "step: 5594, loss: 0.547104477882\n",
      "step: 5599, loss: 0.525807976723\n",
      "Checkpoint is saved\n",
      "step: 5604, loss: 0.575943052769\n",
      "step: 5609, loss: 0.498561024666\n",
      "step: 5614, loss: 0.511637210846\n",
      "step: 5619, loss: 0.487947434187\n",
      "Checkpoint is saved\n",
      "step: 5624, loss: 0.476384341717\n",
      "step: 5629, loss: 0.486060261726\n",
      "step: 5634, loss: 0.429480075836\n",
      "step: 5639, loss: 0.458510279655\n",
      "Checkpoint is saved\n",
      "step: 5644, loss: 0.493269771338\n",
      "step: 5649, loss: 0.523237705231\n",
      "step: 5654, loss: 0.528722286224\n",
      "step: 5659, loss: 0.502117395401\n",
      "Checkpoint is saved\n",
      "step: 5664, loss: 0.47628903389\n",
      "step: 5669, loss: 0.53276693821\n",
      "step: 5674, loss: 0.61084151268\n",
      "step: 5679, loss: 0.525613546371\n",
      "Checkpoint is saved\n",
      "step: 5684, loss: 0.693775773048\n",
      "step: 5689, loss: 0.613416433334\n",
      "step: 5694, loss: 0.605672478676\n",
      "step: 5699, loss: 0.583618819714\n",
      "Checkpoint is saved\n",
      "step: 5704, loss: 0.578805565834\n",
      "step: 5709, loss: 0.510811269283\n",
      "step: 5714, loss: 0.551722407341\n",
      "step: 5719, loss: 0.450335323811\n",
      "Checkpoint is saved\n",
      "step: 5724, loss: 0.46329587698\n",
      "step: 5729, loss: 0.445576369762\n",
      "step: 5734, loss: 0.474490821362\n",
      "step: 5739, loss: 0.519320011139\n",
      "Checkpoint is saved\n",
      "step: 5744, loss: 0.431946694851\n",
      "step: 5749, loss: 0.605642557144\n",
      "step: 5754, loss: 0.455569088459\n",
      "step: 5759, loss: 0.622497618198\n",
      "Checkpoint is saved\n",
      "step: 5764, loss: 0.597695410252\n",
      "step: 5769, loss: 0.5137809515\n",
      "step: 5774, loss: 0.528651714325\n",
      "step: 5779, loss: 0.477389156818\n",
      "Checkpoint is saved\n",
      "step: 5784, loss: 0.474658668041\n",
      "step: 5789, loss: 0.396227180958\n",
      "step: 5794, loss: 0.478863686323\n",
      "step: 5799, loss: 0.427570641041\n",
      "Checkpoint is saved\n",
      "step: 5804, loss: 0.58266299963\n",
      "step: 5809, loss: 0.383081614971\n",
      "step: 5814, loss: 0.507542550564\n",
      "step: 5819, loss: 0.491318315268\n",
      "Checkpoint is saved\n",
      "step: 5824, loss: 0.563517570496\n",
      "step: 5829, loss: 0.480914741755\n",
      "step: 5834, loss: 0.533196687698\n",
      "step: 5839, loss: 0.435263156891\n",
      "Checkpoint is saved\n",
      "step: 5844, loss: 0.4404733181\n",
      "step: 5849, loss: 0.468097686768\n",
      "step: 5854, loss: 0.485117673874\n",
      "step: 5859, loss: 0.473645627499\n",
      "Checkpoint is saved\n",
      "step: 5864, loss: 0.525437831879\n",
      "step: 5869, loss: 0.44687473774\n",
      "step: 5874, loss: 0.505598783493\n",
      "step: 5879, loss: 0.539963781834\n",
      "Checkpoint is saved\n",
      "step: 5884, loss: 0.553670167923\n",
      "step: 5889, loss: 0.528306663036\n",
      "step: 5894, loss: 0.562050402164\n",
      "step: 5899, loss: 0.579336762428\n",
      "Checkpoint is saved\n",
      "step: 5904, loss: 0.508317887783\n",
      "step: 5909, loss: 0.594816327095\n",
      "step: 5914, loss: 0.540733873844\n",
      "step: 5919, loss: 0.491589128971\n",
      "Checkpoint is saved\n",
      "step: 5924, loss: 0.473780184984\n",
      "step: 5929, loss: 0.474264532328\n",
      "step: 5934, loss: 0.550129711628\n",
      "step: 5939, loss: 0.676497459412\n",
      "Checkpoint is saved\n",
      "step: 5944, loss: 0.451663136482\n",
      "step: 5949, loss: 0.488996386528\n",
      "step: 5954, loss: 0.504853963852\n",
      "step: 5959, loss: 0.495805412531\n",
      "Checkpoint is saved\n",
      "step: 5964, loss: 0.555578052998\n",
      "step: 5969, loss: 0.496688067913\n",
      "step: 5974, loss: 0.538054347038\n",
      "step: 5979, loss: 0.503777623177\n",
      "Checkpoint is saved\n",
      "step: 5984, loss: 0.559738397598\n",
      "step: 5989, loss: 0.496737480164\n",
      "step: 5994, loss: 0.504746079445\n",
      "step: 5999, loss: 0.46298366785\n",
      "Checkpoint is saved\n",
      "step: 6004, loss: 0.567895770073\n",
      "step: 6009, loss: 0.536589860916\n",
      "step: 6014, loss: 0.45838701725\n",
      "step: 6019, loss: 0.489275515079\n",
      "Checkpoint is saved\n",
      "step: 6024, loss: 0.482431650162\n",
      "step: 6029, loss: 0.580243945122\n",
      "step: 6034, loss: 0.506700158119\n",
      "step: 6039, loss: 0.484999120235\n",
      "Checkpoint is saved\n",
      "step: 6044, loss: 0.576445996761\n",
      "step: 6049, loss: 0.541730880737\n",
      "step: 6054, loss: 0.431778877974\n",
      "step: 6059, loss: 0.483974426985\n",
      "Checkpoint is saved\n",
      "step: 6064, loss: 0.430714547634\n",
      "step: 6069, loss: 0.572087407112\n",
      "step: 6074, loss: 0.425253421068\n",
      "step: 6079, loss: 0.520944058895\n",
      "Checkpoint is saved\n",
      "step: 6084, loss: 0.49803301692\n",
      "step: 6089, loss: 0.599820077419\n",
      "step: 6094, loss: 0.543879210949\n",
      "step: 6099, loss: 0.557808995247\n",
      "Checkpoint is saved\n",
      "step: 6104, loss: 0.464455127716\n",
      "step: 6109, loss: 0.493162155151\n",
      "step: 6114, loss: 0.523814976215\n",
      "step: 6119, loss: 0.598502278328\n",
      "Checkpoint is saved\n",
      "step: 6124, loss: 0.43595457077\n",
      "step: 6129, loss: 0.400334715843\n",
      "step: 6134, loss: 0.553869724274\n",
      "step: 6139, loss: 0.564885377884\n",
      "Checkpoint is saved\n",
      "step: 6144, loss: 0.444670855999\n",
      "step: 6149, loss: 0.608292639256\n",
      "step: 6154, loss: 0.464535117149\n",
      "step: 6159, loss: 0.466514229774\n",
      "Checkpoint is saved\n",
      "step: 6164, loss: 0.431120872498\n",
      "step: 6169, loss: 0.481519728899\n",
      "step: 6174, loss: 0.674628257751\n",
      "step: 6179, loss: 0.533518195152\n",
      "Checkpoint is saved\n",
      "step: 6184, loss: 0.608654022217\n",
      "step: 6189, loss: 0.402123957872\n",
      "step: 6194, loss: 0.484692871571\n",
      "step: 6199, loss: 0.553659260273\n",
      "Checkpoint is saved\n",
      "step: 6204, loss: 0.410418987274\n",
      "step: 6209, loss: 0.41375887394\n",
      "step: 6214, loss: 0.566636919975\n",
      "step: 6219, loss: 0.515405535698\n",
      "Checkpoint is saved\n",
      "step: 6224, loss: 0.539449810982\n",
      "step: 6229, loss: 0.496412426233\n",
      "step: 6234, loss: 0.43319195509\n",
      "step: 6239, loss: 0.529308736324\n",
      "Checkpoint is saved\n",
      "step: 6244, loss: 0.594158530235\n",
      "step: 6249, loss: 0.539680421352\n",
      "step: 6254, loss: 0.493512690067\n",
      "step: 6259, loss: 0.526837348938\n",
      "Checkpoint is saved\n",
      "step: 6264, loss: 0.372848540545\n",
      "step: 6269, loss: 0.601056277752\n",
      "step: 6274, loss: 0.365337818861\n",
      "step: 6279, loss: 0.614358901978\n",
      "Checkpoint is saved\n",
      "step: 6284, loss: 0.551099777222\n",
      "step: 6289, loss: 0.406735271215\n",
      "step: 6294, loss: 0.540754556656\n",
      "step: 6299, loss: 0.511974334717\n",
      "Checkpoint is saved\n",
      "step: 6304, loss: 0.500723361969\n",
      "step: 6309, loss: 0.585017383099\n",
      "step: 6314, loss: 0.470196485519\n",
      "step: 6319, loss: 0.550301074982\n",
      "Checkpoint is saved\n",
      "step: 6324, loss: 0.523223280907\n",
      "step: 6329, loss: 0.488667875528\n",
      "step: 6334, loss: 0.519587159157\n",
      "step: 6339, loss: 0.433963507414\n",
      "Checkpoint is saved\n",
      "step: 6344, loss: 0.584051370621\n",
      "step: 6349, loss: 0.479408502579\n",
      "step: 6354, loss: 0.463328301907\n",
      "step: 6359, loss: 0.539045572281\n",
      "Checkpoint is saved\n",
      "step: 6364, loss: 0.43612614274\n",
      "step: 6369, loss: 0.54420876503\n",
      "step: 6374, loss: 0.452529460192\n",
      "step: 6379, loss: 0.482074409723\n",
      "Checkpoint is saved\n",
      "step: 6384, loss: 0.471381962299\n",
      "step: 6389, loss: 0.511182069778\n",
      "step: 6394, loss: 0.603381693363\n",
      "step: 6399, loss: 0.538034498692\n",
      "Checkpoint is saved\n",
      "step: 6404, loss: 0.617118835449\n",
      "step: 6409, loss: 0.550490260124\n",
      "step: 6414, loss: 0.620159626007\n",
      "step: 6419, loss: 0.436324775219\n",
      "Checkpoint is saved\n",
      "step: 6424, loss: 0.455384165049\n",
      "step: 6429, loss: 0.519502580166\n",
      "step: 6434, loss: 0.61743080616\n",
      "step: 6439, loss: 0.423923581839\n",
      "Checkpoint is saved\n",
      "step: 6444, loss: 0.371341049671\n",
      "step: 6449, loss: 0.54106092453\n",
      "step: 6454, loss: 0.506789028645\n",
      "step: 6459, loss: 0.5569652915\n",
      "Checkpoint is saved\n",
      "step: 6464, loss: 0.455212444067\n",
      "step: 6469, loss: 0.427466362715\n",
      "step: 6474, loss: 0.517297267914\n",
      "step: 6479, loss: 0.428437530994\n",
      "Checkpoint is saved\n",
      "step: 6484, loss: 0.507375240326\n",
      "step: 6489, loss: 0.530183970928\n",
      "step: 6494, loss: 0.54783475399\n",
      "step: 6499, loss: 0.532555222511\n",
      "Checkpoint is saved\n",
      "step: 6504, loss: 0.3950086236\n",
      "step: 6509, loss: 0.490439355373\n",
      "step: 6514, loss: 0.545914649963\n",
      "step: 6519, loss: 0.465932190418\n",
      "Checkpoint is saved\n",
      "step: 6524, loss: 0.514389514923\n",
      "step: 6529, loss: 0.519658863544\n",
      "step: 6534, loss: 0.363563656807\n",
      "step: 6539, loss: 0.450013637543\n",
      "Checkpoint is saved\n",
      "step: 6544, loss: 0.478589087725\n",
      "step: 6549, loss: 0.500478386879\n",
      "step: 6554, loss: 0.482023954391\n",
      "step: 6559, loss: 0.588703334332\n",
      "Checkpoint is saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6564, loss: 0.50758087635\n",
      "step: 6569, loss: 0.474574834108\n",
      "step: 6574, loss: 0.584380149841\n",
      "step: 6579, loss: 0.57367181778\n",
      "Checkpoint is saved\n",
      "step: 6584, loss: 0.467821657658\n",
      "step: 6589, loss: 0.418170899153\n",
      "step: 6594, loss: 0.489863157272\n",
      "step: 6599, loss: 0.468178510666\n",
      "Checkpoint is saved\n",
      "step: 6604, loss: 0.565271496773\n",
      "step: 6609, loss: 0.51308298111\n",
      "step: 6614, loss: 0.591257452965\n",
      "step: 6619, loss: 0.611610770226\n",
      "Checkpoint is saved\n",
      "step: 6624, loss: 0.478917509317\n",
      "step: 6629, loss: 0.544191718102\n",
      "step: 6634, loss: 0.460880637169\n",
      "step: 6639, loss: 0.547163426876\n",
      "Checkpoint is saved\n",
      "step: 6644, loss: 0.469384670258\n",
      "step: 6649, loss: 0.517069458961\n",
      "step: 6654, loss: 0.449423015118\n",
      "step: 6659, loss: 0.542931735516\n",
      "Checkpoint is saved\n",
      "step: 6664, loss: 0.454904973507\n",
      "step: 6669, loss: 0.539228320122\n",
      "step: 6674, loss: 0.456863850355\n",
      "step: 6679, loss: 0.550505757332\n",
      "Checkpoint is saved\n",
      "step: 6684, loss: 0.446848541498\n",
      "step: 6689, loss: 0.515272498131\n",
      "step: 6694, loss: 0.459951967001\n",
      "step: 6699, loss: 0.437963843346\n",
      "Checkpoint is saved\n",
      "step: 6704, loss: 0.616206288338\n",
      "step: 6709, loss: 0.514317333698\n",
      "step: 6714, loss: 0.504496216774\n",
      "step: 6719, loss: 0.616030395031\n",
      "Checkpoint is saved\n",
      "step: 6724, loss: 0.583642601967\n",
      "step: 6729, loss: 0.554093241692\n",
      "step: 6734, loss: 0.46821859479\n",
      "step: 6739, loss: 0.455400615931\n",
      "Checkpoint is saved\n",
      "step: 6744, loss: 0.473474919796\n",
      "step: 6749, loss: 0.54778277874\n",
      "step: 6754, loss: 0.515097379684\n",
      "step: 6759, loss: 0.533564627171\n",
      "Checkpoint is saved\n",
      "step: 6764, loss: 0.421252906322\n",
      "step: 6769, loss: 0.494200259447\n",
      "step: 6774, loss: 0.444542467594\n",
      "step: 6779, loss: 0.539199590683\n",
      "Checkpoint is saved\n",
      "step: 6784, loss: 0.553490400314\n",
      "step: 6789, loss: 0.474414378405\n",
      "step: 6794, loss: 0.395449757576\n",
      "step: 6799, loss: 0.418040812016\n",
      "Checkpoint is saved\n",
      "step: 6804, loss: 0.552612066269\n",
      "step: 6809, loss: 0.419242680073\n",
      "step: 6814, loss: 0.476333796978\n",
      "step: 6819, loss: 0.492188721895\n",
      "Checkpoint is saved\n",
      "step: 6824, loss: 0.442856580019\n",
      "step: 6829, loss: 0.537849903107\n",
      "step: 6834, loss: 0.563351988792\n",
      "step: 6839, loss: 0.507338285446\n",
      "Checkpoint is saved\n",
      "step: 6844, loss: 0.426293045282\n",
      "step: 6849, loss: 0.432995826006\n",
      "step: 6854, loss: 0.465182870626\n",
      "step: 6859, loss: 0.462600320578\n",
      "Checkpoint is saved\n",
      "step: 6864, loss: 0.531180739403\n",
      "step: 6869, loss: 0.483911037445\n",
      "step: 6874, loss: 0.464029222727\n",
      "step: 6879, loss: 0.451349616051\n",
      "Checkpoint is saved\n",
      "step: 6884, loss: 0.453792750835\n",
      "step: 6889, loss: 0.441157966852\n",
      "step: 6894, loss: 0.514919638634\n",
      "step: 6899, loss: 0.632190108299\n",
      "Checkpoint is saved\n",
      "step: 6904, loss: 0.589113950729\n",
      "step: 6909, loss: 0.446854054928\n",
      "step: 6914, loss: 0.422032058239\n",
      "step: 6919, loss: 0.525805711746\n",
      "Checkpoint is saved\n",
      "step: 6924, loss: 0.470648467541\n",
      "step: 6929, loss: 0.611744523048\n",
      "step: 6934, loss: 0.501583278179\n",
      "step: 6939, loss: 0.446559429169\n",
      "Checkpoint is saved\n",
      "step: 6944, loss: 0.561078310013\n",
      "step: 6949, loss: 0.526052176952\n",
      "step: 6954, loss: 0.505224347115\n",
      "step: 6959, loss: 0.447987735271\n",
      "Checkpoint is saved\n",
      "step: 6964, loss: 0.422494590282\n",
      "step: 6969, loss: 0.533008038998\n",
      "step: 6974, loss: 0.541053771973\n",
      "step: 6979, loss: 0.457511663437\n",
      "Checkpoint is saved\n",
      "step: 6984, loss: 0.550411224365\n",
      "step: 6989, loss: 0.386313021183\n",
      "step: 6994, loss: 0.574573516846\n",
      "step: 6999, loss: 0.46856290102\n",
      "Checkpoint is saved\n",
      "step: 7004, loss: 0.476534843445\n",
      "step: 7009, loss: 0.551117658615\n",
      "step: 7014, loss: 0.519747257233\n",
      "step: 7019, loss: 0.537790477276\n",
      "Checkpoint is saved\n",
      "step: 7024, loss: 0.503479957581\n",
      "step: 7029, loss: 0.532718718052\n",
      "step: 7034, loss: 0.491104424\n",
      "step: 7039, loss: 0.477290272713\n",
      "Checkpoint is saved\n",
      "step: 7044, loss: 0.413988441229\n",
      "step: 7049, loss: 0.363416969776\n",
      "step: 7054, loss: 0.452100276947\n",
      "step: 7059, loss: 0.420931756496\n",
      "Checkpoint is saved\n",
      "step: 7064, loss: 0.436131954193\n",
      "step: 7069, loss: 0.48777616024\n",
      "step: 7074, loss: 0.65979886055\n",
      "step: 7079, loss: 0.438178658485\n",
      "Checkpoint is saved\n",
      "step: 7084, loss: 0.560889661312\n",
      "step: 7089, loss: 0.491403579712\n",
      "step: 7094, loss: 0.459537982941\n",
      "step: 7099, loss: 0.455178141594\n",
      "Checkpoint is saved\n",
      "step: 7104, loss: 0.64617228508\n",
      "step: 7109, loss: 0.539641141891\n",
      "step: 7114, loss: 0.547954618931\n",
      "step: 7119, loss: 0.460913062096\n",
      "Checkpoint is saved\n",
      "step: 7124, loss: 0.501139461994\n",
      "step: 7129, loss: 0.540159642696\n",
      "step: 7134, loss: 0.477385640144\n",
      "step: 7139, loss: 0.527973353863\n",
      "Checkpoint is saved\n",
      "step: 7144, loss: 0.488325327635\n",
      "step: 7149, loss: 0.449143946171\n",
      "step: 7154, loss: 0.539130568504\n",
      "step: 7159, loss: 0.42636603117\n",
      "Checkpoint is saved\n",
      "step: 7164, loss: 0.42080527544\n",
      "step: 7169, loss: 0.568787574768\n",
      "step: 7174, loss: 0.546348571777\n",
      "step: 7179, loss: 0.538581311703\n",
      "Checkpoint is saved\n",
      "step: 7184, loss: 0.435269922018\n",
      "step: 7189, loss: 0.490936040878\n",
      "step: 7194, loss: 0.364498764277\n",
      "step: 7199, loss: 0.464602023363\n",
      "Checkpoint is saved\n",
      "step: 7204, loss: 0.470370441675\n",
      "step: 7209, loss: 0.514469504356\n",
      "step: 7214, loss: 0.484772503376\n",
      "step: 7219, loss: 0.545719087124\n",
      "Checkpoint is saved\n",
      "step: 7224, loss: 0.544901192188\n",
      "step: 7229, loss: 0.420805126429\n",
      "step: 7234, loss: 0.515039205551\n",
      "step: 7239, loss: 0.477171599865\n",
      "Checkpoint is saved\n",
      "step: 7244, loss: 0.545707583427\n",
      "step: 7249, loss: 0.565402805805\n",
      "step: 7254, loss: 0.463809430599\n",
      "step: 7259, loss: 0.558394670486\n",
      "Checkpoint is saved\n",
      "step: 7264, loss: 0.545160353184\n",
      "step: 7269, loss: 0.49820381403\n",
      "step: 7274, loss: 0.613996684551\n",
      "step: 7279, loss: 0.350035011768\n",
      "Checkpoint is saved\n",
      "step: 7284, loss: 0.636835277081\n",
      "step: 7289, loss: 0.615526258945\n",
      "step: 7294, loss: 0.536992788315\n",
      "step: 7299, loss: 0.519952297211\n",
      "Checkpoint is saved\n",
      "step: 7304, loss: 0.432782560587\n",
      "step: 7309, loss: 0.480493634939\n",
      "step: 7314, loss: 0.466967463493\n",
      "step: 7319, loss: 0.449991077185\n",
      "Checkpoint is saved\n",
      "step: 7324, loss: 0.48089915514\n",
      "step: 7329, loss: 0.487954765558\n",
      "step: 7334, loss: 0.528261899948\n",
      "step: 7339, loss: 0.484518349171\n",
      "Checkpoint is saved\n",
      "step: 7344, loss: 0.447345227003\n",
      "step: 7349, loss: 0.586719989777\n",
      "step: 7354, loss: 0.45522364974\n",
      "step: 7359, loss: 0.469438076019\n",
      "Checkpoint is saved\n",
      "step: 7364, loss: 0.426748037338\n",
      "step: 7369, loss: 0.411019325256\n",
      "step: 7374, loss: 0.530804634094\n",
      "step: 7379, loss: 0.429647952318\n",
      "Checkpoint is saved\n",
      "step: 7384, loss: 0.46492856741\n",
      "step: 7389, loss: 0.493766844273\n",
      "step: 7394, loss: 0.501520514488\n",
      "step: 7399, loss: 0.544740080833\n",
      "Checkpoint is saved\n",
      "step: 7404, loss: 0.420752078295\n",
      "step: 7409, loss: 0.49090462923\n",
      "step: 7414, loss: 0.421788811684\n",
      "step: 7419, loss: 0.390810251236\n",
      "Checkpoint is saved\n",
      "step: 7424, loss: 0.45604789257\n",
      "step: 7429, loss: 0.371786504984\n",
      "step: 7434, loss: 0.587338209152\n",
      "step: 7439, loss: 0.573698163033\n",
      "Checkpoint is saved\n",
      "step: 7444, loss: 0.533529043198\n",
      "step: 7449, loss: 0.388934850693\n",
      "step: 7454, loss: 0.405758589506\n",
      "step: 7459, loss: 0.47779789567\n",
      "Checkpoint is saved\n",
      "step: 7464, loss: 0.450519472361\n",
      "step: 7469, loss: 0.424118489027\n",
      "step: 7474, loss: 0.507503449917\n",
      "step: 7479, loss: 0.424289047718\n",
      "Checkpoint is saved\n",
      "step: 7484, loss: 0.591235816479\n",
      "step: 7489, loss: 0.496060460806\n",
      "step: 7494, loss: 0.493112415075\n",
      "step: 7499, loss: 0.48379650712\n",
      "Checkpoint is saved\n",
      "step: 7504, loss: 0.419247746468\n",
      "step: 7509, loss: 0.459958910942\n",
      "step: 7514, loss: 0.41761469841\n",
      "step: 7519, loss: 0.525792479515\n",
      "Checkpoint is saved\n",
      "step: 7524, loss: 0.547241449356\n",
      "step: 7529, loss: 0.419666647911\n",
      "step: 7534, loss: 0.460519403219\n",
      "step: 7539, loss: 0.424533128738\n",
      "Checkpoint is saved\n",
      "step: 7544, loss: 0.497943580151\n",
      "step: 7549, loss: 0.423794686794\n",
      "step: 7554, loss: 0.464564323425\n",
      "step: 7559, loss: 0.474669456482\n",
      "Checkpoint is saved\n",
      "step: 7564, loss: 0.425518482924\n",
      "step: 7569, loss: 0.551178991795\n",
      "step: 7574, loss: 0.426265239716\n",
      "step: 7579, loss: 0.57559722662\n",
      "Checkpoint is saved\n",
      "step: 7584, loss: 0.531889915466\n",
      "step: 7589, loss: 0.490336567163\n",
      "step: 7594, loss: 0.530435442924\n",
      "step: 7599, loss: 0.490347921848\n",
      "Checkpoint is saved\n",
      "step: 7604, loss: 0.524567604065\n",
      "step: 7609, loss: 0.49772632122\n",
      "step: 7614, loss: 0.478638708591\n",
      "step: 7619, loss: 0.556045114994\n",
      "Checkpoint is saved\n",
      "step: 7624, loss: 0.395825743675\n",
      "step: 7629, loss: 0.461452156305\n",
      "step: 7634, loss: 0.434245198965\n",
      "step: 7639, loss: 0.562734603882\n",
      "Checkpoint is saved\n",
      "step: 7644, loss: 0.462426185608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7649, loss: 0.415870636702\n",
      "step: 7654, loss: 0.463086366653\n",
      "step: 7659, loss: 0.446862488985\n",
      "Checkpoint is saved\n",
      "step: 7664, loss: 0.47581627965\n",
      "step: 7669, loss: 0.423181831837\n",
      "step: 7674, loss: 0.429022669792\n",
      "step: 7679, loss: 0.429570317268\n",
      "Checkpoint is saved\n",
      "step: 7684, loss: 0.433787405491\n",
      "step: 7689, loss: 0.568915963173\n",
      "step: 7694, loss: 0.495822250843\n",
      "step: 7699, loss: 0.425463974476\n",
      "Checkpoint is saved\n",
      "step: 7704, loss: 0.467108368874\n",
      "step: 7709, loss: 0.512805342674\n",
      "step: 7714, loss: 0.504407346249\n",
      "step: 7719, loss: 0.459614634514\n",
      "Checkpoint is saved\n",
      "step: 7724, loss: 0.535427808762\n",
      "step: 7729, loss: 0.485875189304\n",
      "step: 7734, loss: 0.47445705533\n",
      "step: 7739, loss: 0.546576023102\n",
      "Checkpoint is saved\n",
      "step: 7744, loss: 0.431714653969\n",
      "step: 7749, loss: 0.4830108881\n",
      "step: 7754, loss: 0.404764860868\n",
      "step: 7759, loss: 0.496067404747\n",
      "Checkpoint is saved\n",
      "step: 7764, loss: 0.436010599136\n",
      "step: 7769, loss: 0.444344103336\n",
      "step: 7774, loss: 0.617556452751\n",
      "step: 7779, loss: 0.421294122934\n",
      "Checkpoint is saved\n",
      "step: 7784, loss: 0.430987417698\n",
      "step: 7789, loss: 0.437936276197\n",
      "step: 7794, loss: 0.455837547779\n",
      "step: 7799, loss: 0.450924754143\n",
      "Checkpoint is saved\n",
      "step: 7804, loss: 0.536692857742\n",
      "step: 7809, loss: 0.422676056623\n",
      "step: 7814, loss: 0.508409559727\n",
      "step: 7819, loss: 0.349505990744\n",
      "Checkpoint is saved\n",
      "step: 7824, loss: 0.357896357775\n",
      "step: 7829, loss: 0.438301563263\n",
      "step: 7834, loss: 0.527006328106\n",
      "step: 7839, loss: 0.442960560322\n",
      "Checkpoint is saved\n",
      "step: 7844, loss: 0.468009620905\n",
      "step: 7849, loss: 0.57311630249\n",
      "step: 7854, loss: 0.448238730431\n",
      "step: 7859, loss: 0.440058082342\n",
      "Checkpoint is saved\n",
      "step: 7864, loss: 0.479491263628\n",
      "step: 7869, loss: 0.419264942408\n",
      "step: 7874, loss: 0.433881163597\n",
      "step: 7879, loss: 0.532667517662\n",
      "Checkpoint is saved\n",
      "step: 7884, loss: 0.477953135967\n",
      "step: 7889, loss: 0.438103616238\n",
      "step: 7894, loss: 0.491121590137\n",
      "step: 7899, loss: 0.423320233822\n",
      "Checkpoint is saved\n",
      "step: 7904, loss: 0.453824400902\n",
      "step: 7909, loss: 0.445139110088\n",
      "step: 7914, loss: 0.459111154079\n",
      "step: 7919, loss: 0.480852752924\n",
      "Checkpoint is saved\n",
      "step: 7924, loss: 0.444175541401\n",
      "step: 7929, loss: 0.391136944294\n",
      "step: 7934, loss: 0.417146712542\n",
      "step: 7939, loss: 0.471943229437\n",
      "Checkpoint is saved\n",
      "step: 7944, loss: 0.477418392897\n",
      "step: 7949, loss: 0.436943948269\n",
      "step: 7954, loss: 0.509564220905\n",
      "step: 7959, loss: 0.512905359268\n",
      "Checkpoint is saved\n",
      "step: 7964, loss: 0.43703481555\n",
      "step: 7969, loss: 0.434149473906\n",
      "step: 7974, loss: 0.523231744766\n",
      "step: 7979, loss: 0.432281821966\n",
      "Checkpoint is saved\n",
      "step: 7984, loss: 0.459169477224\n",
      "step: 7989, loss: 0.453066408634\n",
      "step: 7994, loss: 0.475265860558\n",
      "step: 7999, loss: 0.480415582657\n",
      "Checkpoint is saved\n",
      "step: 8004, loss: 0.557264208794\n",
      "step: 8009, loss: 0.452951133251\n",
      "step: 8014, loss: 0.524301826954\n",
      "step: 8019, loss: 0.532096028328\n",
      "Checkpoint is saved\n",
      "step: 8024, loss: 0.475887417793\n",
      "step: 8029, loss: 0.481645226479\n",
      "step: 8034, loss: 0.398072451353\n",
      "step: 8039, loss: 0.445657551289\n",
      "Checkpoint is saved\n",
      "step: 8044, loss: 0.44095787406\n",
      "step: 8049, loss: 0.512115478516\n",
      "step: 8054, loss: 0.479922413826\n",
      "step: 8059, loss: 0.429209172726\n",
      "Checkpoint is saved\n",
      "step: 8064, loss: 0.436084479094\n",
      "step: 8069, loss: 0.460954338312\n",
      "step: 8074, loss: 0.527952492237\n",
      "step: 8079, loss: 0.448162019253\n",
      "Checkpoint is saved\n",
      "step: 8084, loss: 0.562144577503\n",
      "step: 8089, loss: 0.453968971968\n",
      "step: 8094, loss: 0.505111813545\n",
      "step: 8099, loss: 0.406917095184\n",
      "Checkpoint is saved\n",
      "step: 8104, loss: 0.604695558548\n",
      "step: 8109, loss: 0.596114337444\n",
      "step: 8114, loss: 0.416522204876\n",
      "step: 8119, loss: 0.447984069586\n",
      "Checkpoint is saved\n",
      "step: 8124, loss: 0.520007252693\n",
      "step: 8129, loss: 0.529437303543\n",
      "step: 8134, loss: 0.404223173857\n",
      "step: 8139, loss: 0.487279266119\n",
      "Checkpoint is saved\n",
      "step: 8144, loss: 0.463375210762\n",
      "step: 8149, loss: 0.533978760242\n",
      "step: 8154, loss: 0.399045109749\n",
      "step: 8159, loss: 0.447741627693\n",
      "Checkpoint is saved\n",
      "step: 8164, loss: 0.533360719681\n",
      "step: 8169, loss: 0.435149848461\n",
      "step: 8174, loss: 0.356730639935\n",
      "step: 8179, loss: 0.382135272026\n",
      "Checkpoint is saved\n",
      "step: 8184, loss: 0.531155705452\n",
      "step: 8189, loss: 0.377155691385\n",
      "step: 8194, loss: 0.462914675474\n",
      "step: 8199, loss: 0.502202272415\n",
      "Checkpoint is saved\n",
      "step: 8204, loss: 0.419880390167\n",
      "step: 8209, loss: 0.47425276041\n",
      "step: 8214, loss: 0.424856066704\n",
      "step: 8219, loss: 0.477532088757\n",
      "Checkpoint is saved\n",
      "step: 8224, loss: 0.504276812077\n",
      "step: 8229, loss: 0.490900397301\n",
      "step: 8234, loss: 0.400682151318\n",
      "step: 8239, loss: 0.41215249896\n",
      "Checkpoint is saved\n",
      "step: 8244, loss: 0.438477575779\n",
      "step: 8249, loss: 0.408029407263\n",
      "step: 8254, loss: 0.545310974121\n",
      "step: 8259, loss: 0.334075450897\n",
      "Checkpoint is saved\n",
      "step: 8264, loss: 0.429703056812\n",
      "step: 8269, loss: 0.402345597744\n",
      "step: 8274, loss: 0.41160389781\n",
      "step: 8279, loss: 0.538702428341\n",
      "Checkpoint is saved\n",
      "step: 8284, loss: 0.443389803171\n",
      "step: 8289, loss: 0.391404747963\n",
      "step: 8294, loss: 0.497376859188\n",
      "step: 8299, loss: 0.561397433281\n",
      "Checkpoint is saved\n",
      "step: 8304, loss: 0.509151220322\n",
      "step: 8309, loss: 0.463665187359\n",
      "step: 8314, loss: 0.400532484055\n",
      "step: 8319, loss: 0.460927665234\n",
      "Checkpoint is saved\n",
      "step: 8324, loss: 0.384436845779\n",
      "step: 8329, loss: 0.48649430275\n",
      "step: 8334, loss: 0.48042973876\n",
      "step: 8339, loss: 0.479877352715\n",
      "Checkpoint is saved\n",
      "step: 8344, loss: 0.37521314621\n",
      "step: 8349, loss: 0.516080975533\n",
      "step: 8354, loss: 0.447452902794\n",
      "step: 8359, loss: 0.541324019432\n",
      "Checkpoint is saved\n",
      "step: 8364, loss: 0.54378259182\n",
      "step: 8369, loss: 0.394187241793\n",
      "step: 8374, loss: 0.477717727423\n",
      "step: 8379, loss: 0.54231095314\n",
      "Checkpoint is saved\n",
      "step: 8384, loss: 0.477917671204\n",
      "step: 8389, loss: 0.3736551404\n",
      "step: 8394, loss: 0.46632361412\n",
      "step: 8399, loss: 0.402014911175\n",
      "Checkpoint is saved\n",
      "step: 8404, loss: 0.380834490061\n",
      "step: 8409, loss: 0.448162257671\n",
      "step: 8414, loss: 0.393214285374\n",
      "step: 8419, loss: 0.530436754227\n",
      "Checkpoint is saved\n",
      "step: 8424, loss: 0.518546462059\n",
      "step: 8429, loss: 0.505034863949\n",
      "step: 8434, loss: 0.483067512512\n",
      "step: 8439, loss: 0.486155897379\n",
      "Checkpoint is saved\n",
      "step: 8444, loss: 0.52212536335\n",
      "step: 8449, loss: 0.472674369812\n",
      "step: 8454, loss: 0.477429032326\n",
      "step: 8459, loss: 0.397852063179\n",
      "Checkpoint is saved\n",
      "step: 8464, loss: 0.374814152718\n",
      "step: 8469, loss: 0.325100839138\n",
      "step: 8474, loss: 0.43349134922\n",
      "step: 8479, loss: 0.510701358318\n",
      "Checkpoint is saved\n",
      "step: 8484, loss: 0.497127354145\n",
      "step: 8489, loss: 0.443474888802\n",
      "step: 8494, loss: 0.3904055655\n",
      "step: 8499, loss: 0.557845473289\n",
      "Checkpoint is saved\n",
      "step: 8504, loss: 0.433333456516\n",
      "step: 8509, loss: 0.467702031136\n",
      "step: 8514, loss: 0.467126637697\n",
      "step: 8519, loss: 0.438503205776\n",
      "Checkpoint is saved\n",
      "step: 8524, loss: 0.514717340469\n",
      "step: 8529, loss: 0.540563285351\n",
      "step: 8534, loss: 0.490810990334\n",
      "step: 8539, loss: 0.454310119152\n",
      "Checkpoint is saved\n",
      "step: 8544, loss: 0.704691648483\n",
      "step: 8549, loss: 0.409301757812\n",
      "step: 8554, loss: 0.449448883533\n",
      "step: 8559, loss: 0.286196798086\n",
      "Checkpoint is saved\n",
      "step: 8564, loss: 0.433686465025\n",
      "step: 8569, loss: 0.523856639862\n",
      "step: 8574, loss: 0.432950109243\n",
      "step: 8579, loss: 0.470955520868\n",
      "Checkpoint is saved\n",
      "step: 8584, loss: 0.508825480938\n",
      "step: 8589, loss: 0.436636567116\n",
      "step: 8594, loss: 0.373476922512\n",
      "step: 8599, loss: 0.523618996143\n",
      "Checkpoint is saved\n",
      "step: 8604, loss: 0.378849864006\n",
      "step: 8609, loss: 0.369289159775\n",
      "step: 8614, loss: 0.477206766605\n",
      "step: 8619, loss: 0.55613720417\n",
      "Checkpoint is saved\n",
      "step: 8624, loss: 0.435614585876\n",
      "step: 8629, loss: 0.497899949551\n",
      "step: 8634, loss: 0.476515769958\n",
      "step: 8639, loss: 0.533899903297\n",
      "Checkpoint is saved\n",
      "step: 8644, loss: 0.449421733618\n",
      "step: 8649, loss: 0.573310256004\n",
      "step: 8654, loss: 0.449691176414\n",
      "step: 8659, loss: 0.336921155453\n",
      "Checkpoint is saved\n",
      "step: 8664, loss: 0.595202326775\n",
      "step: 8669, loss: 0.427227199078\n",
      "step: 8674, loss: 0.407753229141\n",
      "step: 8679, loss: 0.532580554485\n",
      "Checkpoint is saved\n",
      "step: 8684, loss: 0.441794455051\n",
      "step: 8689, loss: 0.4669085145\n",
      "step: 8694, loss: 0.645097970963\n",
      "step: 8699, loss: 0.406595945358\n",
      "Checkpoint is saved\n",
      "step: 8704, loss: 0.461542487144\n",
      "step: 8709, loss: 0.504260659218\n",
      "step: 8714, loss: 0.473818480968\n",
      "step: 8719, loss: 0.400337100029\n",
      "Checkpoint is saved\n",
      "step: 8724, loss: 0.441625654697\n",
      "step: 8729, loss: 0.493850946426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8734, loss: 0.422891497612\n",
      "step: 8739, loss: 0.459006428719\n",
      "Checkpoint is saved\n",
      "step: 8744, loss: 0.53845179081\n",
      "step: 8749, loss: 0.546643137932\n",
      "step: 8754, loss: 0.520439326763\n",
      "step: 8759, loss: 0.437934219837\n",
      "Checkpoint is saved\n",
      "step: 8764, loss: 0.510752081871\n",
      "step: 8769, loss: 0.498391389847\n",
      "step: 8774, loss: 0.558626651764\n",
      "step: 8779, loss: 0.435430049896\n",
      "Checkpoint is saved\n",
      "step: 8784, loss: 0.365146934986\n",
      "step: 8789, loss: 0.487807422876\n",
      "step: 8794, loss: 0.450491935015\n",
      "step: 8799, loss: 0.42739841342\n",
      "Checkpoint is saved\n",
      "step: 8804, loss: 0.51668292284\n",
      "step: 8809, loss: 0.484479188919\n",
      "step: 8814, loss: 0.510768294334\n",
      "step: 8819, loss: 0.496273964643\n",
      "Checkpoint is saved\n",
      "step: 8824, loss: 0.453173726797\n",
      "step: 8829, loss: 0.467524796724\n",
      "step: 8834, loss: 0.437727808952\n",
      "step: 8839, loss: 0.432397931814\n",
      "Checkpoint is saved\n",
      "step: 8844, loss: 0.444600313902\n",
      "step: 8849, loss: 0.503513276577\n",
      "step: 8854, loss: 0.404534459114\n",
      "step: 8859, loss: 0.371380031109\n",
      "Checkpoint is saved\n",
      "step: 8864, loss: 0.467550724745\n",
      "step: 8869, loss: 0.425328791142\n",
      "step: 8874, loss: 0.435654759407\n",
      "step: 8879, loss: 0.41792306304\n",
      "Checkpoint is saved\n",
      "step: 8884, loss: 0.313501417637\n",
      "step: 8889, loss: 0.551300764084\n",
      "step: 8894, loss: 0.384950697422\n",
      "step: 8899, loss: 0.437555640936\n",
      "Checkpoint is saved\n",
      "step: 8904, loss: 0.421646744013\n",
      "step: 8909, loss: 0.396345466375\n",
      "step: 8914, loss: 0.470723748207\n",
      "step: 8919, loss: 0.523991405964\n",
      "Checkpoint is saved\n",
      "step: 8924, loss: 0.528079032898\n",
      "step: 8929, loss: 0.492698282003\n",
      "step: 8934, loss: 0.433063447475\n",
      "step: 8939, loss: 0.529274821281\n",
      "Checkpoint is saved\n",
      "step: 8944, loss: 0.538041710854\n",
      "step: 8949, loss: 0.441667675972\n",
      "step: 8954, loss: 0.409075260162\n",
      "step: 8959, loss: 0.468296349049\n",
      "Checkpoint is saved\n",
      "step: 8964, loss: 0.516266942024\n",
      "step: 8969, loss: 0.444390118122\n",
      "step: 8974, loss: 0.505786418915\n",
      "step: 8979, loss: 0.493642061949\n",
      "Checkpoint is saved\n",
      "step: 8984, loss: 0.454592913389\n",
      "step: 8989, loss: 0.342767119408\n",
      "step: 8994, loss: 0.40716034174\n",
      "step: 8999, loss: 0.458706140518\n",
      "Checkpoint is saved\n",
      "step: 9004, loss: 0.521607935429\n",
      "step: 9009, loss: 0.43016281724\n",
      "step: 9014, loss: 0.42142868042\n",
      "step: 9019, loss: 0.555458426476\n",
      "Checkpoint is saved\n",
      "step: 9024, loss: 0.465537726879\n",
      "step: 9029, loss: 0.554237246513\n",
      "step: 9034, loss: 0.429116904736\n",
      "step: 9039, loss: 0.49726614356\n",
      "Checkpoint is saved\n",
      "step: 9044, loss: 0.445704758167\n",
      "step: 9049, loss: 0.658963799477\n",
      "step: 9054, loss: 0.394328713417\n",
      "step: 9059, loss: 0.481665492058\n",
      "Checkpoint is saved\n",
      "step: 9064, loss: 0.484564572573\n",
      "step: 9069, loss: 0.412909924984\n",
      "step: 9074, loss: 0.417803049088\n",
      "step: 9079, loss: 0.42017531395\n",
      "Checkpoint is saved\n",
      "step: 9084, loss: 0.39782834053\n",
      "step: 9089, loss: 0.475503504276\n",
      "step: 9094, loss: 0.480257481337\n",
      "step: 9099, loss: 0.353000193834\n",
      "Checkpoint is saved\n",
      "step: 9104, loss: 0.544800639153\n",
      "step: 9109, loss: 0.519235491753\n",
      "step: 9114, loss: 0.451012462378\n",
      "step: 9119, loss: 0.433943927288\n",
      "Checkpoint is saved\n",
      "step: 9124, loss: 0.49969625473\n",
      "step: 9129, loss: 0.40772947669\n",
      "step: 9134, loss: 0.385044962168\n",
      "step: 9139, loss: 0.423391520977\n",
      "Checkpoint is saved\n",
      "step: 9144, loss: 0.431399077177\n",
      "step: 9149, loss: 0.391903281212\n",
      "step: 9154, loss: 0.397753506899\n",
      "step: 9159, loss: 0.46894043684\n",
      "Checkpoint is saved\n",
      "step: 9164, loss: 0.494766324759\n",
      "step: 9169, loss: 0.533286392689\n",
      "step: 9174, loss: 0.469979465008\n",
      "step: 9179, loss: 0.455482721329\n",
      "Checkpoint is saved\n",
      "step: 9184, loss: 0.443626254797\n",
      "step: 9189, loss: 0.551115632057\n",
      "step: 9194, loss: 0.413634866476\n",
      "step: 9199, loss: 0.417986631393\n",
      "Checkpoint is saved\n",
      "step: 9204, loss: 0.323842972517\n",
      "step: 9209, loss: 0.487074524164\n",
      "step: 9214, loss: 0.470619082451\n",
      "step: 9219, loss: 0.498835086823\n",
      "Checkpoint is saved\n",
      "step: 9224, loss: 0.477919250727\n",
      "step: 9229, loss: 0.377341210842\n",
      "step: 9234, loss: 0.50422501564\n",
      "step: 9239, loss: 0.437099456787\n",
      "Checkpoint is saved\n",
      "step: 9244, loss: 0.476503163576\n",
      "step: 9249, loss: 0.472411215305\n",
      "step: 9254, loss: 0.401394724846\n",
      "step: 9259, loss: 0.43315076828\n",
      "Checkpoint is saved\n",
      "step: 9264, loss: 0.506525933743\n",
      "step: 9269, loss: 0.532748997211\n",
      "step: 9274, loss: 0.553281068802\n",
      "step: 9279, loss: 0.466788649559\n",
      "Checkpoint is saved\n",
      "step: 9284, loss: 0.51720482111\n",
      "step: 9289, loss: 0.46145015955\n",
      "step: 9294, loss: 0.472517073154\n",
      "step: 9299, loss: 0.454566717148\n",
      "Checkpoint is saved\n",
      "step: 9304, loss: 0.516361415386\n",
      "step: 9309, loss: 0.390889108181\n",
      "step: 9314, loss: 0.480272322893\n",
      "step: 9319, loss: 0.420940697193\n",
      "Checkpoint is saved\n",
      "step: 9324, loss: 0.496314883232\n",
      "step: 9329, loss: 0.4462634027\n",
      "step: 9334, loss: 0.445498347282\n",
      "step: 9339, loss: 0.396570026875\n",
      "Checkpoint is saved\n",
      "step: 9344, loss: 0.407857209444\n",
      "step: 9349, loss: 0.525773644447\n",
      "step: 9354, loss: 0.356890559196\n",
      "step: 9359, loss: 0.42075073719\n",
      "Checkpoint is saved\n",
      "step: 9364, loss: 0.404844641685\n",
      "step: 9369, loss: 0.39402410388\n",
      "step: 9374, loss: 0.462125182152\n",
      "step: 9379, loss: 0.403279155493\n",
      "Checkpoint is saved\n",
      "step: 9384, loss: 0.335184842348\n",
      "step: 9389, loss: 0.557442307472\n",
      "step: 9394, loss: 0.400275558233\n",
      "step: 9399, loss: 0.498931258917\n",
      "Checkpoint is saved\n",
      "step: 9404, loss: 0.428846567869\n",
      "step: 9409, loss: 0.545514464378\n",
      "step: 9414, loss: 0.381837785244\n",
      "step: 9419, loss: 0.474822580814\n",
      "Checkpoint is saved\n",
      "step: 9424, loss: 0.534718632698\n",
      "step: 9429, loss: 0.383449435234\n",
      "step: 9434, loss: 0.455384105444\n",
      "step: 9439, loss: 0.412002325058\n",
      "Checkpoint is saved\n",
      "step: 9444, loss: 0.467320501804\n",
      "step: 9449, loss: 0.565140962601\n",
      "step: 9454, loss: 0.442208647728\n",
      "step: 9459, loss: 0.382018625736\n",
      "Checkpoint is saved\n",
      "step: 9464, loss: 0.493484020233\n",
      "step: 9469, loss: 0.386029243469\n",
      "step: 9474, loss: 0.407533437014\n",
      "step: 9479, loss: 0.521006345749\n",
      "Checkpoint is saved\n",
      "step: 9484, loss: 0.474392414093\n",
      "step: 9489, loss: 0.479987025261\n",
      "step: 9494, loss: 0.416458845139\n",
      "step: 9499, loss: 0.431545972824\n",
      "Checkpoint is saved\n",
      "step: 9504, loss: 0.575708985329\n",
      "step: 9509, loss: 0.456291139126\n",
      "step: 9514, loss: 0.343215286732\n",
      "step: 9519, loss: 0.38430839777\n",
      "Checkpoint is saved\n",
      "step: 9524, loss: 0.441870450974\n",
      "step: 9529, loss: 0.459457308054\n",
      "step: 9534, loss: 0.474939703941\n",
      "step: 9539, loss: 0.469469249249\n",
      "Checkpoint is saved\n",
      "step: 9544, loss: 0.511538028717\n",
      "step: 9549, loss: 0.498482018709\n",
      "step: 9554, loss: 0.464595973492\n",
      "step: 9559, loss: 0.43598985672\n",
      "Checkpoint is saved\n",
      "step: 9564, loss: 0.48117724061\n",
      "step: 9569, loss: 0.569334387779\n",
      "step: 9574, loss: 0.50390714407\n",
      "step: 9579, loss: 0.459081172943\n",
      "Checkpoint is saved\n",
      "step: 9584, loss: 0.340072214603\n",
      "step: 9589, loss: 0.499482959509\n",
      "step: 9594, loss: 0.543611228466\n",
      "step: 9599, loss: 0.507970750332\n",
      "Checkpoint is saved\n",
      "step: 9604, loss: 0.589327156544\n",
      "step: 9609, loss: 0.441645532846\n",
      "step: 9614, loss: 0.421225786209\n",
      "step: 9619, loss: 0.504698991776\n",
      "Checkpoint is saved\n",
      "step: 9624, loss: 0.490817517042\n",
      "step: 9629, loss: 0.456655144691\n",
      "step: 9634, loss: 0.388480067253\n",
      "step: 9639, loss: 0.51302921772\n",
      "Checkpoint is saved\n",
      "step: 9644, loss: 0.475075960159\n",
      "step: 9649, loss: 0.43476459384\n",
      "step: 9654, loss: 0.530203163624\n",
      "step: 9659, loss: 0.502176523209\n",
      "Checkpoint is saved\n",
      "step: 9664, loss: 0.318108648062\n",
      "step: 9669, loss: 0.425974488258\n",
      "step: 9674, loss: 0.427463054657\n",
      "step: 9679, loss: 0.561687290668\n",
      "Checkpoint is saved\n",
      "step: 9684, loss: 0.525601506233\n",
      "step: 9689, loss: 0.4225679636\n",
      "step: 9694, loss: 0.375618636608\n",
      "step: 9699, loss: 0.468742579222\n",
      "Checkpoint is saved\n",
      "step: 9704, loss: 0.472593426704\n",
      "step: 9709, loss: 0.488524228334\n",
      "step: 9714, loss: 0.456116139889\n",
      "step: 9719, loss: 0.482483774424\n",
      "Checkpoint is saved\n",
      "step: 9724, loss: 0.501079976559\n",
      "step: 9729, loss: 0.422228097916\n",
      "step: 9734, loss: 0.527839422226\n",
      "step: 9739, loss: 0.399284541607\n",
      "Checkpoint is saved\n",
      "step: 9744, loss: 0.424817830324\n",
      "step: 9749, loss: 0.414986848831\n",
      "step: 9754, loss: 0.514232873917\n",
      "step: 9759, loss: 0.414479702711\n",
      "Checkpoint is saved\n",
      "step: 9764, loss: 0.404443740845\n",
      "step: 9769, loss: 0.579141080379\n",
      "step: 9774, loss: 0.575408577919\n",
      "step: 9779, loss: 0.41860306263\n",
      "Checkpoint is saved\n",
      "step: 9784, loss: 0.434424638748\n",
      "step: 9789, loss: 0.489171564579\n",
      "step: 9794, loss: 0.381109535694\n",
      "step: 9799, loss: 0.469634056091\n",
      "Checkpoint is saved\n",
      "step: 9804, loss: 0.460054188967\n",
      "step: 9809, loss: 0.49292320013\n",
      "step: 9814, loss: 0.462748765945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 9819, loss: 0.444146215916\n",
      "Checkpoint is saved\n",
      "step: 9824, loss: 0.49671292305\n",
      "step: 9829, loss: 0.44187349081\n",
      "step: 9834, loss: 0.412432104349\n",
      "step: 9839, loss: 0.371022701263\n",
      "Checkpoint is saved\n",
      "step: 9844, loss: 0.399272620678\n",
      "step: 9849, loss: 0.415164530277\n",
      "step: 9854, loss: 0.403636515141\n",
      "step: 9859, loss: 0.518464565277\n",
      "Checkpoint is saved\n",
      "step: 9864, loss: 0.471900105476\n",
      "step: 9869, loss: 0.431535094976\n",
      "step: 9874, loss: 0.413908779621\n",
      "step: 9879, loss: 0.501074075699\n",
      "Checkpoint is saved\n",
      "step: 9884, loss: 0.475125342607\n",
      "step: 9889, loss: 0.420495867729\n",
      "step: 9894, loss: 0.419728070498\n",
      "step: 9899, loss: 0.510385215282\n",
      "Checkpoint is saved\n",
      "step: 9904, loss: 0.566861987114\n",
      "step: 9909, loss: 0.436008930206\n",
      "step: 9914, loss: 0.435705065727\n",
      "step: 9919, loss: 0.355878353119\n",
      "Checkpoint is saved\n",
      "step: 9924, loss: 0.533562958241\n",
      "step: 9929, loss: 0.384016871452\n",
      "step: 9934, loss: 0.421040832996\n",
      "step: 9939, loss: 0.393413335085\n",
      "Checkpoint is saved\n",
      "step: 9944, loss: 0.417788088322\n",
      "step: 9949, loss: 0.37556707859\n",
      "step: 9954, loss: 0.407368004322\n",
      "step: 9959, loss: 0.500816106796\n",
      "Checkpoint is saved\n",
      "step: 9964, loss: 0.376837968826\n",
      "step: 9969, loss: 0.465064287186\n",
      "step: 9974, loss: 0.363136857748\n",
      "step: 9979, loss: 0.453382611275\n",
      "Checkpoint is saved\n",
      "step: 9984, loss: 0.526470661163\n",
      "step: 9989, loss: 0.50760114193\n",
      "step: 9994, loss: 0.491914451122\n",
      "step: 9999, loss: 0.445900142193\n",
      "Checkpoint is saved\n",
      "Training time for 10000 steps: 3748.70777416s\n"
     ]
    }
   ],
   "source": [
    "# let's train the model\n",
    "\n",
    "# we will use this list to plot losses through steps\n",
    "losses = []\n",
    "\n",
    "# save a checkpoint so we can restore the model later \n",
    "saver = tf.train.Saver()\n",
    "\n",
    "print '------------------TRAINING------------------'\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    t = time.time()\n",
    "    for step in range(steps):\n",
    "        feed = feed_dict(X_train, Y_train)\n",
    "            \n",
    "        backward_step(sess, feed)\n",
    "        \n",
    "        if step % 5 == 4 or step == 0:\n",
    "            loss_value = sess.run(loss, feed_dict = feed)\n",
    "            print 'step: {}, loss: {}'.format(step, loss_value)\n",
    "            losses.append(loss_value)\n",
    "        \n",
    "        if step % 20 == 19:\n",
    "            saver.save(sess, 'checkpoints/', global_step=step)\n",
    "            print 'Checkpoint is saved'\n",
    "            \n",
    "    print 'Training time for {} steps: {}s'.format(steps, time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbEAAAEkCAYAAAChew9BAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMSIislgMMSIislj/D1xEOkV0CxB1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb775070290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot losses\n",
    "\n",
    "with plt.style.context('fivethirtyeight'):\n",
    "    plt.plot(losses, linewidth = 1)\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Losses')\n",
    "    plt.ylim((0, 12))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/-9999\n",
      "1.\n",
      "--------------------------------\n",
      "reboot the server\n",
      "<ukn> la <ukn> \n",
      "--------------------------------\n",
      "2.\n",
      "--------------------------------\n",
      "What' s your name\n",
      "<ukn> en su nombre \n",
      "--------------------------------\n",
      "3.\n",
      "--------------------------------\n",
      "My name is\n",
      "Mi nombre es \n",
      "--------------------------------\n",
      "4.\n",
      "--------------------------------\n",
      "What are you doing\n",
      "¿Cuáles serían haciendo \n",
      "--------------------------------\n",
      "5.\n",
      "--------------------------------\n",
      "I am reading a book\n",
      "Soy que se <ukn> \n",
      "--------------------------------\n",
      "6.\n",
      "--------------------------------\n",
      "How are you\n",
      "¿Cómo se lo es \n",
      "--------------------------------\n",
      "7.\n",
      "--------------------------------\n",
      "I am good\n",
      "Soy buena \n",
      "--------------------------------\n",
      "8.\n",
      "--------------------------------\n",
      "Do you speak English\n",
      "¿Cree hablar de la inglés \n",
      "--------------------------------\n",
      "9.\n",
      "--------------------------------\n",
      "What time is it\n",
      "Lo que es el momento \n",
      "--------------------------------\n",
      "10.\n",
      "--------------------------------\n",
      "Hi\n",
      "<ukn> \n",
      "--------------------------------\n",
      "11.\n",
      "--------------------------------\n",
      "Goodbye\n",
      "<ukn> \n",
      "--------------------------------\n",
      "12.\n",
      "--------------------------------\n",
      "Yes\n",
      "Sí \n",
      "--------------------------------\n",
      "13.\n",
      "--------------------------------\n",
      "No\n",
      "No \n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "# let's test the model\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    # placeholders\n",
    "    encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'encoder{}'.format(i)) for i in range(input_seq_len)]\n",
    "    decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'decoder{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "    # output projection\n",
    "    size = 512\n",
    "    w_t = tf.get_variable('proj_w', [es_vocab_size, size], tf.float32)\n",
    "    b = tf.get_variable('proj_b', [es_vocab_size], tf.float32)\n",
    "    w = tf.transpose(w_t)\n",
    "    output_projection = (w, b)\n",
    "    \n",
    "    # change the model so that output at time t can be fed as input at time t+1\n",
    "    outputs, states = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "                                                encoder_inputs,\n",
    "                                                decoder_inputs,\n",
    "                                                tf.contrib.rnn.BasicLSTMCell(size),\n",
    "                                                num_encoder_symbols = en_vocab_size,\n",
    "                                                num_decoder_symbols = es_vocab_size,\n",
    "                                                embedding_size = 100,\n",
    "                                                feed_previous = True, # <-----this is changed----->\n",
    "                                                output_projection = output_projection,\n",
    "                                                dtype = tf.float32)\n",
    "    \n",
    "    # ops for projecting outputs\n",
    "    outputs_proj = [tf.matmul(outputs[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "\n",
    "    # let's translate these sentences     \n",
    "    en_sentences = [\"reboot the server\", \"What' s your name\", 'My name is', 'What are you doing', 'I am reading a book',\\\n",
    "                    'How are you', 'I am good', 'Do you speak English', 'What time is it', 'Hi', 'Goodbye', 'Yes', 'No']\n",
    "    en_sentences_encoded = [[en_word2idx.get(word, 0) for word in en_sentence.split()] for en_sentence in en_sentences]\n",
    "    \n",
    "    # padding to fit encoder input\n",
    "    for i in range(len(en_sentences_encoded)):\n",
    "        en_sentences_encoded[i] += (15 - len(en_sentences_encoded[i])) * [en_word2idx['<pad>']]\n",
    "    \n",
    "    # restore all variables - use the last checkpoint saved\n",
    "    saver = tf.train.Saver()\n",
    "    path = tf.train.latest_checkpoint('checkpoints')\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # restore\n",
    "        saver.restore(sess, path)\n",
    "        \n",
    "        # feed data into placeholders\n",
    "        feed = {}\n",
    "        for i in range(input_seq_len):\n",
    "            feed[encoder_inputs[i].name] = np.array([en_sentences_encoded[j][i] for j in range(len(en_sentences_encoded))], dtype = np.int32)\n",
    "            \n",
    "        feed[decoder_inputs[0].name] = np.array([es_word2idx['<go>']] * len(en_sentences_encoded), dtype = np.int32)\n",
    "        \n",
    "        # translate\n",
    "        output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
    "        \n",
    "        # decode seq.\n",
    "        for i in range(len(en_sentences_encoded)):\n",
    "            print '{}.\\n--------------------------------'.format(i+1)\n",
    "            ouput_seq = [output_sequences[j][i] for j in range(output_seq_len)]\n",
    "            #decode output sequence\n",
    "            words = decode_output(ouput_seq)\n",
    "        \n",
    "            print en_sentences[i]\n",
    "            for i in range(len(words)):\n",
    "                if words[i] not in ['<eos>', '<pad>', '<go>']:\n",
    "                    print words[i],\n",
    "            \n",
    "            print '\\n--------------------------------'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-py2",
   "language": "python",
   "name": "tensorflow-py2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
