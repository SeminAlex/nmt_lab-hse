{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle as pickle\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readed en0\n",
      "readed es0\n",
      "readed en1\n",
      "readed es1\n",
      "readed en2\n",
      "readed es2\n",
      "readed en3\n",
      "readed es3\n"
     ]
    }
   ],
   "source": [
    "#dataRead\n",
    "\n",
    "en_sentences = []\n",
    "es_sentences = []\n",
    "\n",
    "for i in range(4):\n",
    "    with open(\"dataset/europarl-v7.es-en-\" + str(i) +\".en\", 'r') as en_file:\n",
    "        for en_line in en_file:\n",
    "            en_sentences.append(en_line)\n",
    "    print \"readed en\" + str(i)\n",
    "    with open(\"dataset/europarl-v7.es-en-\" + str(i) +\".es\", 'r') as es_file:\n",
    "        for es_line in es_file:\n",
    "            es_sentences.append(es_line)\n",
    "    print \"readed es\" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apart from anything else, they divisively create social problems for workers in the shipping industry and residents of island regions.\n",
      "\n",
      "Aparte de todo, crean, generando discrepancias, problemas sociales para los trabajadores del sector naviero y los residentes de las regiones insulares.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = 999999\n",
    "print en_sentences[index]\n",
    "print es_sentences[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataPreparation\n",
    "def create_dataset(source_sentences,target_sentences):\n",
    "    source_vocab_dict = Counter(word.strip(',.\" ;:)(][?!') for sentence in source_sentences for word in sentence.split())\n",
    "    target_vocab_dict = Counter(word.strip(',.\" ;:)(][?!') for sentence in target_sentences for word in sentence.split())\n",
    "\n",
    "    source_vocab = map(lambda x: x[0], sorted(source_vocab_dict.items(), key = lambda x: -x[1]))\n",
    "    target_vocab = map(lambda x: x[0], sorted(target_vocab_dict.items(), key = lambda x: -x[1]))\n",
    "    \n",
    "    source_vocab = source_vocab[:20000]\n",
    "    target_vocab = target_vocab[:30000]\n",
    "    \n",
    "    start_idx = 2\n",
    "    source_word2idx = dict([(word, idx+start_idx) for idx, word in enumerate(source_vocab)])\n",
    "    source_word2idx['<ukn>'] = 0\n",
    "    source_word2idx['<pad>'] = 1\n",
    "    source_idx2word = dict([(idx, word) for word, idx in source_word2idx.iteritems()])\n",
    "    \n",
    "    start_idx = 4\n",
    "    target_word2idx = dict([(word, idx+start_idx) for idx, word in enumerate(target_vocab)])\n",
    "    target_word2idx['<ukn>'] = 0\n",
    "    target_word2idx['<go>']  = 1\n",
    "    target_word2idx['<eos>'] = 2\n",
    "    target_word2idx['<pad>'] = 3\n",
    "    \n",
    "    target_idx2word = dict([(idx, word) for word, idx in target_word2idx.iteritems()])\n",
    "    x = [[source_word2idx.get(word.strip(',.\" ;:)(][?!'), 0) for word in sentence.split()] for sentence in source_sentences]\n",
    "    y = [[target_word2idx.get(word.strip(',.\" ;:)(][?!'), 0) for word in sentence.split()] for sentence in target_sentences]\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        n1 = len(x[i])\n",
    "        n2 = len(y[i])\n",
    "        n = n1 if n1 < n2 else n2 \n",
    "        if abs(n1 - n2) <= 0.3 * n:\n",
    "            if n1 <= 15 and n2 <= 15:\n",
    "                X.append(x[i])\n",
    "                Y.append(y[i])\n",
    "    return X, Y, source_word2idx, source_idx2word, source_vocab, target_word2idx, target_idx2word, target_vocab\n",
    "\n",
    "def save_dataset(file_path, obj):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(obj, f, -1)\n",
    "\n",
    "def read_dataset(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#en_sentences = ['hello my friend', 'we need to reboot the server']\n",
    "#es_sentences = ['hola mi amigo', 'necesitamos reiniciar el servidor']\n",
    "save_dataset('./data.pkl', create_dataset(en_sentences, es_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "X, Y, en_word2idx, en_idx2word, en_vocab, es_word2idx, es_idx2word, es_vocab = read_dataset('data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence in English - encoded: [8425, 3, 2, 1695]\n",
      "Sentence in Spanish - encoded: [10624, 13, 575, 4, 1420]\n",
      "Decoded:\n",
      "------------------------\n",
      "Resumption of the session \n",
      "\n",
      "Reanudación del período de sesiones\n"
     ]
    }
   ],
   "source": [
    "#CHECK THAT WORKs\n",
    "print 'Sentence in English - encoded:', X[0]\n",
    "print 'Sentence in Spanish - encoded:', Y[0]\n",
    "print 'Decoded:\\n------------------------'\n",
    "\n",
    "for i in range(len(X[0])):\n",
    "    print en_idx2word[X[0][i]],\n",
    "    \n",
    "print '\\n'\n",
    "\n",
    "for i in range(len(Y[0])):\n",
    "    print es_idx2word[Y[0][i]],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data processing\n",
    "\n",
    "# data padding\n",
    "def data_padding(x, y, length = 15):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = x[i] + (length - len(x[i])) * [en_word2idx['<pad>']]\n",
    "        y[i] = [es_word2idx['<go>']] + y[i] + [es_word2idx['<eos>']] + (length-len(y[i])) * [es_word2idx['<pad>']]\n",
    "\n",
    "data_padding(X, Y)\n",
    "print X\n",
    "\n",
    "# data splitting\n",
    "X_train,  X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1)\n",
    "\n",
    "del X\n",
    "del Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model\n",
    "\n",
    "input_seq_len = 15\n",
    "output_seq_len = 17\n",
    "en_vocab_size = len(en_vocab) + 2 # + <pad>, <ukn>\n",
    "es_vocab_size = len(es_vocab) + 4 # + <pad>, <ukn>, <eos>, <go>\n",
    "\n",
    "# placeholders\n",
    "encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'encoder{}'.format(i)) for i in range(input_seq_len)]\n",
    "decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'decoder{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "targets = [decoder_inputs[i+1] for i in range(output_seq_len-1)]\n",
    "# add one more target\n",
    "targets.append(tf.placeholder(dtype = tf.int32, shape = [None], name = 'last_target'))\n",
    "target_weights = [tf.placeholder(dtype = tf.float32, shape = [None], name = 'target_w{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "# output projection\n",
    "size = 512\n",
    "w_t = tf.get_variable('proj_w', [es_vocab_size, size], tf.float32)\n",
    "b = tf.get_variable('proj_b', [es_vocab_size], tf.float32)\n",
    "w = tf.transpose(w_t)\n",
    "output_projection = (w, b)\n",
    "\n",
    "outputs, states = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "                                            encoder_inputs,\n",
    "                                            decoder_inputs,\n",
    "                                            tf.contrib.rnn.BasicLSTMCell(size),\n",
    "                                            num_encoder_symbols = en_vocab_size,\n",
    "                                            num_decoder_symbols = es_vocab_size,\n",
    "                                            embedding_size = 100,\n",
    "                                            feed_previous = False,\n",
    "                                            output_projection = output_projection,\n",
    "                                            dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our loss function\n",
    "\n",
    "# sampled softmax loss - returns: A batch_size 1-D tensor of per-example sampled softmax losses\n",
    "def sampled_loss(labels, logits):\n",
    "    return tf.nn.sampled_softmax_loss(\n",
    "                        weights = w_t,\n",
    "                        biases = b,\n",
    "                        labels = tf.reshape(labels, [-1, 1]),\n",
    "                        inputs = logits,\n",
    "                        num_sampled = 512,\n",
    "                        num_classes = es_vocab_size)\n",
    "\n",
    "# Weighted cross-entropy loss for a sequence of logits\n",
    "loss = tf.contrib.legacy_seq2seq.sequence_loss(outputs, targets, target_weights, softmax_loss_function = sampled_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define some helper functions\n",
    "\n",
    "# simple softmax function\n",
    "def softmax(x):\n",
    "    n = np.max(x)\n",
    "    e_x = np.exp(x - n)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# feed data into placeholders\n",
    "def feed_dict(x, y, batch_size = 64):\n",
    "    feed = {}\n",
    "    \n",
    "    idxes = np.random.choice(len(x), size = batch_size, replace = False)\n",
    "    \n",
    "    for i in range(input_seq_len):\n",
    "        feed[encoder_inputs[i].name] = np.array([x[j][i] for j in idxes], dtype = np.int32)\n",
    "        \n",
    "    for i in range(output_seq_len):\n",
    "        feed[decoder_inputs[i].name] = np.array([y[j][i] for j in idxes], dtype = np.int32)\n",
    "        \n",
    "    feed[targets[len(targets)-1].name] = np.full(shape = [batch_size], fill_value = es_word2idx['<pad>'], dtype = np.int32)\n",
    "    \n",
    "    for i in range(output_seq_len-1):\n",
    "        batch_weights = np.ones(batch_size, dtype = np.float32)\n",
    "        target = feed[decoder_inputs[i+1].name]\n",
    "        for j in range(batch_size):\n",
    "            if target[j] == es_word2idx['<pad>']:\n",
    "                batch_weights[j] = 0.0\n",
    "        feed[target_weights[i].name] = batch_weights\n",
    "        \n",
    "    feed[target_weights[output_seq_len-1].name] = np.zeros(batch_size, dtype = np.float32)\n",
    "    \n",
    "    return feed\n",
    "\n",
    "# decode output sequence\n",
    "def decode_output(output_seq):\n",
    "    words = []\n",
    "    for i in range(output_seq_len):\n",
    "        smax = softmax(output_seq[i])\n",
    "        idx = np.argmax(smax)\n",
    "        words.append(se_idx2word[idx])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ops and hyperparameters\n",
    "learning_rate = 5e-3\n",
    "batch_size = 64\n",
    "steps = 1000\n",
    "\n",
    "# ops for projecting outputs\n",
    "outputs_proj = [tf.matmul(outputs[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "\n",
    "# training op\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# init op\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# forward step\n",
    "def forward_step(sess, feed):\n",
    "    output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
    "    return output_sequences\n",
    "\n",
    "# training step\n",
    "def backward_step(sess, feed):\n",
    "    sess.run(optimizer, feed_dict = feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------TRAINING------------------\n",
      "step: 0, loss: 8.92558860779\n",
      "step: 4, loss: 9.06633377075\n",
      "step: 9, loss: 9.02219867706\n",
      "step: 14, loss: 9.09511947632\n",
      "step: 19, loss: 9.00766563416\n",
      "Checkpoint is saved\n",
      "step: 24, loss: 9.03717803955\n",
      "step: 29, loss: 9.01612567902\n",
      "step: 34, loss: 8.99985313416\n",
      "step: 39, loss: 8.96087741852\n",
      "Checkpoint is saved\n",
      "step: 44, loss: 8.87794685364\n",
      "step: 49, loss: 8.88743305206\n",
      "step: 54, loss: 8.74408435822\n",
      "step: 59, loss: 8.054728508\n",
      "Checkpoint is saved\n",
      "step: 64, loss: 7.58675718307\n",
      "step: 69, loss: 6.99651384354\n",
      "step: 74, loss: 6.85336017609\n",
      "step: 79, loss: 6.15587091446\n",
      "Checkpoint is saved\n",
      "step: 84, loss: 6.20940351486\n",
      "step: 89, loss: 6.20745563507\n",
      "step: 94, loss: 6.43007087708\n",
      "step: 99, loss: 6.10704040527\n",
      "Checkpoint is saved\n",
      "step: 104, loss: 5.55369186401\n",
      "step: 109, loss: 5.6913599968\n",
      "step: 114, loss: 5.7345533371\n",
      "step: 119, loss: 6.28576087952\n",
      "Checkpoint is saved\n",
      "step: 124, loss: 5.48876857758\n",
      "step: 129, loss: 7.00510120392\n",
      "step: 134, loss: 5.30073070526\n",
      "step: 139, loss: 5.23182344437\n",
      "Checkpoint is saved\n",
      "step: 144, loss: 5.46565628052\n",
      "step: 149, loss: 6.43339300156\n",
      "step: 154, loss: 4.98730516434\n",
      "step: 159, loss: 4.7909784317\n",
      "Checkpoint is saved\n",
      "step: 164, loss: 4.98559999466\n",
      "step: 169, loss: 5.24494838715\n",
      "step: 174, loss: 5.08930587769\n",
      "step: 179, loss: 5.30606937408\n",
      "Checkpoint is saved\n",
      "step: 184, loss: 4.8882856369\n",
      "step: 189, loss: 4.85701990128\n",
      "step: 194, loss: 5.05048274994\n",
      "step: 199, loss: 4.53820610046\n",
      "Checkpoint is saved\n",
      "step: 204, loss: 4.77499771118\n",
      "step: 209, loss: 4.34143924713\n",
      "step: 214, loss: 4.53493595123\n",
      "step: 219, loss: 4.2360086441\n",
      "Checkpoint is saved\n",
      "step: 224, loss: 4.43618822098\n",
      "step: 229, loss: 4.53355693817\n",
      "step: 234, loss: 4.38798236847\n",
      "step: 239, loss: 4.1531829834\n",
      "Checkpoint is saved\n",
      "step: 244, loss: 3.71070742607\n",
      "step: 249, loss: 3.86033940315\n",
      "step: 254, loss: 4.03124904633\n",
      "step: 259, loss: 3.67005372047\n",
      "Checkpoint is saved\n",
      "step: 264, loss: 3.47524404526\n",
      "step: 269, loss: 3.88377833366\n",
      "step: 274, loss: 4.00588512421\n",
      "step: 279, loss: 3.13337755203\n",
      "Checkpoint is saved\n",
      "step: 284, loss: 3.68928909302\n",
      "step: 289, loss: 3.33679413795\n",
      "step: 294, loss: 3.28311753273\n",
      "step: 299, loss: 3.39050579071\n",
      "Checkpoint is saved\n",
      "step: 304, loss: 3.14247965813\n",
      "step: 309, loss: 3.54907226562\n",
      "step: 314, loss: 3.38231134415\n",
      "step: 319, loss: 3.02172493935\n",
      "Checkpoint is saved\n",
      "step: 324, loss: 3.58394551277\n",
      "step: 329, loss: 3.03760695457\n",
      "step: 334, loss: 2.83631467819\n",
      "step: 339, loss: 3.39379858971\n",
      "Checkpoint is saved\n",
      "step: 344, loss: 2.68682146072\n",
      "step: 349, loss: 3.39361667633\n",
      "step: 354, loss: 2.75686788559\n",
      "step: 359, loss: 3.04542708397\n",
      "Checkpoint is saved\n",
      "step: 364, loss: 2.9369392395\n",
      "step: 369, loss: 3.0882768631\n",
      "step: 374, loss: 2.53012752533\n",
      "step: 379, loss: 3.01166653633\n"
     ]
    }
   ],
   "source": [
    "# let's train the model\n",
    "\n",
    "# we will use this list to plot losses through steps\n",
    "losses = []\n",
    "\n",
    "# save a checkpoint so we can restore the model later \n",
    "saver = tf.train.Saver()\n",
    "\n",
    "print '------------------TRAINING------------------'\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    t = time.time()\n",
    "    for step in range(steps):\n",
    "        feed = feed_dict(X_train, Y_train)\n",
    "            \n",
    "        backward_step(sess, feed)\n",
    "        \n",
    "        if step % 5 == 4 or step == 0:\n",
    "            loss_value = sess.run(loss, feed_dict = feed)\n",
    "            print 'step: {}, loss: {}'.format(step, loss_value)\n",
    "            losses.append(loss_value)\n",
    "        \n",
    "        if step % 20 == 19:\n",
    "            saver.save(sess, 'checkpoints/', global_step=step)\n",
    "            print 'Checkpoint is saved'\n",
    "            \n",
    "    print 'Training time for {} steps: {}s'.format(steps, time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "\n",
    "with plt.style.context('fivethirtyeight'):\n",
    "    plt.plot(losses, linewidth = 1)\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Losses')\n",
    "    plt.ylim((0, 12))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's test the model\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    # placeholders\n",
    "    encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'encoder{}'.format(i)) for i in range(input_seq_len)]\n",
    "    decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'decoder{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "    # output projection\n",
    "    size = 512\n",
    "    w_t = tf.get_variable('proj_w', [de_vocab_size, size], tf.float32)\n",
    "    b = tf.get_variable('proj_b', [de_vocab_size], tf.float32)\n",
    "    w = tf.transpose(w_t)\n",
    "    output_projection = (w, b)\n",
    "    \n",
    "    # change the model so that output at time t can be fed as input at time t+1\n",
    "    outputs, states = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "                                                encoder_inputs,\n",
    "                                                decoder_inputs,\n",
    "                                                tf.contrib.rnn.BasicLSTMCell(size),\n",
    "                                                num_encoder_symbols = en_vocab_size,\n",
    "                                                num_decoder_symbols = de_vocab_size,\n",
    "                                                embedding_size = 100,\n",
    "                                                feed_previous = True, # <-----this is changed----->\n",
    "                                                output_projection = output_projection,\n",
    "                                                dtype = tf.float32)\n",
    "    \n",
    "    # ops for projecting outputs\n",
    "    outputs_proj = [tf.matmul(outputs[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "\n",
    "    # let's translate these sentences     \n",
    "    en_sentences = [\"What' s your name\", 'My name is', 'What are you doing', 'I am reading a book',\\\n",
    "                    'How are you', 'I am good', 'Do you speak English', 'What time is it', 'Hi', 'Goodbye', 'Yes', 'No']\n",
    "    en_sentences_encoded = [[en_word2idx.get(word, 0) for word in en_sentence.split()] for en_sentence in en_sentences]\n",
    "    \n",
    "    # padding to fit encoder input\n",
    "    for i in range(len(en_sentences_encoded)):\n",
    "        en_sentences_encoded[i] += (15 - len(en_sentences_encoded[i])) * [en_word2idx['<pad>']]\n",
    "    \n",
    "    # restore all variables - use the last checkpoint saved\n",
    "    saver = tf.train.Saver()\n",
    "    path = tf.train.latest_checkpoint('checkpoints')\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # restore\n",
    "        saver.restore(sess, path)\n",
    "        \n",
    "        # feed data into placeholders\n",
    "        feed = {}\n",
    "        for i in range(input_seq_len):\n",
    "            feed[encoder_inputs[i].name] = np.array([en_sentences_encoded[j][i] for j in range(len(en_sentences_encoded))], dtype = np.int32)\n",
    "            \n",
    "        feed[decoder_inputs[0].name] = np.array([de_word2idx['<go>']] * len(en_sentences_encoded), dtype = np.int32)\n",
    "        \n",
    "        # translate\n",
    "        output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
    "        \n",
    "        # decode seq.\n",
    "        for i in range(len(en_sentences_encoded)):\n",
    "            print '{}.\\n--------------------------------'.format(i+1)\n",
    "            ouput_seq = [output_sequences[j][i] for j in range(output_seq_len)]\n",
    "            #decode output sequence\n",
    "            words = decode_output(ouput_seq)\n",
    "        \n",
    "            print en_sentences[i]\n",
    "            for i in range(len(words)):\n",
    "                if words[i] not in ['<eos>', '<pad>', '<go>']:\n",
    "                    print words[i],\n",
    "            \n",
    "            print '\\n--------------------------------'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-py2",
   "language": "python",
   "name": "tensorflow-py2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
